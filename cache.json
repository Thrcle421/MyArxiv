{"2025-05-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2505.18154v1","updated":"2025-05-23T17:59:50Z","published":"2025-05-23T17:59:50Z","title":"The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step\n  Induction to Complex Moral Dilemmas","summary":"  Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs.\n","authors":["Ya Wu","Qiang Sheng","Danding Wang","Guang Yang","Yifan Sun","Zhengjia Wang","Yuyan Bu","Juan Cao"],"pdf_url":"https://arxiv.org/pdf/2505.18154v1.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.18152v1","updated":"2025-05-23T17:59:29Z","published":"2025-05-23T17:59:29Z","title":"Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry\n  Understanding in LLMs","summary":"  Arabic poetry stands as one of the most sophisticated and culturally embedded\nforms of expression in the Arabic language, known for its layered meanings,\nstylistic diversity, and deep historical continuity. Although large language\nmodels (LLMs) have demonstrated strong performance across languages and tasks,\ntheir ability to understand Arabic poetry remains largely unexplored. In this\nwork, we introduce `Fann or Flop`, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in twelve historical eras, covering 21\ncore poetic genres and a variety of metrical forms, from classical structures\nto contemporary free verse. The benchmark comprises a curated corpus of poems\nwith explanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM is in understanding\nclassical Arabic through the Arabic poetry. Unlike surface-level tasks, this\ndomain demands deeper interpretive reasoning and cultural sensitivity. Our\nevaluation of state-of-the-art LLMs shows that most models struggle with poetic\nunderstanding despite strong results on standard Arabic benchmarks. We release\n`Fann or Flop` along with the evaluation suite as an open-source resource to\nenable rigorous evaluation and advancement for Arabic language models. Code is\navailable at: https://github.com/mbzuai-oryx/FannOrFlop.\n","authors":["Wafa Alghallabi","Ritesh Thawkar","Sara Ghaboura","Ketan More","Omkar Thawakar","Hisham Cholakkal","Salman Khan","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2505.18152v1.pdf","comment":"Github:https://github.com/mbzuai-oryx/FannOrFlop,\n  Dataset:https://huggingface.co/datasets/omkarthawakar/FannOrFlop"},{"id":"http://arxiv.org/abs/2505.18149v1","updated":"2025-05-23T17:57:43Z","published":"2025-05-23T17:57:43Z","title":"First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models","summary":"  Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches $n$ independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.\n","authors":["Aradhye Agarwal","Ayan Sengupta","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2505.18149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18148v1","updated":"2025-05-23T17:57:42Z","published":"2025-05-23T17:57:42Z","title":"Lost in the Haystack: Smaller Needles are More Difficult for LLMs to\n  Find","summary":"  Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.\n","authors":["Owen Bianchi","Mathew J. Koretsky","Maya Willey","Chelsea X. Alvarado","Tanay Nayak","Adi Asija","Nicole Kuznetsov","Mike A. Nalls","Faraz Faghri","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2505.18148v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.18136v1","updated":"2025-05-23T17:44:06Z","published":"2025-05-23T17:44:06Z","title":"Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism\n  Detection","summary":"  We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.\n","authors":["Mykola Trokhymovych","Lydia Pintscher","Ricardo Baeza-Yates","Diego Saez-Trumper"],"pdf_url":"https://arxiv.org/pdf/2505.18136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18135v1","updated":"2025-05-23T17:43:48Z","published":"2025-05-23T17:43:48Z","title":"Gaming Tool Preferences in Agentic LLMs","summary":"  Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.\n","authors":["Kazem Faghih","Wenxiao Wang","Yize Cheng","Siddhant Bharti","Gaurang Sriramanan","Sriram Balasubramanian","Parsa Hosseini","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2505.18135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18134v1","updated":"2025-05-23T17:43:27Z","published":"2025-05-23T17:43:27Z","title":"VideoGameBench: Can Vision-Language Models complete popular video games?","summary":"  Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.\n","authors":["Alex L. Zhang","Thomas L. Griffiths","Karthik R. Narasimhan","Ofir Press"],"pdf_url":"https://arxiv.org/pdf/2505.18134v1.pdf","comment":"9 pages, 33 pages including supplementary"},{"id":"http://arxiv.org/abs/2505.18129v1","updated":"2025-05-23T17:41:14Z","published":"2025-05-23T17:41:14Z","title":"One RL to See Them All: Visual Triple Unified Reinforcement Learning","summary":"  Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.\n","authors":["Yan Ma","Linge Du","Xuyang Shen","Shaoxiang Chen","Pengfei Li","Qibing Ren","Lizhuang Ma","Yuchao Dai","Pengfei Liu","Junjie Yan"],"pdf_url":"https://arxiv.org/pdf/2505.18129v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2505.18128v1","updated":"2025-05-23T17:38:47Z","published":"2025-05-23T17:38:47Z","title":"Frankentext: Stitching random text fragments into long-form narratives","summary":"  We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.\n","authors":["Chau Minh Pham","Jenna Russell","Dzung Pham","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2505.18128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19614v2","updated":"2025-05-23T17:37:43Z","published":"2025-02-26T23:04:05Z","title":"Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection\n  in Peer Review","summary":"  Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in large language models (LLMs), a new risk to the peer review\nprocess is that negligent reviewers will rely on LLMs to perform the often time\nconsuming process of reviewing a paper. However, there is a lack of existing\nresources for benchmarking the detectability of AI text in the domain of peer\nreview. To address this deficiency, we introduce a comprehensive dataset\ncontaining a total of 788,984 AI-written peer reviews paired with corresponding\nhuman reviews, covering 8 years of papers submitted to each of two leading AI\nresearch conferences (ICLR and NeurIPS). We use this new resource to evaluate\nthe ability of 18 existing AI text detection algorithms to distinguish between\npeer reviews fully written by humans and different state-of-the-art LLMs.\nAdditionally, we explore a context-aware detection method called Anchor, which\nleverages manuscript content to detect AI-generated reviews, and analyze the\nsensitivity of detection models to LLM-assisted editing of human-written text.\nOur work reveals the difficulty of identifying AI-generated text at the\nindividual peer review level, highlighting the urgent need for new tools and\nmethods to detect this unethical use of generative AI. Our dataset is publicly\navailable at:\nhttps://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.\n","authors":["Sungduk Yu","Man Luo","Avinash Madusu","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2502.19614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18126v1","updated":"2025-05-23T17:36:13Z","published":"2025-05-23T17:36:13Z","title":"Reward Model Overoptimisation in Iterated RLHF","summary":"  Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.\n","authors":["Lorenz Wolf","Robert Kirk","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2505.18126v1.pdf","comment":"20 pages, 17 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.18125v1","updated":"2025-05-23T17:34:28Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18122v1","updated":"2025-05-23T17:28:43Z","published":"2025-05-23T17:28:43Z","title":"UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema\n  Simplification","summary":"  Recent advances in large language models (LLMs) have greatly improved\nText-to-SQL performance for single-table queries. But, it remains challenging\nin multi-table databases due to complex schema and relational operations.\nExisting methods often struggle with retrieving the right tables and columns,\ngenerating accurate JOINs and UNIONs, and generalizing across diverse schemas.\nTo address these issues, we introduce UNJOIN, a two-stage framework that\ndecouples the retrieval of schema elements from SQL logic generation. In the\nfirst stage, we merge the column names of all tables in the database into a\nsingle-table representation by prefixing each column with its table name. This\nallows the model to focus purely on accurate retrieval without being distracted\nby the need to write complex SQL logic. In the second stage, the SQL query is\ngenerated on this simplified schema and mapped back to the original schema by\nreconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and\nBIRD datasets show that UNJOIN matches or exceeds the state-of-the-art\nbaselines. UNJOIN uses only schema information, which does not require data\naccess or fine-tuning, making it scalable and adaptable across databases.\n","authors":["Poojah Ganesan","Rajat Aayush Jha","Dan Roth","Vivek Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.18122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18121v1","updated":"2025-05-23T17:23:11Z","published":"2025-05-23T17:23:11Z","title":"ProgRM: Build Better GUI Agents with Progress Rewards","summary":"  LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.\n","authors":["Danyang Zhang","Situo Zhang","Ziyue Yang","Zichen Zhu","Zihan Zhao","Ruisheng Cao","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2505.18121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18116v1","updated":"2025-05-23T17:17:40Z","published":"2025-05-23T17:17:40Z","title":"Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning","summary":"  Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.\n","authors":["Huayu Chen","Kaiwen Zheng","Qinsheng Zhang","Ganqu Cui","Yin Cui","Haotian Ye","Tsung-Yi Lin","Ming-Yu Liu","Jun Zhu","Haoxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.18116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14311v2","updated":"2025-05-23T17:13:28Z","published":"2025-05-20T12:59:55Z","title":"HausaNLP: Current Status, Challenges and Future Directions for Hausa\n  Natural Language Processing","summary":"  Hausa Natural Language Processing (NLP) has gained increasing attention in\nrecent years, yet remains understudied as a low-resource language despite\nhaving over 120 million first-language (L1) and 80 million second-language (L2)\nspeakers worldwide. While significant advances have been made in high-resource\nlanguages, Hausa NLP faces persistent challenges, including limited open-source\ndatasets and inadequate model representation. This paper presents an overview\nof the current state of Hausa NLP, systematically examining existing resources,\nresearch contributions, and gaps across fundamental NLP tasks: text\nclassification, machine translation, named entity recognition, speech\nrecognition, and question answering. We introduce HausaNLP\n(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\ntools, and research works to enhance accessibility and drive further\ndevelopment. Furthermore, we discuss challenges in integrating Hausa into large\nlanguage models (LLMs), addressing issues of suboptimal tokenization and\ndialectal variation. Finally, we propose strategic research directions\nemphasizing dataset expansion, improved language modeling approaches, and\nstrengthened community collaboration to advance Hausa NLP. Our work provides\nboth a foundation for accelerating Hausa NLP progress and valuable insights for\nbroader multilingual NLP research.\n","authors":["Shamsuddeen Hassan Muhammad","Ibrahim Said Ahmad","Idris Abdulmumin","Falalu Ibrahim Lawan","Babangida Sani","Sukairaj Hafiz Imam","Yusuf Aliyu","Sani Abdullahi Sani","Ali Usman Umar","Tajuddeen Gwadabe","Kenneth Church","Vukosi Marivate"],"pdf_url":"https://arxiv.org/pdf/2505.14311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18110v1","updated":"2025-05-23T17:04:27Z","published":"2025-05-23T17:04:27Z","title":"Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM","summary":"  Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.\n","authors":["Zinuo Li","Xian Zhang","Yongxin Guo","Mohammed Bennamoun","Farid Boussaid","Girish Dwivedi","Luqi Gong","Qiuhong Ke"],"pdf_url":"https://arxiv.org/pdf/2505.18110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18105v1","updated":"2025-05-23T17:02:02Z","published":"2025-05-23T17:02:02Z","title":"ManuSearch: Democratizing Deep Search in Large Language Models with a\n  Transparent and Open Multi-Agent Framework","summary":"  Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch\n","authors":["Lisheng Huang","Yichen Liu","Jinhao Jiang","Rongxiang Zhang","Jiahao Yan","Junyi Li","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.18105v1.pdf","comment":"LLM, Complex Search Benchmark"},{"id":"http://arxiv.org/abs/2505.18102v1","updated":"2025-05-23T16:57:34Z","published":"2025-05-23T16:57:34Z","title":"How Can I Publish My LLM Benchmark Without Giving the True Answers Away?","summary":"  Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.\n","authors":["Takashi Ishida","Thanawat Lodkaew","Ikko Yamane"],"pdf_url":"https://arxiv.org/pdf/2505.18102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18098v1","updated":"2025-05-23T16:51:54Z","published":"2025-05-23T16:51:54Z","title":"Planning without Search: Refining Frontier LLMs with Offline\n  Goal-Conditioned RL","summary":"  Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.\n","authors":["Joey Hong","Anca Dragan","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2505.18098v1.pdf","comment":"18 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.18092v1","updated":"2025-05-23T16:47:00Z","published":"2025-05-23T16:47:00Z","title":"QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization","summary":"  This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59$\\times$ context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.\n","authors":["Weizhou Shen","Chenliang Li","Fanqi Wan","Shengyi Liao","Shaopeng Lai","Bo Zhang","Yingcheng Shi","Yuning Wu","Gang Fu","Zhansheng Li","Bin Yang","Ji Zhang","Fei Huang","Jingren Zhou","Ming Yan"],"pdf_url":"https://arxiv.org/pdf/2505.18092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18091v1","updated":"2025-05-23T16:46:24Z","published":"2025-05-23T16:46:24Z","title":"Data Mixing Can Induce Phase Transitions in Knowledge Acquisition","summary":"  Large Language Models (LLMs) are typically trained on data mixtures: most\ndata come from web scrapes, while a small portion is curated from high-quality\nsources with dense domain-specific knowledge. In this paper, we show that when\ntraining LLMs on such data mixtures, knowledge acquisition from knowledge-dense\ndatasets, unlike training exclusively on knowledge-dense data\n(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit\nphase transitions with respect to the mixing ratio and model size. Through\ncontrolled experiments on a synthetic biography dataset mixed with web-scraped\ndata, we demonstrate that: (1) as we increase the model size to a critical\nvalue, the model suddenly transitions from memorizing very few to most of the\nbiographies; (2) below a critical mixing ratio, the model memorizes almost\nnothing even with extensive training, but beyond this threshold, it rapidly\nmemorizes more biographies. We attribute these phase transitions to a capacity\nallocation phenomenon: a model with bounded capacity must act like a knapsack\nproblem solver to minimize the overall test loss, and the optimal allocation\nacross datasets can change discontinuously as the model size or mixing ratio\nvaries. We formalize this intuition in an information-theoretic framework and\nreveal that these phase transitions are predictable, with the critical mixing\nratio following a power-law relationship with the model size. Our findings\nhighlight a concrete case where a good mixing recipe for large models may not\nbe optimal for small models, and vice versa.\n","authors":["Xinran Gu","Kaifeng Lyu","Jiazheng Li","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.18091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18079v1","updated":"2025-05-23T16:37:36Z","published":"2025-05-23T16:37:36Z","title":"Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding","summary":"  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.\n","authors":["Xiaoyi Zhang","Zhaoyang Jia","Zongyu Guo","Jiahao Li","Bin Li","Houqiang Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2505.18079v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2504.06261v3","updated":"2025-05-23T16:36:12Z","published":"2025-04-08T17:59:41Z","title":"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention","summary":"  Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.\n","authors":["Gleb Rodionov","Roman Garipov","Alina Shutova","George Yakushev","Erik Schultheis","Vage Egiazarian","Anton Sinitsin","Denis Kuznedelev","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2504.06261v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.11704v2","updated":"2025-05-23T16:32:54Z","published":"2024-09-18T05:13:18Z","title":"From Lists to Emojis: How Format Bias Affects Model Alignment","summary":"  In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.\n","authors":["Xuanchang Zhang","Wei Xiong","Lichang Chen","Tianyi Zhou","Heng Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.11704v2.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2505.18071v1","updated":"2025-05-23T16:16:46Z","published":"2025-05-23T16:16:46Z","title":"Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals","summary":"  Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18511v2","updated":"2025-05-23T16:07:21Z","published":"2025-01-30T17:21:44Z","title":"WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training","summary":"  Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.\n","authors":["Benjamin Feuer","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2501.18511v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.16023v2","updated":"2025-05-23T16:04:18Z","published":"2025-05-21T21:13:01Z","title":"Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing\n  in the Wild","summary":"  As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment.\n","authors":["Sheshera Mysore","Debarati Das","Hancheng Cao","Bahareh Sarrafzadeh"],"pdf_url":"https://arxiv.org/pdf/2505.16023v2.pdf","comment":"Pre-print under-review"},{"id":"http://arxiv.org/abs/2410.12972v3","updated":"2025-05-23T16:03:10Z","published":"2024-10-16T19:07:37Z","title":"KCIF: Knowledge-Conditioned Instruction Following","summary":"  LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers). We evaluate\nmodels at varying parameter sizes (1B-405B) from different model families and\nfind that, surprisingly, all models report a significant drop in performance on\nsuch simple task compositions. While large-sized and frontier models report\nperformance drops of 40-50%, in small and medium sized models the drop is\nsevere (sometimes exceeding 80%). Our results highlight a limitation in the\ntraditional separation of knowledge/reasoning and instruction following, and\nsuggest that joint-study of these capabilities are important. We release our\nbenchmark dataset, evaluation framework code, and results for future work.\n","authors":["Rudra Murthy","Praveen Venkateswaran","Prince Kumar","Danish Contractor"],"pdf_url":"https://arxiv.org/pdf/2410.12972v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.18056v1","updated":"2025-05-23T15:59:39Z","published":"2025-05-23T15:59:39Z","title":"MathEDU: Towards Adaptive Feedback for Student Mathematical\n  Problem-Solving","summary":"  Online learning enhances educational accessibility, offering students the\nflexibility to learn anytime, anywhere. However, a key limitation is the lack\nof immediate, personalized feedback, particularly in helping students correct\nerrors in math problem-solving. Several studies have investigated the\napplications of large language models (LLMs) in educational contexts. In this\npaper, we explore the capabilities of LLMs to assess students' math\nproblem-solving processes and provide adaptive feedback. The MathEDU dataset is\nintroduced, comprising authentic student solutions annotated with teacher\nfeedback. We evaluate the model's ability to support personalized learning in\ntwo scenarios: one where the model has access to students' prior answer\nhistories, and another simulating a cold-start context. Experimental results\nshow that the fine-tuned model performs well in identifying correctness.\nHowever, the model still faces challenges in generating detailed feedback for\npedagogical purposes.\n","authors":["Wei-Ling Hsu","Yu-Chien Tang","An-Zi Yen"],"pdf_url":"https://arxiv.org/pdf/2505.18056v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2505.18040v1","updated":"2025-05-23T15:44:26Z","published":"2025-05-23T15:44:26Z","title":"Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot\n  Emotion Recognition","summary":"  The ability to handle various emotion labels without dedicated training is\ncrucial for building adaptable Emotion Recognition (ER) systems. Conventional\nER models rely on training using fixed label sets and struggle to generalize\nbeyond them. On the other hand, Large Language Models (LLMs) have shown strong\nzero-shot ER performance across diverse label spaces, but their scale limits\ntheir use on edge devices. In this work, we propose a contrastive distillation\nframework that transfers rich emotional knowledge from LLMs into a compact\nmodel without the use of human annotations. We use GPT-4 to generate\ndescriptive emotion annotations, offering rich supervision beyond fixed label\nsets. By aligning text samples with emotion descriptors in a shared embedding\nspace, our method enables zero-shot prediction on different emotion classes,\ngranularity, and label schema. The distilled model is effective across multiple\ndatasets and label spaces, outperforming strong baselines of similar size and\napproaching GPT-4's zero-shot performance, while being over 10,000 times\nsmaller.\n","authors":["Minxue Niu","Emily Mower Provost"],"pdf_url":"https://arxiv.org/pdf/2505.18040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04556v3","updated":"2025-05-23T15:43:17Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in AI.\nMeasuring the extent of these behaviors requires principled evaluation methods.\nWe explore a unified perspective that considers both behaviors simultaneously,\ntermed compositional causal reasoning (CCR): the ability to infer how causal\nmeasures compose and, equivalently, how causal quantities propagate through\ngraphs. We instantiate a framework for the systematic evaluation of CCR for the\naverage treatment effect and the probability of necessity and sufficiency. As\nproof of concept, we demonstrate CCR evaluation for language models in the\nLLama, Phi, and GPT families. On a math word problem, our framework revealed a\nrange of taxonomically distinct error patterns. CCR errors increased with the\ncomplexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hyk","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08590v2","updated":"2025-05-23T15:39:07Z","published":"2025-04-11T14:49:33Z","title":"Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction","summary":"  Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction.\n","authors":["Nicola Horst","Davide Mazzaccara","Antonia Schmidt","Michael Sullivan","Filippo Moment","Luca Franceschetti","Philipp Sadler","Sherzod Hakimov","Alberto Testoni","Raffaella Bernardi","Raquel Fernndez","Alexander Koller","Oliver Lemon","David Schlangen","Mario Giulianelli","Alessandro Suglia"],"pdf_url":"https://arxiv.org/pdf/2504.08590v2.pdf","comment":"Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com"},{"id":"http://arxiv.org/abs/2505.18034v1","updated":"2025-05-23T15:37:40Z","published":"2025-05-23T15:37:40Z","title":"Structured Thinking Matters: Improving LLMs Generalization in Causal\n  Inference Tasks","summary":"  Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.\n","authors":["Wentao Sun","Joao Paulo Nogueira","Alonso Silva"],"pdf_url":"https://arxiv.org/pdf/2505.18034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10440v2","updated":"2025-05-23T15:35:10Z","published":"2025-02-10T09:15:56Z","title":"Towards Copyright Protection for Knowledge Bases of Retrieval-augmented\n  Language Models via Reasoning","summary":"  Large language models (LLMs) are increasingly integrated into real-world\npersonalized applications through retrieval-augmented generation (RAG)\nmechanisms to supplement their responses with domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning or backdoor attacks. However, these\nmethods require altering the LLM's results of verification samples, inevitably\nmaking these watermarks susceptible to anomaly detection and even introducing\nnew security risks. To address these challenges, we propose \\name{} for\n`harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct yet benign verification behaviors\nin the space of chain-of-thought (CoT) reasoning, maintaining the correctness\nof the final answer. Our method has three main stages: (1) Generating CoTs: For\neach verification question, we generate two `innocent' CoTs, including a target\nCoT for building watermark behaviors; (2) Optimizing Watermark Phrases and\nTarget CoTs: Inspired by our theoretical analysis, we optimize them to minimize\nretrieval errors under the \\emph{black-box} and \\emph{text-only} setting of\nsuspicious LLM, ensuring that only watermarked verification queries can\nretrieve their correspondingly target CoTs contained in the knowledge base; (3)\nOwnership Verification: We exploit a pairwise Wilcoxon test to verify whether a\nsuspicious LLM is augmented with the protected knowledge base by comparing its\nresponses to watermarked and benign verification queries. Our experiments on\ndiverse benchmarks demonstrate that \\name{} effectively protects knowledge\nbases and its resistance to adaptive attacks.\n","authors":["Junfeng Guo","Yiming Li","Ruibo Chen","Yihan Wu","Chenxi Liu","Yanshuo Chen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.10440v2.pdf","comment":"The first two authors contributed equally to this work. 25 pages"},{"id":"http://arxiv.org/abs/2503.12908v4","updated":"2025-05-23T15:32:24Z","published":"2025-03-17T08:17:28Z","title":"HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.\n","authors":["Xinyan Jiang","Hang Ye","Yongxin Zhu","Xiaoying Zheng","Zikang Chen","Jun Gong"],"pdf_url":"https://arxiv.org/pdf/2503.12908v4.pdf","comment":"Accepted by ACL2025 findings"},{"id":"http://arxiv.org/abs/2410.15135v3","updated":"2025-05-23T15:31:01Z","published":"2024-10-19T15:25:19Z","title":"TrendFact: A Benchmark for Explainable Hotspot Perception in\n  Fact-Checking with Natural Language Explanation","summary":"  Although fact verification remains fundamental, explanation generation serves\nas a critical enabler for trustworthy fact-checking systems by producing\ninterpretable rationales and facilitating comprehensive verification processes.\nHowever, current benchmarks have limitations that include the lack of impact\nassessment, insufficient high-quality explanatory annotations, and an\nEnglish-centric bias. To address these, we introduce TrendFact, the first\nhotspot perception fact-checking benchmark that comprehensively evaluates fact\nverification, evidence retrieval, and explanation generation tasks. TrendFact\nconsists of 7,643 carefully curated samples sourced from trending platforms and\nprofessional fact-checking datasets, as well as an evidence library of 66,217\nentries with publication dates. We further propose two metrics, ECS and HCPI,\nto complement existing benchmarks by evaluating the system's explanation\nconsistency and hotspot perception capability, respectively. Experimental\nresults show that current fact-checking systems, including advanced RLMs such\nas DeepSeek-R1, face significant limitations when evaluated on TrendFact,\nhighlighting the real-world challenges posed by it. To enhance the\nfact-checking capabilities of reasoning large language models (RLMs), we\npropose FactISR, which integrates dynamic evidence augmentation, evidence\ntriangulation, and an iterative self-reflection mechanism. Accordingly, FactISR\neffectively improves RLM performance, offering new insights for explainable and\ncomplex fact-checking.\n","authors":["Xiaocheng Zhang","Xi Wang","Yifei Lu","Jianing Wang","Zhuangzhuang Ye","Mengjiao Bao","Peng Yan","Xiaohong Su"],"pdf_url":"https://arxiv.org/pdf/2410.15135v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18011v1","updated":"2025-05-23T15:14:29Z","published":"2025-05-23T15:14:29Z","title":"Training with Pseudo-Code for Instruction Following","summary":"  Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.\n","authors":["Prince Kumar","Rudra Murthy","Riyaz Bhat","Danish Contractor"],"pdf_url":"https://arxiv.org/pdf/2505.18011v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.17998v1","updated":"2025-05-23T15:03:51Z","published":"2025-05-23T15:03:51Z","title":"TRACE for Tracking the Emergence of Semantic Representations in\n  Transformers","summary":"  Modern transformer models exhibit phase transitions during training, distinct\nshifts from memorisation to abstraction, but the mechanisms underlying these\ntransitions remain poorly understood. Prior work has often focused on endpoint\nrepresentations or isolated signals like curvature or mutual information,\ntypically in symbolic or arithmetic domains, overlooking the emergence of\nlinguistic structure. We introduce TRACE (Tracking Representation Abstraction\nand Compositional Emergence), a diagnostic framework combining geometric,\ninformational, and linguistic signals to detect phase transitions in\nTransformer-based LMs. TRACE leverages a frame-semantic data generation method,\nABSynth, that produces annotated synthetic corpora with controllable\ncomplexity, lexical distributions, and structural entropy, while being fully\nannotated with linguistic categories, enabling precise analysis of abstraction\nemergence. Experiments reveal that (i) phase transitions align with clear\nintersections between curvature collapse and dimension stabilisation; (ii)\nthese geometric shifts coincide with emerging syntactic and semantic accuracy;\n(iii) abstraction patterns persist across architectural variants, with\ncomponents like feedforward networks affecting optimisation stability rather\nthan fundamentally altering trajectories. This work advances our understanding\nof how linguistic abstractions emerge in LMs, offering insights into model\ninterpretability, training efficiency, and compositional generalisation that\ncould inform more principled approaches to LM development.\n","authors":["Nura Aljaafari","Danilo S. Carvalho","Andr Freitas"],"pdf_url":"https://arxiv.org/pdf/2505.17998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17997v1","updated":"2025-05-23T15:03:41Z","published":"2025-05-23T15:03:41Z","title":"Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective","summary":"  The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.\n","authors":["Jintian Shao","Yiming Cheng","Hongyi Huang","Beiwen Zhang","Zhiyu Wu","You Shan","Mingkai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.17997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12158v2","updated":"2025-05-23T14:59:46Z","published":"2025-05-17T22:35:40Z","title":"The AI Gap: How Socioeconomic Status Affects Language Technology\n  Interactions","summary":"  Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups.\n","authors":["Elisa Bassignana","Amanda Cercas Curry","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2505.12158v2.pdf","comment":"Accepted at ACL Main 2025"},{"id":"http://arxiv.org/abs/2502.16529v2","updated":"2025-05-23T14:53:20Z","published":"2025-02-23T10:27:44Z","title":"Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual\n  Program Generation","summary":"  Visual programming languages (VPLs) allow users to create programs through\ngraphical interfaces, which results in easier accessibility and their\nwidespread usage in various domains. To further enhance this accessibility,\nrecent research has focused on generating VPL code from user instructions using\nlarge language models (LLMs). Specifically, by employing prompting-based\nmethods, these studies have shown promising results. Nevertheless, such\napproaches can be less effective for industrial VPLs such as Ladder Diagram\n(LD). LD is a pivotal language used in industrial automation processes and\ninvolves extensive domain-specific configurations, which are difficult to\ncapture in a single prompt. In this work, we demonstrate that training-based\nmethods outperform prompting-based methods for LD generation accuracy, even\nwith smaller backbone models. Building on these findings, we propose a\ntwo-stage training strategy to further enhance VPL generation. First, we employ\nretrieval-augmented fine-tuning to leverage the repetitive use of subroutines\ncommonly seen in industrial VPLs. Second, we apply direct preference\noptimization (DPO) to further guide the model toward accurate outputs, using\nsystematically generated preference pairs through graph editing operations.\nExtensive experiments on real-world LD data demonstrate that our approach\nimproves program-level accuracy by over 10% compared to supervised fine-tuning,\nwhich highlights its potential to advance industrial automation.\n","authors":["Deokhyung Kang","Jeonghun Cho","Yejin Jeon","Sunbin Jang","Minsub Lee","Jawoon Cho","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.16529v2.pdf","comment":"Accepted at ACL 2025 (Main, long paper)"},{"id":"http://arxiv.org/abs/2505.17978v1","updated":"2025-05-23T14:45:48Z","published":"2025-05-23T14:45:48Z","title":"AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims\n  with Evidence from the Web","summary":"  Textual claims are often accompanied by images to enhance their credibility\nand spread on social media, but this also raises concerns about the spread of\nmisinformation. Existing datasets for automated verification of image-text\nclaims remain limited, as they often consist of synthetic claims and lack\nevidence annotations to capture the reasoning behind the verdict. In this work,\nwe introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text\nclaims. Each claim is annotated with question-answer (QA) pairs containing\nevidence from the web, reflecting a decomposed reasoning regarding the verdict.\nWe mitigate common challenges in fact-checking datasets such as contextual\ndependence, temporal leakage, and evidence insufficiency, via claim\nnormalization, temporally constrained evidence annotation, and a two-stage\nsufficiency check. We assess the consistency of the annotation in AVerImaTeC\nvia inter-annotator studies, achieving a $\\kappa=0.742$ on verdicts and\n$74.7\\%$ consistency on QA pairs. We also propose a novel evaluation method for\nevidence retrieval and conduct extensive experiments to establish baselines for\nverifying image-text claims using open-web evidence.\n","authors":["Rui Cao","Zifeng Ding","Zhijiang Guo","Michael Schlichtkrull","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2505.17978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11960v2","updated":"2025-05-23T14:39:19Z","published":"2025-01-21T08:13:10Z","title":"TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly\n  Detection","summary":"  Text anomaly detection is crucial for identifying spam, misinformation, and\noffensive language in natural language processing tasks. Despite the growing\nadoption of embedding-based methods, their effectiveness and generalizability\nacross diverse application scenarios remain under-explored. To address this, we\npresent TAD-Bench, a comprehensive benchmark designed to systematically\nevaluate embedding-based approaches for text anomaly detection. TAD-Bench\nintegrates multiple datasets spanning different domains, combining\nstate-of-the-art embeddings from large language models with a variety of\nanomaly detection algorithms. Through extensive experiments, we analyze the\ninterplay between embeddings and detection methods, uncovering their strengths,\nweaknesses, and applicability to different tasks. These findings offer new\nperspectives on building more robust, efficient, and generalizable anomaly\ndetection systems for real-world applications.\n","authors":["Yang Cao","Sikun Yang","Chen Li","Haolong Xiang","Lianyong Qi","Bo Liu","Rongsheng Li","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2501.11960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17968v1","updated":"2025-05-23T14:37:36Z","published":"2025-05-23T14:37:36Z","title":"Are Large Language Models Reliable AI Scientists? Assessing\n  Reverse-Engineering of Black-Box Systems","summary":"  Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.\n","authors":["Jiayi Geng","Howard Chen","Dilip Arumugam","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2505.17968v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2505.17964v1","updated":"2025-05-23T14:34:40Z","published":"2025-05-23T14:34:40Z","title":"Counting Cycles with Deepseek","summary":"  Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.\n","authors":["Jiashun Jin","Tracy Ke","Bingcheng Sui","Zhenggang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17952v1","updated":"2025-05-23T14:27:37Z","published":"2025-05-23T14:27:37Z","title":"Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL","summary":"  Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.\n","authors":["Che Liu","Haozhe Wang","Jiazhen Pan","Zhongwei Wan","Yong Dai","Fangzhen Lin","Wenjia Bai","Daniel Rueckert","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2505.17952v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.17950v1","updated":"2025-05-23T14:26:33Z","published":"2025-05-23T14:26:33Z","title":"Handling Symbolic Language in Student Texts: A Comparative Study of NLP\n  Embedding Models","summary":"  Recent advancements in Natural Language Processing (NLP) have facilitated the\nanalysis of student-generated language products in learning analytics (LA),\nparticularly through the use of NLP embedding models. Yet when it comes to\nscience-related language, symbolic expressions such as equations and formulas\nintroduce challenges that current embedding models struggle to address.\nExisting studies and applications often either overlook these challenges or\nremove symbolic expressions altogether, potentially leading to biased findings\nand diminished performance of LA applications. This study therefore explores\nhow contemporary embedding models differ in their capability to process and\ninterpret science-related symbolic expressions. To this end, various embedding\nmodels are evaluated using physics-specific symbolic expressions drawn from\nauthentic student responses, with performance assessed via two approaches:\nsimilarity-based analyses and integration into a machine learning pipeline. Our\nfindings reveal significant differences in model performance, with OpenAI's\nGPT-text-embedding-3-large outperforming all other examined models, though its\nadvantage over other models was moderate rather than decisive. Beyond\nperformance, additional factors such as cost, regulatory compliance, and model\ntransparency are discussed as key considerations for model selection. Overall,\nthis study underscores the importance for LA researchers and practitioners of\ncarefully selecting NLP embedding models when working with science-related\nlanguage products that include symbolic expressions.\n","authors":["Tom Bleckmann","Paul Tschisgale"],"pdf_url":"https://arxiv.org/pdf/2505.17950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11799v3","updated":"2025-05-23T14:18:47Z","published":"2025-02-17T13:42:12Z","title":"Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning","summary":"  Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.\n","authors":["Peiying Yu","Guoxin Chen","Jingjing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.11799v3.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.17900v2","updated":"2025-05-23T14:17:49Z","published":"2025-03-23T02:03:56Z","title":"MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan\n  Generation","summary":"  Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\n\\ours{}, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality.\n","authors":["Hsin-Ling Hsu","Cong-Tinh Dao","Luning Wang","Zitao Shuai","Thao Nguyen Minh Phan","Jun-En Ding","Chun-Chieh Liao","Pengfei Hu","Xiaoxue Han","Chih-Ho Hsu","Dongsheng Luo","Wen-Chih Peng","Feng Liu","Fang-Ming Hung","Chenwei Wu"],"pdf_url":"https://arxiv.org/pdf/2503.17900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14645v2","updated":"2025-05-23T14:17:01Z","published":"2025-02-20T15:32:31Z","title":"Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs","summary":"  Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.\n","authors":["Yuchen Wu","Liang Ding","Li Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.14645v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.17936v1","updated":"2025-05-23T14:14:17Z","published":"2025-05-23T14:14:17Z","title":"Understanding Gated Neurons in Transformers from Their Input-Output\n  Functionality","summary":"  Interpretability researchers have attempted to understand MLP neurons of\nlanguage models based on both the contexts in which they activate and their\noutput weight vectors. They have paid little attention to a complementary\naspect: the interactions between input and output. For example, when neurons\ndetect a direction in the input, they might add much the same direction to the\nresidual stream (\"enrichment neurons\") or reduce its presence (\"depletion\nneurons\"). We address this aspect by examining the cosine similarity between\ninput and output weights of a neuron. We apply our method to 12 models and find\nthat enrichment neurons dominate in early-middle layers whereas later layers\ntend more towards depletion. To explain this finding, we argue that enrichment\nneurons are largely responsible for enriching concept representations, one of\nthe first steps of factual recall. Our input-output perspective is a complement\nto activation-dependent analyses and to approaches that treat input and output\nseparately.\n","authors":["Sebastian Gerstner","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2505.17936v1.pdf","comment":"31 pages, 22 figures"},{"id":"http://arxiv.org/abs/2505.17928v1","updated":"2025-05-23T14:06:26Z","published":"2025-05-23T14:06:26Z","title":"Towards Practical Defect-Focused Automated Code Review","summary":"  The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability.\n","authors":["Junyi Lu","Lili Jiang","Xiaojia Li","Jianbing Fang","Fengjun Zhang","Li Yang","Chun Zuo"],"pdf_url":"https://arxiv.org/pdf/2505.17928v1.pdf","comment":"Accepted to Forty-Second International Conference on Machine Learning\n  (ICML 2025)"},{"id":"http://arxiv.org/abs/2411.16365v4","updated":"2025-05-23T14:05:39Z","published":"2024-11-25T13:20:19Z","title":"Multi-modal Retrieval Augmented Multi-modal Generation: Datasets,\n  Evaluation Metrics and Strong Baselines","summary":"  We present a systematic investigation of Multi-modal Retrieval Augmented\nMulti-modal Generation (M$^2$RAG), a novel task that enables foundation models\nto process multi-modal web content and generate multi-modal responses, which\nexhibits better information density and readability. Despite its potential\nimpact, M$^2$RAG remains understudied, lacking comprehensive analysis and\nhigh-quality data resources. To address this gap, we establish a comprehensive\nbenchmark through a rigorous data curation pipeline, and employ text-modal\nmetrics and multi-modal metrics based on foundation models for evaluation. We\nfurther propose several strategies for foundation models to process M$^2$RAG\ntask effectively and construct a training set by filtering high-quality samples\nusing our designed metrics. Our extensive experiments demonstrate the\nreliability of our proposed metrics, a landscape of model performance within\nour designed strategies, and show that our fine-tuned 7B-8B models outperform\nthe GPT-4o model and approach the state-of-the-art OpenAI o3-mini.\nAdditionally, we perform fine-grained analyses across diverse domains and\nvalidate the effectiveness of our designs in data curation pipeline. All\nresources, including codes, datasets, and model weights, will be publicly\nreleased.\n","authors":["Zi-Ao Ma","Tian Lan","Rong-Cheng Tu","Yong Hu","Yu-Shi Zhu","Tong Zhang","Heyan Huang","Zhijing Wu","Xian-Ling Mao"],"pdf_url":"https://arxiv.org/pdf/2411.16365v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17923v1","updated":"2025-05-23T14:01:56Z","published":"2025-05-23T14:01:56Z","title":"Language models can learn implicit multi-hop reasoning, but only if they\n  have lots of training data","summary":"  Implicit reasoning is the ability of a language model to solve multi-hop\nreasoning tasks in a single forward pass, without chain of thought. We\ninvestigate this capability using GPT2-style language models trained from\nscratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that\nwhile such models can indeed learn implicit $k$-hop reasoning, the required\ntraining data grows exponentially in $k$, and the required number of\ntransformer layers grows linearly in $k$. We offer a theoretical explanation\nfor why this depth growth is necessary. We further find that the data\nrequirement can be mitigated, but not eliminated, through curriculum learning.\n","authors":["Yuekun Yao","Yupei Du","Dawei Zhu","Michael Hahn","Alexander Koller"],"pdf_url":"https://arxiv.org/pdf/2505.17923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17897v1","updated":"2025-05-23T13:44:59Z","published":"2025-05-23T13:44:59Z","title":"T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable\n  Text-to-Image Evaluation","summary":"  The rapid progress in diffusion-based text-to-image (T2I) generation has\ncreated an urgent need for interpretable automatic evaluation methods that can\nassess the quality of generated images, therefore reducing the human annotation\nburden. To reduce the prohibitive cost of relying on commercial models for\nlarge-scale evaluation, and to improve the reasoning capabilities of\nopen-source models, recent research has explored supervised fine-tuning (SFT)\nof multimodal large language models (MLLMs) as dedicated T2I evaluators.\nHowever, SFT approaches typically rely on high-quality critique datasets, which\nare either generated by proprietary LLMs-with potential issues of bias and\ninconsistency-or annotated by humans at high cost, limiting their scalability\nand generalization. To address these limitations, we propose T2I-Eval-R1, a\nnovel reinforcement learning framework that trains open-source MLLMs using only\ncoarse-grained quality scores, thereby avoiding the need for annotating\nhigh-quality interpretable evaluation rationale. Our approach integrates Group\nRelative Policy Optimization (GRPO) into the instruction-tuning process,\nenabling models to generate both scalar scores and interpretable reasoning\nchains with only easy accessible annotated judgment scores or preferences.\nFurthermore, we introduce a continuous reward formulation that encourages score\ndiversity and provides stable optimization signals, leading to more robust and\ndiscriminative evaluation behavior. Experimental results on three established\nT2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves\nsignificantly higher alignment with human assessments and offers more accurate\ninterpretable score rationales compared to strong baseline methods.\n","authors":["Zi-Ao Ma","Tian Lan","Rong-Cheng Tu","Shu-Hang Liu","Heyan Huang","Zhijing Wu","Chen Xu","Xian-Ling Mao"],"pdf_url":"https://arxiv.org/pdf/2505.17897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08498v6","updated":"2025-05-23T13:44:41Z","published":"2024-02-13T14:53:12Z","title":"\"Reasoning\" with Rhetoric: On the Style-Evidence Tradeoff in\n  LLM-Generated Counter-Arguments","summary":"  Large language models (LLMs) play a key role in generating evidence-based and\nstylistic counter-arguments, yet their effectiveness in real-world applications\nhas been underexplored. Previous research often neglects the balance between\nevidentiality and style, which are crucial for persuasive arguments. To address\nthis, we evaluated the effectiveness of stylized evidence-based\ncounter-argument generation in Counterfire, a new dataset of 38,000\ncounter-arguments generated by revising counter-arguments to Reddit's\nChangeMyView community to follow different discursive styles. We evaluated\ngeneric and stylized counter-arguments from basic and fine-tuned models such as\nGPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku,\nLLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings\nreveals that humans prefer stylized counter-arguments over the original\noutputs, with GPT-3.5 Turbo performing well, though still not reaching human\nstandards of rhetorical quality nor persuasiveness indicating a persisting\nstyle-evidence tradeoff in counter-argument generation by LLMs. We conclude\nwith an examination of ethical considerations in LLM persuasion research,\naddressing potential risks of deceptive practices and the need for transparent\ndeployment methodologies to safeguard against misuse in public discourse. The\ncode and dataset are available at\nhttps://github.com/Preetika764/Style_control/.\n","authors":["Preetika Verma","Kokil Jaidka","Svetlana Churina"],"pdf_url":"https://arxiv.org/pdf/2402.08498v6.pdf","comment":"24 pages, 9 figures, 13 tables"},{"id":"http://arxiv.org/abs/2501.06346v2","updated":"2025-05-23T13:42:27Z","published":"2025-01-10T21:18:21Z","title":"Large Language Models Share Representations of Latent Grammatical\n  Concepts Across Typologically Diverse Languages","summary":"  Human bilinguals often use similar brain regions to process multiple\nlanguages, depending on when they learned their second language and their\nproficiency. In large language models (LLMs), how are multiple languages\nlearned and encoded? In this work, we explore the extent to which LLMs share\nrepresentations of morphsyntactic concepts such as grammatical number, gender,\nand tense across languages. We train sparse autoencoders on Llama-3-8B and\nAya-23-8B, and demonstrate that abstract grammatical concepts are often encoded\nin feature directions shared across many languages. We use causal interventions\nto verify the multilingual nature of these representations; specifically, we\nshow that ablating only multilingual features decreases classifier performance\nto near-chance across languages. We then use these features to precisely modify\nmodel behavior in a machine translation task; this demonstrates both the\ngenerality and selectivity of these feature's roles in the network. Our\nfindings suggest that even models trained predominantly on English data can\ndevelop robust, cross-lingual abstractions of morphosyntactic concepts.\n","authors":["Jannik Brinkmann","Chris Wendler","Christian Bartelt","Aaron Mueller"],"pdf_url":"https://arxiv.org/pdf/2501.06346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17894v1","updated":"2025-05-23T13:42:21Z","published":"2025-05-23T13:42:21Z","title":"Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model","summary":"  We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.\n","authors":["Khalil Hennara","Muhammad Hreden","Mohamed Motaism Hamed","Zeina Aldallal","Sara Chrouf","Safwan AlModhayan"],"pdf_url":"https://arxiv.org/pdf/2505.17894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10808v2","updated":"2025-05-23T13:27:21Z","published":"2024-05-17T14:23:54Z","title":"ActiveLLM: Large Language Model-based Active Learning for Textual\n  Few-Shot Scenarios","summary":"  Active learning is designed to minimize annotation efforts by prioritizing\ninstances that most enhance learning. However, many active learning strategies\nstruggle with a `cold-start' problem, needing substantial initial data to be\neffective. This limitation reduces their utility in the increasingly relevant\nfew-shot scenarios, where the instance selection has a substantial impact. To\naddress this, we introduce ActiveLLM, a novel active learning approach that\nleverages Large Language Models such as GPT-4, o1, Llama 3, or Mistral Large\nfor selecting instances. We demonstrate that ActiveLLM significantly enhances\nthe classification performance of BERT classifiers in few-shot scenarios,\noutperforming traditional active learning methods as well as improving the\nfew-shot learning methods ADAPET, PERFECT, and SetFit. Additionally, ActiveLLM\ncan be extended to non-few-shot scenarios, allowing for iterative selections.\nIn this way, ActiveLLM can even help other active learning strategies to\novercome their cold-start problem. Our results suggest that ActiveLLM offers a\npromising solution for improving model performance across various learning\nsetups.\n","authors":["Markus Bayer","Justin Lutz","Christian Reuter"],"pdf_url":"https://arxiv.org/pdf/2405.10808v2.pdf","comment":"20 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2505.17873v1","updated":"2025-05-23T13:24:50Z","published":"2025-05-23T13:24:50Z","title":"MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback","summary":"  Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.\n","authors":["Wanhao Liu","Zonglin Yang","Jue Wang","Lidong Bing","Di Zhang","Dongzhan Zhou","Yuqiang Li","Houqiang Li","Erik Cambria","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.17873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17870v1","updated":"2025-05-23T13:20:23Z","published":"2025-05-23T13:20:23Z","title":"Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods","summary":"  Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.\n","authors":["Shaina Raza","Rizwan Qureshi","Marcelo Lotif","Aman Chadha","Deval Pandya","Christos Emmanouilidis"],"pdf_url":"https://arxiv.org/pdf/2505.17870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11171v3","updated":"2025-05-23T13:18:16Z","published":"2024-11-17T20:44:34Z","title":"LLMmlein: Compact and Competitive German-Only Language Models from\n  Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v3.pdf","comment":"camera ready;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2502.01406v2","updated":"2025-05-23T13:10:08Z","published":"2025-02-03T14:38:27Z","title":"GRADIEND: Monosemantic Feature Learning within Neural Networks Applied\n  to Gender Debiasing of Transformer Models","summary":"  AI systems frequently exhibit and amplify social biases, including gender\nbias, leading to harmful consequences in critical areas. This study introduces\na novel encoder-decoder approach that leverages model gradients to learn a\nsingle monosemantic feature neuron encoding gender information. We show that\nour method can be used to debias transformer-based language models, while\nmaintaining other capabilities. We demonstrate the effectiveness of our\napproach across various model architectures and highlight its potential for\nbroader applications.\n","authors":["Jonathan Drechsel","Steffen Herbold"],"pdf_url":"https://arxiv.org/pdf/2502.01406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17855v1","updated":"2025-05-23T13:06:43Z","published":"2025-05-23T13:06:43Z","title":"Explaining Sources of Uncertainty in Automated Fact-Checking","summary":"  Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.\n","authors":["Jingyi Sun","Greta Warren","Irina Shklovski","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2505.17855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16973v2","updated":"2025-05-23T12:49:24Z","published":"2025-05-22T17:51:25Z","title":"VeriFastScore: Speeding up long-form factuality evaluation","summary":"  Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.\n","authors":["Rishanth Rajendhran","Amir Zadeh","Matthew Sarte","Chuan Li","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2505.16973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17833v1","updated":"2025-05-23T12:47:21Z","published":"2025-05-23T12:47:21Z","title":"Investigating Affect Mining Techniques for Annotation Sample Selection\n  in the Creation of Finnish Affective Speech Corpus","summary":"  Study of affect in speech requires suitable data, as emotional expression and\nperception vary across languages. Until now, no corpus has existed for natural\nexpression of affect in spontaneous Finnish, existing data being acted or from\na very specific communicative setting. This paper presents the first such\ncorpus, created by annotating 12,000 utterances for emotional arousal and\nvalence, sampled from three large-scale Finnish speech corpora. To ensure\ndiverse affective expression, sample selection was conducted with an affect\nmining approach combining acoustic, cross-linguistic speech emotion, and text\nsentiment features. We compare this method to random sampling in terms of\nannotation diversity, and conduct post-hoc analyses to identify sampling\nchoices that would have maximized the diversity. As an outcome, the work\nintroduces a spontaneous Finnish affective speech corpus and informs sampling\nstrategies for affective speech corpus creation in other languages or domains.\n","authors":["Kalle Lahtinen","Einari Vaaras","Liisa Mustanoja","Okko Rsnen"],"pdf_url":"https://arxiv.org/pdf/2505.17833v1.pdf","comment":"Accepted for publication at Interspeech 2025, Rotterdam, The\n  Netherlands"},{"id":"http://arxiv.org/abs/2505.17832v1","updated":"2025-05-23T12:46:52Z","published":"2025-05-23T12:46:52Z","title":"Emerging categories in scientific explanations","summary":"  Clear and effective explanations are essential for human understanding and\nknowledge dissemination. The scope of scientific research aiming to understand\nthe essence of explanations has recently expanded from the social sciences to\nmachine learning and artificial intelligence. Explanations for machine learning\ndecisions must be impactful and human-like, and there is a lack of large-scale\ndatasets focusing on human-like and human-generated explanations. This work\naims to provide such a dataset by: extracting sentences that indicate\nexplanations from scientific literature among various sources in the\nbiotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access\nsubset); providing a multi-class notation derived inductively from the data;\nevaluating annotator consensus on the emerging categories. The sentences are\norganized in an openly-available dataset, with two different classifications\n(6-class and 3-class category annotation), and the 3-class notation achieves a\n0.667 Krippendorf Alpha value.\n","authors":["Giacomo Magnifico","Eduard Barbu"],"pdf_url":"https://arxiv.org/pdf/2505.17832v1.pdf","comment":"Accepted at the 3rd TRR 318 Conference: Contextualizing Explanations\n  (ContEx25), as a two-pager abstract. Will be published at BiUP (Bielefeld\n  University Press) at a later date"},{"id":"http://arxiv.org/abs/2505.17829v1","updated":"2025-05-23T12:42:50Z","published":"2025-05-23T12:42:50Z","title":"Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to\n  Enhance LLMs' Reasoning","summary":"  Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a\npowerful capability of Large Language Models (LLMs), which can be further\nenhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.\nHowever, these methods, despite improving accuracy by allocating more\ncomputational resources during inference, often suffer from path homogenization\nand inefficient use of intermediate results. To address these limitations, we\npropose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that\nintroduces checkpoints between reasoning steps. It incorporates two key\nstrategies: (1) Answer-Clustered Search, which groups reasoning paths by their\nintermediate checkpoint answers to maintain diversity while ensuring quality,\nand (2) Checkpoint Candidate Augmentation, which leverages all intermediate\nanswers for final decision-making. Our approach effectively reduces path\nhomogenization and creates a fault-tolerant mechanism by utilizing high-quality\nintermediate results. Experimental results show that SRCA improves reasoning\naccuracy compared to existing TTS methods across various mathematical datasets.\n","authors":["Zezhong Wang","Xingshan Zeng","Weiwen Liu","Yufei Wang","Liangyou Li","Yasheng Wang","Lifeng Shang","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2505.17829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17827v1","updated":"2025-05-23T12:41:29Z","published":"2025-05-23T12:41:29Z","title":"Not All Tokens Are What You Need In Thinking","summary":"  Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit\nimpressive problem-solving capabilities but suffer from critical\ninefficiencies: high inference latency, excessive computational resource\nconsumption, and a tendency toward overthinking -- generating verbose chains of\nthought (CoT) laden with redundant tokens that contribute minimally to the\nfinal answer. To address these issues, we propose Conditional Token Selection\n(CTS), a token-level compression framework with a flexible and variable\ncompression ratio that identifies and preserves only the most essential tokens\nin CoT. CTS evaluates each token's contribution to deriving correct answers\nusing conditional importance scoring, then trains models on compressed CoT.\nExtensive experiments demonstrate that CTS effectively compresses long CoT\nwhile maintaining strong reasoning performance. Notably, on the GPQA benchmark,\nQwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with\n13.2% fewer reasoning tokens (13% training token reduction). Further reducing\ntraining tokens by 42% incurs only a marginal 5% accuracy drop while yielding a\n75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy\nin existing CoT.\n","authors":["Hang Yuan","Bin Yu","Haotian Li","Shijun Yang","Christina Dan Wang","Zhou Yu","Xueyin Xu","Weizhen Qi","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2505.17827v1.pdf","comment":"11 pages, 7 figures and 3 tables"},{"id":"http://arxiv.org/abs/2505.17826v1","updated":"2025-05-23T12:41:09Z","published":"2025-05-23T12:41:09Z","title":"Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models","summary":"  Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.\n","authors":["Xuchen Pan","Yanxi Chen","Yushuo Chen","Yuchang Sun","Daoyuan Chen","Wenhao Zhang","Yuexiang Xie","Yilun Huang","Yilei Zhang","Dawei Gao","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17826v1.pdf","comment":"This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT"},{"id":"http://arxiv.org/abs/2505.17818v1","updated":"2025-05-23T12:34:48Z","published":"2025-05-23T12:34:48Z","title":"PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions","summary":"  Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.\n","authors":["Daeun Kyung","Hyunseung Chung","Seongsu Bae","Jiho Kim","Jae Ho Sohn","Taerim Kim","Soo Kyung Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2505.17818v1.pdf","comment":"9 pages for main text, 4 pages for references, 27 pages for\n  supplementary materials"},{"id":"http://arxiv.org/abs/2502.11614v2","updated":"2025-05-23T12:32:36Z","published":"2025-02-17T09:56:46Z","title":"Is Human-Like Text Liked by Humans? Multilingual Human Detection and\n  Preference Against AI","summary":"  Prior studies have shown that distinguishing text generated by large language\nmodels (LLMs) from human-written one is highly challenging, and often no better\nthan random guessing. To verify the generalizability of this finding across\nlanguages and domains, we perform an extensive case study to identify the upper\nbound of human detection accuracy. Across 16 datasets covering 9 languages and\n9 domains, 19 annotators achieved an average detection accuracy of 87.6\\%, thus\nchallenging previous conclusions. We find that major gaps between human and\nmachine text lie in concreteness, cultural nuances, and diversity. Prompting by\nexplicitly explaining the distinctions in the prompts can partially bridge the\ngaps in over 50\\% of the cases. However, we also find that humans do not always\nprefer human-written text, particularly when they cannot clearly identify its\nsource.\n","authors":["Yuxia Wang","Rui Xing","Jonibek Mansurov","Giovanni Puccetti","Zhuohan Xie","Minh Ngoc Ta","Jiahui Geng","Jinyan Su","Mervat Abassy","Saad El Dine Ahmed","Kareem Elozeiri","Nurkhan Laiyk","Maiya Goloburda","Tarek Mahmoud","Raj Vardhan Tomar","Alexander Aziz","Ryuto Koike","Masahiro Kaneko","Artem Shelmanov","Ekaterina Artemova","Vladislav Mikhailov","Akim Tsvigun","Alham Fikri Aji","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2502.11614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17816v1","updated":"2025-05-23T12:32:01Z","published":"2025-05-23T12:32:01Z","title":"Low-Resource NMT: A Case Study on the Written and Spoken Languages in\n  Hong Kong","summary":"  The majority of inhabitants in Hong Kong are able to read and write in\nstandard Chinese but use Cantonese as the primary spoken language in daily\nlife. Spoken Cantonese can be transcribed into Chinese characters, which\nconstitute the so-called written Cantonese. Written Cantonese exhibits\nsignificant lexical and grammatical differences from standard written Chinese.\nThe rise of written Cantonese is increasingly evident in the cyber world. The\ngrowing interaction between Mandarin speakers and Cantonese speakers is leading\nto a clear demand for automatic translation between Chinese and Cantonese. This\npaper describes a transformer-based neural machine translation (NMT) system for\nwritten-Chinese-to-written-Cantonese translation. Given that parallel text data\nof Chinese and Cantonese are extremely scarce, a major focus of this study is\non the effort of preparing good amount of training data for NMT. In addition to\ncollecting 28K parallel sentences from previous linguistic studies and\nscattered internet resources, we devise an effective approach to obtaining 72K\nparallel sentences by automatically extracting pairs of semantically similar\nsentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.\nWe show that leveraging highly similar sentence pairs mined from Wikipedia\nimproves translation performance in all test sets. Our system outperforms Baidu\nFanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU\nscores. Translation examples reveal that our system is able to capture\nimportant linguistic transformations between standard Chinese and spoken\nCantonese.\n","authors":["Hei Yi Mak","Tan Lee"],"pdf_url":"https://arxiv.org/pdf/2505.17816v1.pdf","comment":"Proceedings of the 2021 5th International Conference on Natural\n  Language Processing and Information Retrieval"},{"id":"http://arxiv.org/abs/2505.17813v1","updated":"2025-05-23T12:29:06Z","published":"2025-05-23T12:29:06Z","title":"Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning","summary":"  Reasoning large language models (LLMs) heavily rely on scaling test-time\ncompute to perform complex reasoning tasks by generating extensive \"thinking\"\nchains. While demonstrating impressive results, this approach incurs\nsignificant computational costs and inference time. In this work, we challenge\nthe assumption that long thinking chains results in better reasoning\ncapabilities. We first demonstrate that shorter reasoning chains within\nindividual questions are significantly more likely to yield correct answers -\nup to 34.5% more accurate than the longest chain sampled for the same question.\nBased on these results, we suggest short-m@k, a novel reasoning LLM inference\nmethod. Our method executes k independent generations in parallel and halts\ncomputation once the first m thinking processes are done. The final answer is\nchosen using majority voting among these m chains. Basic short-1@k demonstrates\nsimilar or even superior performance over standard majority voting in\nlow-compute settings - using up to 40% fewer thinking tokens. short-3@k, while\nslightly less efficient than short-1@k, consistently surpasses majority voting\nacross all compute budgets, while still being substantially faster (up to 33%\nwall time reduction). Inspired by our results, we finetune an LLM using short,\nlong, and randomly selected reasoning chains. We then observe that training on\nthe shorter ones leads to better performance. Our findings suggest rethinking\ncurrent methods of test-time compute in reasoning LLMs, emphasizing that longer\n\"thinking\" does not necessarily translate to improved performance and can,\ncounter-intuitively, lead to degraded results.\n","authors":["Michael Hassid","Gabriel Synnaeve","Yossi Adi","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2505.17813v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.14359v2","updated":"2025-05-23T12:25:46Z","published":"2025-02-20T08:36:58Z","title":"Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests","summary":"  We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs.\n","authors":["Filippo Moment","Alessandro Suglia","Mario Giulianelli","Ambra Ferrari","Alexander Koller","Oliver Lemon","David Schlangen","Raquel Fernndez","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2502.14359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17673v2","updated":"2025-05-23T12:14:48Z","published":"2024-09-26T09:32:12Z","title":"Cross-lingual Human-Preference Alignment for Neural Machine Translation\n  with Direct Quality Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) and derivative techniques\nlike Direct Preference Optimization (DPO) are task-alignment algorithms used to\nrepurpose general, foundational models for specific tasks. We show that\napplying task-alignment to neural machine translation (NMT) addresses an\nexisting task--data mismatch in NMT, leading to improvements across all\nlanguages of a multilingual model, even when task-alignment is only applied to\na subset of those languages. We do so by introducing Direct Quality\nOptimization (DQO), a variant of DPO leveraging a pre-trained translation\nquality estimation model as a proxy for human preferences, and verify the\nimprovements with both automatic metrics and human evaluation.\n","authors":["Kaden Uhlig","Joern Wuebker","Raphael Reinauer","John DeNero"],"pdf_url":"https://arxiv.org/pdf/2409.17673v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.17795v1","updated":"2025-05-23T12:12:40Z","published":"2025-05-23T12:12:40Z","title":"DialogXpert: Driving Intelligent and Emotion-Aware Conversations through\n  Online Value-Based Reinforcement Learning with LLM Priors","summary":"  Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/\n","authors":["Tazeek Bin Abdur Rakib","Ambuj Mehrish","Lay-Ki Soon","Wern Han Lim","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2505.17795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17793v1","updated":"2025-05-23T12:11:03Z","published":"2025-05-23T12:11:03Z","title":"Compression Hacking: A Supplementary Perspective on Informatics Metric\n  of Language Models from Geometric Distortion","summary":"  Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.\n","authors":["Jianxiang Zang","Meiling Ning","Yongda Wei","Shihan Dou","Jiazheng Zhang","Nijia Mo","Binhong Li","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.17793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15684v2","updated":"2025-05-23T11:59:22Z","published":"2025-05-21T15:58:16Z","title":"ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy","summary":"  While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.\n","authors":["Gengyang Li","Yifeng Gao","Yuming Li","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2505.15684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17784v1","updated":"2025-05-23T11:56:48Z","published":"2025-05-23T11:56:48Z","title":"EXECUTE: A Multilingual Benchmark for LLM Token Understanding","summary":"  The CUTE benchmark showed that LLMs struggle with character understanding in\nEnglish. We extend it to more languages with diverse scripts and writing\nsystems, introducing EXECUTE. Our simplified framework allows easy expansion to\nany language. Tests across multiple LLMs reveal that challenges in other\nlanguages are not always on the character level as in English. Some languages\nshow word-level processing issues, some show no issues at all. We also examine\nsub-character tasks in Chinese, Japanese, and Korean to assess LLMs'\nunderstanding of character components.\n","authors":["Lukas Edman","Helmut Schmid","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2505.17784v1.pdf","comment":"Accepted to Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2505.17767v1","updated":"2025-05-23T11:40:58Z","published":"2025-05-23T11:40:58Z","title":"The Real Barrier to LLM Agent Usability is Agentic ROI","summary":"  Large Language Model (LLM) agents represent a promising shift in human-AI\ninteraction, moving beyond passive prompt-response systems to autonomous agents\ncapable of reasoning, planning, and goal-directed action. Despite the\nwidespread application in specialized, high-effort tasks like coding and\nscientific research, we highlight a critical usability gap in high-demand,\nmass-market applications. This position paper argues that the limited\nreal-world adoption of LLM agents stems not only from gaps in model\ncapabilities, but also from a fundamental tradeoff between the value an agent\ncan provide and the costs incurred during real-world use. Hence, we call for a\nshift from solely optimizing model performance to a broader, utility-driven\nperspective: evaluating agents through the lens of the overall agentic return\non investment (Agent ROI). By identifying key factors that determine Agentic\nROI--information quality, agent time, and cost--we posit a zigzag development\ntrajectory in optimizing agentic ROI: first scaling up to improve the\ninformation quality, then scaling down to minimize the time and cost. We\noutline the roadmap across different development stages to bridge the current\nusability gaps, aiming to make LLM agents truly scalable, accessible, and\neffective in real-world contexts.\n","authors":["Weiwen Liu","Jiarui Qin","Xu Huang","Xingshan Zeng","Yunjia Xi","Jianghao Lin","Chuhan Wu","Yasheng Wang","Lifeng Shang","Ruiming Tang","Defu Lian","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.17767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17762v1","updated":"2025-05-23T11:35:03Z","published":"2025-05-23T11:35:03Z","title":"Resolving Conflicting Evidence in Automated Fact-Checking: A Study on\n  Retrieval-Augmented LLMs","summary":"  Large Language Models (LLMs) augmented with retrieval mechanisms have\ndemonstrated significant potential in fact-checking tasks by integrating\nexternal knowledge. However, their reliability decreases when confronted with\nconflicting evidence from sources of varying credibility. This paper presents\nthe first systematic evaluation of Retrieval-Augmented Generation (RAG) models\nfor fact-checking in the presence of conflicting evidence. To support this\nstudy, we introduce \\textbf{CONFACT} (\\textbf{Con}flicting Evidence for\n\\textbf{Fact}-Checking) (Dataset available at\nhttps://github.com/zoeyyes/CONFACT), a novel dataset comprising questions\npaired with conflicting information from various sources. Extensive experiments\nreveal critical vulnerabilities in state-of-the-art RAG methods, particularly\nin resolving conflicts stemming from differences in media source credibility.\nTo address these challenges, we investigate strategies to integrate media\nbackground information into both the retrieval and generation stages. Our\nresults show that effectively incorporating source credibility significantly\nenhances the ability of RAG models to resolve conflicting evidence and improve\nfact-checking performance.\n","authors":["Ziyu Ge","Yuhao Wu","Daniel Wai Kit Chin","Roy Ka-Wei Lee","Rui Cao"],"pdf_url":"https://arxiv.org/pdf/2505.17762v1.pdf","comment":"Camera-ready for IJCAI 2025, AI and Social Good"},{"id":"http://arxiv.org/abs/2502.12896v4","updated":"2025-05-23T11:27:49Z","published":"2025-02-18T14:32:44Z","title":"None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks","summary":"  In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.\n","authors":["Eva Snchez Salido","Julio Gonzalo","Guillermo Marco"],"pdf_url":"https://arxiv.org/pdf/2502.12896v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16900v2","updated":"2025-05-23T11:27:02Z","published":"2025-05-22T16:59:26Z","title":"Power-Law Decay Loss for Large Language Model Finetuning: Focusing on\n  Information Sparsity to Enhance Generation Quality","summary":"  During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer.\n","authors":["Jintian Shao","Yiming Cheng","Hongyi Huang","Jiayi Wu","Beiwen Zhang","Zhiyu Wu","You Shan","Mingkai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.16900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17747v1","updated":"2025-05-23T11:14:27Z","published":"2025-05-23T11:14:27Z","title":"Discriminating Form and Meaning in Multilingual Models with Minimal-Pair\n  ABX Tasks","summary":"  We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations.\n","authors":["Maureen de Seyssel","Jie Chi","Skyler Seto","Maartje ter Hoeve","Masha Fedzechkina","Natalie Schluter"],"pdf_url":"https://arxiv.org/pdf/2505.17747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17746v1","updated":"2025-05-23T11:14:12Z","published":"2025-05-23T11:14:12Z","title":"Fast Quiet-STaR: Thinking Without Thought Tokens","summary":"  Large Language Models (LLMs) have achieved impressive performance across a\nrange of natural language processing tasks. However, recent advances\ndemonstrate that further gains particularly in complex reasoning tasks require\nmore than merely scaling up model sizes or training data. One promising\ndirection is to enable models to think during the reasoning process. Recently,\nQuiet STaR significantly improves reasoning by generating token-level thought\ntraces, but incurs substantial inference overhead. In this work, we propose\nFast Quiet STaR, a more efficient reasoning framework that preserves the\nbenefits of token-level reasoning while reducing computational cost. Our method\nintroduces a curriculum learning based training strategy that gradually reduces\nthe number of thought tokens, enabling the model to internalize more abstract\nand concise reasoning processes. We further extend this approach to the\nstandard Next Token Prediction (NTP) setting through reinforcement\nlearning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates\nthe need for explicit thought token generation during inference. Experiments on\nfour benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast\nQuiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy\nunder the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an\naverage accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B,\nwhile maintaining the same inference latency. Our code will be available at\nhttps://github.com/huangwei200012/Fast-Quiet-STaR.\n","authors":["Wei Huang","Yizhe Xiong","Xin Ye","Zhijie Deng","Hui Chen","Zijia Lin","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2505.17746v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.17733v1","updated":"2025-05-23T10:53:00Z","published":"2025-05-23T10:53:00Z","title":"The Pilot Corpus of the English Semantic Sketches","summary":"  The paper is devoted to the creation of the semantic sketches for English\nverbs. The pilot corpus consists of the English-Russian sketch pairs and is\naimed to show what kind of contrastive studies the sketches help to conduct.\nSpecial attention is paid to the cross-language differences between the\nsketches with similar semantics. Moreover, we discuss the process of building a\nsemantic sketch, and analyse the mistakes that could give insight to the\nlinguistic nature of sketches.\n","authors":["Maria Petrova","Maria Ponomareva","Alexandra Ivoylova"],"pdf_url":"https://arxiv.org/pdf/2505.17733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20556v2","updated":"2025-05-23T10:40:23Z","published":"2025-03-26T13:54:16Z","title":"A Retrieval-Based Approach to Medical Procedure Matching in Romanian","summary":"  Accurately mapping medical procedure names from healthcare providers to\nstandardized terminology used by insurance companies is a crucial yet complex\ntask. Inconsistencies in naming conventions lead to missclasified procedures,\ncausing administrative inefficiencies and insurance claim problems in private\nhealthcare settings. Many companies still use human resources for manual\nmapping, while there is a clear opportunity for automation. This paper proposes\na retrieval-based architecture leveraging sentence embeddings for medical name\nmatching in the Romanian healthcare system. This challenge is significantly\nmore difficult in underrepresented languages such as Romanian, where existing\npretrained language models lack domain-specific adaptation to medical text. We\nevaluate multiple embedding models, including Romanian, multilingual, and\nmedical-domain-specific representations, to identify the most effective\nsolution for this task. Our findings contribute to the broader field of medical\nNLP for low-resource languages such as Romanian.\n","authors":["Andrei Niculae","Adrian Cosma","Emilian Radoi"],"pdf_url":"https://arxiv.org/pdf/2503.20556v2.pdf","comment":"Accepted at BIONLP 2025 and Shared Tasks, ACL 2025"},{"id":"http://arxiv.org/abs/2409.10999v2","updated":"2025-05-23T10:37:01Z","published":"2024-09-17T09:04:03Z","title":"Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models","summary":"  Audio language models process audio inputs using textual prompts for tasks\nlike speech recognition and audio captioning. Although built on multilingual\npre-trained components, most are trained primarily on English, limiting their\nusability for other languages. This paper evaluates audio language models on\nThai, a low-resource language, and finds that they lack emergent cross-lingual\nabilities despite their multilingual foundations. To address this, we explore\ndata mixtures that optimize audio language models for both a target language\nand English while integrating audio comprehension and speech\ninstruction-following into a unified model. Our experiments provide insights\ninto improving instruction-following in low-resource languages by balancing\nlanguage-specific and multilingual training data. The proposed model,\nTyphoon-Audio, significantly outperforms existing open-source models and\nachieves performance comparable to state-of-the-art Gemini-1.5-Pro in both\nEnglish and Thai.\n","authors":["Potsawee Manakul","Guangzhi Sun","Warit Sirichotedumrong","Kasima Tharnpipitchai","Kunat Pipatanakul"],"pdf_url":"https://arxiv.org/pdf/2409.10999v2.pdf","comment":"Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.17714v1","updated":"2025-05-23T10:30:58Z","published":"2025-05-23T10:30:58Z","title":"PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy\n  Optimization","summary":"  Despite Proximal Policy Optimization (PPO) dominating policy gradient methods\n-- from robotic control to game AI -- its static trust region forces a brittle\ntrade-off: aggressive clipping stifles early exploration, while late-stage\nupdates destabilize convergence. PPO-BR establishes a new paradigm in adaptive\nRL by fusing exploration and convergence signals into a single bounded trust\nregion -- a theoretically grounded innovation that outperforms five SOTA\nbaselines with less than 2% overhead. This work bridges a critical gap in\nphase-aware learning, enabling real-world deployment in safety-critical systems\nlike robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%\nfaster convergence by combining: (1) entropy-driven expansion (epsilon up) for\nexploration in high-uncertainty states, and (2) reward-guided contraction\n(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,\nAtari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),\n2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with\nonly five lines of code change. PPO-BR's simplicity and theoretical guarantees\nmake it ready-to-deploy in safety-critical domains -- from surgical robotics to\nautonomous drones. In contrast to recent methods such as Group Relative Policy\nOptimization (GRPO), PPO-BR offers a unified entropy-reward mechanism\napplicable to both language models and general reinforcement learning\nenvironments.\n","authors":["Ben Rahman"],"pdf_url":"https://arxiv.org/pdf/2505.17714v1.pdf","comment":"This manuscript builds upon an earlier version posted to TechRxiv.\n  This arXiv version includes an updated comparison with GRPO (Group Relative\n  Policy Optimization)"},{"id":"http://arxiv.org/abs/2505.17712v1","updated":"2025-05-23T10:30:09Z","published":"2025-05-23T10:30:09Z","title":"Understanding How Value Neurons Shape the Generation of Specified Values\n  in LLMs","summary":"  Rapid integration of large language models (LLMs) into societal applications\nhas intensified concerns about their alignment with universal ethical\nprinciples, as their internal value representations remain opaque despite\nbehavioral alignment advancements. Current approaches struggle to\nsystematically interpret how values are encoded in neural architectures,\nlimited by datasets that prioritize superficial judgments over mechanistic\nanalysis. We introduce ValueLocate, a mechanistic interpretability framework\ngrounded in the Schwartz Values Survey, to address this gap. Our method first\nconstructs ValueInsight, a dataset that operationalizes four dimensions of\nuniversal value through behavioral contexts in the real world. Leveraging this\ndataset, we develop a neuron identification method that calculates activation\ndifferences between opposing value aspects, enabling precise localization of\nvalue-critical neurons without relying on computationally intensive attribution\nmethods. Our proposed validation method demonstrates that targeted manipulation\nof these neurons effectively alters model value orientations, establishing\ncausal relationships between neurons and value representations. This work\nadvances the foundation for value alignment by bridging psychological value\nframeworks with neuron analysis in LLMs.\n","authors":["Yi Su","Jiayi Zhang","Shu Yang","Xinhai Wang","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13202v2","updated":"2025-05-23T10:27:36Z","published":"2025-04-13T15:49:41Z","title":"The Quantum LLM: Modeling Semantic Spaces with Quantum Principles","summary":"  In the previous article, we presented a quantum-inspired framework for\nmodeling semantic representation and processing in Large Language Models\n(LLMs), drawing upon mathematical tools and conceptual analogies from quantum\nmechanics to offer a new perspective on these complex systems. In this paper,\nwe clarify the core assumptions of this model, providing a detailed exposition\nof six key principles that govern semantic representation, interaction, and\ndynamics within LLMs. The goal is to justify that a quantum-inspired framework\nis a valid approach to studying semantic spaces. This framework offers valuable\ninsights into their information processing and response generation, and we\nfurther discuss the potential of leveraging quantum computing to develop\nsignificantly more powerful and efficient LLMs based on these principles.\n","authors":["Timo Aukusti Laine"],"pdf_url":"https://arxiv.org/pdf/2504.13202v2.pdf","comment":"16 pages, 6 figures. Some corrections"},{"id":"http://arxiv.org/abs/2505.15872v2","updated":"2025-05-23T10:16:01Z","published":"2025-05-21T14:44:40Z","title":"InfoDeepSeek: Benchmarking Agentic Information Seeking for\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research.\n","authors":["Yunjia Xi","Jianghao Lin","Menghui Zhu","Yongzhao Xiao","Zhuoying Ou","Jiaqi Liu","Tong Wan","Bo Chen","Weiwen Liu","Yasheng Wang","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.15872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17704v1","updated":"2025-05-23T10:15:22Z","published":"2025-05-23T10:15:22Z","title":"SemSketches-2021: experimenting with the machine processing of the pilot\n  semantic sketches corpus","summary":"  The paper deals with elaborating different approaches to the machine\nprocessing of semantic sketches. It presents the pilot open corpus of semantic\nsketches. Different aspects of creating the sketches are discussed, as well as\nthe tasks that the sketches can help to solve. Special attention is paid to the\ncreation of the machine processing tools for the corpus. For this purpose, the\nSemSketches-2021 Shared Task was organized. The participants were given the\nanonymous sketches and a set of contexts containing the necessary predicates.\nDuring the Task, one had to assign the proper contexts to the corresponding\nsketches.\n","authors":["Maria Ponomareva","Maria Petrova","Julia Detkova","Oleg Serikov","Maria Yarova"],"pdf_url":"https://arxiv.org/pdf/2505.17704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07199v3","updated":"2025-05-23T10:14:35Z","published":"2025-04-09T18:26:46Z","title":"SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog","summary":"  We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.\n","authors":["Jennifer D'Souza","Sameer Sadruddin","Holger Israel","Mathias Begoin","Diana Slawig"],"pdf_url":"https://arxiv.org/pdf/2504.07199v3.pdf","comment":"10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper"},{"id":"http://arxiv.org/abs/2505.12345v2","updated":"2025-05-23T10:13:05Z","published":"2025-05-18T10:19:01Z","title":"UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models","summary":"  Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors.\n","authors":["Qizhou Chen","Dakan Wang","Taolin Zhang","Zaoming Yan","Chengsong You","Chengyu Wang","Xiaofeng He"],"pdf_url":"https://arxiv.org/pdf/2505.12345v2.pdf","comment":"UniEdit Dataset: https://huggingface.co/datasets/qizhou/UniEdit Code:\n  https://github.com/qizhou000/UniEdit"},{"id":"http://arxiv.org/abs/2505.17701v1","updated":"2025-05-23T10:10:22Z","published":"2025-05-23T10:10:22Z","title":"COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary\n  Weights in Down Projection","summary":"  The growing size of large language models has created significant\ncomputational inefficiencies. To address this challenge, sparse activation\nmethods selectively deactivates non-essential parameters during inference,\nreducing computational costs in FFNN layers. While existing methods focus on\nnon-linear gating mechanisms, we hypothesize that the sparsity of the FFNN\nlayer lies globally in the form of a linear combination over its internal down\nprojection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,\nleveraging indirect coefficients, and D-COUNTDOWN, utilizing direct\ncoefficients of the linear combination. Experimental results demonstrate that\nD-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%\nideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%\nbetter performance preservation compared to existing methods. Our specialized\nkernel implementations effectively realize these theoretical gains into\nsubstantial real-world acceleration.\n","authors":["Jaewon Cheon","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2505.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11789v2","updated":"2025-05-23T10:09:17Z","published":"2025-02-17T13:28:14Z","title":"Personality Editing for Language Models through Relevant Knowledge\n  Editing","summary":"  Large Language Models (LLMs) play a vital role in applications like\nconversational agents and content creation, where controlling a model's\npersonality is crucial for maintaining tone, consistency, and engagement.\nHowever, traditional prompt-based techniques for controlling personality often\nfall short, as they do not effectively mitigate the model's inherent biases. In\nthis paper, we introduce a novel method PALETTE that enhances personality\ncontrol through knowledge editing. By generating adjustment queries inspired by\npsychological assessments, our approach systematically adjusts responses to\npersonality-related queries similar to modifying factual knowledge, thereby\nachieving controlled shifts in personality traits. Experimental results from\nboth automatic and human evaluations demonstrate that our method enables more\nstable and well-balanced personality control in LLMs.\n","authors":["Seojin Hwang","Yumin Kim","Byeongjeong Kim","Donghoon Shin","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2502.11789v2.pdf","comment":"19 pages, 3 figures, 24 tables"},{"id":"http://arxiv.org/abs/2505.17697v1","updated":"2025-05-23T10:07:18Z","published":"2025-05-23T10:07:18Z","title":"Activation Control for Efficiently Eliciting Long Chain-of-thought\n  Ability of Language Models","summary":"  Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.\n","authors":["Zekai Zhao","Qi Liu","Kun Zhou","Zihan Liu","Yifei Shao","Zhiting Hu","Biwei Huang"],"pdf_url":"https://arxiv.org/pdf/2505.17697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02536v3","updated":"2025-05-23T10:01:57Z","published":"2024-06-04T17:55:38Z","title":"Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension","summary":"  Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.\n","authors":["Yijiong Yu","Huiqiang Jiang","Xufang Luo","Qianhui Wu","Chin-Yew Lin","Dongsheng Li","Yuqing Yang","Yongfeng Huang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2406.02536v3.pdf","comment":"Accepted at Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2505.17691v1","updated":"2025-05-23T10:00:03Z","published":"2025-05-23T10:00:03Z","title":"ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive\n  Preferences via Tournament Graph Reconstruction","summary":"  Large language models (LLMs) are widely used as evaluators for open-ended\ntasks, while previous research has emphasized biases in LLM evaluations, the\nissue of non-transitivity in pairwise comparisons remains unresolved:\nnon-transitive preferences for pairwise comparisons, where evaluators prefer A\nover B, B over C, but C over A. Our results suggest that low-quality training\ndata may reduce the transitivity of preferences generated by the Evaluator LLM.\nTo address this, We propose a graph-theoretic framework to analyze and mitigate\nthis problem by modeling pairwise preferences as tournament graphs. We quantify\nnon-transitivity and introduce directed graph structural entropy to measure the\noverall clarity of preferences. Our analysis reveals significant\nnon-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting\n67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting\nlow overall clarity of preferences. To address this issue, we designed a\nfiltering strategy, ELSPR, to eliminate preference data that induces\nnon-transitivity, retaining only consistent and transitive preference data for\nmodel fine-tuning. Experiments demonstrate that models fine-tuned with filtered\ndata reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease\nstructural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely\nwith human evaluators (human agreement rate improves by 0.6% and Spearman\ncorrelation increases by 0.01).\n","authors":["Yan Yu","Yilun Liu","Minggui He","Shimin Tao","Weibin Meng","Xinhua Yang","Li Zhang","Hongxia Ma","Chang Su","Hao Yang","Fuliang Li"],"pdf_url":"https://arxiv.org/pdf/2505.17691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11274v2","updated":"2025-05-23T09:59:36Z","published":"2024-09-17T15:25:11Z","title":"Task Arithmetic for Language Expansion in Speech Translation","summary":"  Recent progress in large language models (LLMs) has gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-tuned speech translation (ST). However, expanding language pairs is\ncostly due to re-training on combined new and previous datasets. To address\nthis, we aim to build a one-to-many ST system from existing one-to-one ST\nsystems using task arithmetic without re-training. Direct application of task\narithmetic in ST leads to language confusion; therefore, we introduce an\naugmented task arithmetic method incorporating a language control model to\nensure correct target language generation. Our experiments on MuST-C and\nCoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains\nof 8.87 and 11.83. In addition, we demonstrate our framework can extend to\nlanguage pairs lacking paired ST training data or pre-trained ST models by\nsynthesizing ST models based on existing machine translation (MT) and ST models\nvia task analogies.\n","authors":["Yao-Fei Cheng","Hayato Futami","Yosuke Kashiwagi","Emiru Tsunoo","Wen Shen Teo","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2409.11274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14522v2","updated":"2025-05-23T09:57:41Z","published":"2024-05-23T13:03:26Z","title":"Explaining Black-box Model Predictions via Two-level Nested Feature\n  Attributions with Consistency Property","summary":"  Techniques that explain the predictions of black-box machine learning models\nare crucial to make the models transparent, thereby increasing trust in AI\nsystems. The input features to the models often have a nested structure that\nconsists of high- and low-level features, and each high-level feature is\ndecomposed into multiple low-level features. For such inputs, both high-level\nfeature attributions (HiFAs) and low-level feature attributions (LoFAs) are\nimportant for better understanding the model's decision. In this paper, we\npropose a model-agnostic local explanation method that effectively exploits the\nnested structure of the input to estimate the two-level feature attributions\nsimultaneously. A key idea of the proposed method is to introduce the\nconsistency property that should exist between the HiFAs and LoFAs, thereby\nbridging the separate optimization problems for estimating them. Thanks to this\nconsistency property, the proposed method can produce HiFAs and LoFAs that are\nboth faithful to the black-box models and consistent with each other, using a\nsmaller number of queries to the models. In experiments on image classification\nin multiple instance learning and text classification using language models, we\ndemonstrate that the HiFAs and LoFAs estimated by the proposed method are\naccurate, faithful to the behaviors of the black-box models, and provide\nconsistent explanations.\n","authors":["Yuya Yoshikawa","Masanari Kimura","Ryotaro Shimizu","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2405.14522v2.pdf","comment":"This manuscript is an extended version of our paper accepted at\n  IJCAI2025, with detailed proofs and additional experimental results"},{"id":"http://arxiv.org/abs/2505.16160v2","updated":"2025-05-23T09:54:10Z","published":"2025-05-22T03:01:28Z","title":"EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large\n  Language Models in Diverse Educational Scenarios","summary":"  As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.\n","authors":["Bin Xu","Yu Bai","Huashan Sun","Yiguan Lin","Siming Liu","Xinyue Liang","Yaolin Li","Yang Gao","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.16160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04694v2","updated":"2025-05-23T09:53:45Z","published":"2025-01-08T18:58:15Z","title":"EpiCoder: Encompassing Diversity and Complexity in Code Generation","summary":"  Existing methods for code generation use code snippets as seed data,\nrestricting the complexity and diversity of the synthesized data. In this\npaper, we introduce a novel feature tree-based synthesis framework, which\nrevolves around hierarchical code features derived from high-level abstractions\nof code. The feature tree is constructed from raw data and refined iteratively\nto increase the quantity and diversity of the extracted features, which\ncaptures and recognizes more complex patterns and relationships within the\ncode. By adjusting the depth and breadth of the sampled subtrees, our framework\nprovides precise control over the complexity of the generated code, enabling\nfunctionalities that range from function-level operations to multi-file\nscenarios. We fine-tuned widely-used base models to obtain EpiCoder series,\nachieving state-of-the-art performance on multiple benchmarks at both the\nfunction and file levels. In particular, empirical evidence indicates that our\napproach shows significant potential in the synthesizing of repository-level\ncode data. Our code and data are publicly available at\nhttps://github.com/microsoft/EpiCoder.\n","authors":["Yaoxiang Wang","Haoling Li","Xin Zhang","Jie Wu","Xiao Liu","Wenxiang Hu","Zhongxin Guo","Yangyu Huang","Ying Xin","Yujiu Yang","Jinsong Su","Qi Chen","Scarlett Li"],"pdf_url":"https://arxiv.org/pdf/2501.04694v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.17682v1","updated":"2025-05-23T09:53:43Z","published":"2025-05-23T09:53:43Z","title":"Tuning Language Models for Robust Prediction of Diverse User Behaviors","summary":"  Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples.\n","authors":["Fanjin Meng","Jingtao Ding","Jiahui Gong","Chen Yang","Hong Chen","Zuojian Wang","Haisheng Lu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.17682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17116v2","updated":"2025-05-23T09:44:25Z","published":"2025-01-28T18:04:50Z","title":"Optimizing Large Language Model Training Using FP4 Quantization","summary":"  The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.\n","authors":["Ruizhe Wang","Yeyun Gong","Xiao Liu","Guoshuai Zhao","Ziyue Yang","Baining Guo","Zhengjun Zha","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.17116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02623v3","updated":"2025-05-23T09:40:34Z","published":"2025-03-04T13:48:50Z","title":"Rewarding Doubt: A Reinforcement Learning Approach to Calibrated\n  Confidence Expression of Large Language Models","summary":"  A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We propose a novel\nReinforcement Learning approach that allows to directly fine-tune LLMs to\nexpress calibrated confidence estimates alongside their answers to factual\nquestions. Our method optimizes a reward based on the logarithmic scoring rule,\nexplicitly penalizing both over- and under-confidence. This encourages the\nmodel to align its confidence estimates with the actual predictive accuracy.\nThe optimal policy under our reward design would result in perfectly calibrated\nconfidence expressions. Unlike prior approaches that decouple confidence\nestimation from response generation, our method integrates confidence\ncalibration seamlessly into the generative process of the LLM. Empirically, we\ndemonstrate that models trained with our approach exhibit substantially\nimproved calibration and generalize to unseen tasks without further\nfine-tuning, suggesting the emergence of general confidence awareness. We\nprovide our training and evaluation code in the supplementary and will make it\npublicly available upon acceptance.\n","authors":["Paul Stangel","David Bani-Harouni","Chantal Pellegrini","Ege zsoy","Kamilia Zaripova","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2503.02623v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17671v1","updated":"2025-05-23T09:37:10Z","published":"2025-05-23T09:37:10Z","title":"MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual\n  Instruction Synthesis","summary":"  Despite doubts on data quality, instruction synthesis has been widely applied\ninto instruction tuning (IT) of LLMs as an economic and rapid alternative.\nRecent endeavors focus on improving data quality for synthesized instruction\npairs in English and have facilitated IT of English-centric LLMs. However, data\nquality issues in multilingual synthesized instruction pairs are even more\nsevere, since the common synthesizing practice is to translate English\nsynthesized data into other languages using machine translation (MT). Besides\nthe known content errors in these English synthesized data, multilingual\nsynthesized instruction data are further exposed to defects introduced by MT\nand face insufficient localization of the target languages. In this paper, we\npropose MIDB, a Multilingual Instruction Data Booster to automatically address\nthe quality issues in multilingual synthesized data. MIDB is trained on around\n36.8k revision examples across 16 languages by human linguistic experts,\nthereby can boost the low-quality data by addressing content errors and MT\ndefects, and improving localization in these synthesized data. Both automatic\nand human evaluation indicate that not only MIDB steadily improved instruction\ndata quality in 16 languages, but also the instruction-following and\ncultural-understanding abilities of multilingual LLMs fine-tuned on\nMIDB-boosted data were significantly enhanced.\n","authors":["Yilun Liu","Chunguang Zhao","Xinhua Yang","Hongyong Zeng","Shimin Tao","Weibin Meng","Minggui He","Chang Su","Yan Yu","Hongxia Ma","Li Zhang","Daimeng Wei","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12486v3","updated":"2025-05-23T09:33:32Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Management via Query-Guided Activation Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v3.pdf","comment":"ACL25 Main Conference"},{"id":"http://arxiv.org/abs/2505.17667v1","updated":"2025-05-23T09:31:55Z","published":"2025-05-23T09:31:55Z","title":"QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning","summary":"  Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.\n","authors":["Fanqi Wan","Weizhou Shen","Shengyi Liao","Yingcheng Shi","Chenliang Li","Ziyi Yang","Ji Zhang","Fei Huang","Jingren Zhou","Ming Yan"],"pdf_url":"https://arxiv.org/pdf/2505.17667v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2502.17262v2","updated":"2025-05-23T09:30:02Z","published":"2025-02-24T15:44:57Z","title":"Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective","summary":"  The escalating scale and cost of Large Language Models (LLMs) training\nnecessitate accurate pre-training prediction of downstream task performance for\nefficient resource allocation. This is challenged by: 1) the emergence\nphenomenon, where metrics become meaningful only after extensive training,\nhindering prediction by smaller models; and 2) uneven task difficulty and\ninconsistent performance scaling patterns, leading to high metric variability.\nCurrent prediction methods lack accuracy and reliability. We propose a\nClustering-On-Difficulty (COD) framework for downstream performance prediction.\nThe COD framework clusters tasks by their difficulty scaling features, thereby\nestablishing a more stable and predictable support subset through the exclusion\nof tasks exhibiting non-emergent behavior or irregular scaling. We adopt a\nperformance scaling law to predict cluster-wise performance with theoretical\nsupport. Predictable subset performance acts as an intermediate predictor for\nthe full evaluation set. We further derive a mapping function to accurately\nextrapolate the performance of the subset to the full set. Applied to an LLM\nwith 70B parameters, COD achieved a 1.36% average prediction error across eight\nkey LLM benchmarks, offering actionable insights for resource allocation and\ntraining monitoring of LLMs pretraining.\n","authors":["Chengyin Xu","Kaiyuan Chen","Xiao Li","Ke Shen","Chenggang Li"],"pdf_url":"https://arxiv.org/pdf/2502.17262v2.pdf","comment":"19 pages,6 figures"},{"id":"http://arxiv.org/abs/2505.17663v1","updated":"2025-05-23T09:27:40Z","published":"2025-05-23T09:27:40Z","title":"Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States","summary":"  As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present \\textsc{DynToM}, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.\n","authors":["Yang Xiao","Jiashuo Wang","Qiancheng Xu","Changhe Song","Chunpu Xu","Yi Cheng","Wenjie Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.17663v1.pdf","comment":"Accepted by ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.15594v2","updated":"2025-05-23T09:26:58Z","published":"2025-02-21T17:12:35Z","title":"SafeInt: Shielding Large Language Models from Jailbreak Attacks via\n  Safety-Aware Representation Intervention","summary":"  With the widespread real-world deployment of large language models (LLMs),\nensuring their behavior complies with safety standards has become crucial.\nJailbreak attacks exploit vulnerabilities in LLMs to induce undesirable\nbehavior, posing a significant threat to LLM safety. Previous defenses often\nfail to achieve both effectiveness and efficiency simultaneously. Defenses from\na representation perspective offer new insights, but existing interventions\ncannot dynamically adjust representations based on the harmfulness of the\nqueries. To address this limitation, we propose SafeIntervention (SafeInt), a\nnovel defense method that shields LLMs from jailbreak attacks through\nsafety-aware representation intervention. Built on our analysis of the\nrepresentations of jailbreak samples, the core idea of SafeInt is to relocate\njailbreak-related representations into the rejection region. This is achieved\nby intervening in the representation distributions of jailbreak samples to\nalign them with those of unsafe samples. We conduct comprehensive experiments\ncovering six jailbreak attacks, two jailbreak datasets, and two utility\nbenchmarks. Experimental results demonstrate that SafeInt outperforms all\nbaselines in defending LLMs against jailbreak attacks while largely maintaining\nutility. Additionally, we evaluate SafeInt against adaptive attacks and verify\nits effectiveness in mitigating real-time attacks.\n","authors":["Jiaqi Wu","Chen Chen","Chunyan Hou","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.15594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17656v1","updated":"2025-05-23T09:18:56Z","published":"2025-05-23T09:18:56Z","title":"Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs","summary":"  As large language models (LLMs) often generate plausible but incorrect\ncontent, error detection has become increasingly critical to ensure\ntruthfulness. However, existing detection methods often overlook a critical\nproblem we term as self-consistent error, where LLMs repeatly generate the same\nincorrect response across multiple stochastic samples. This work formally\ndefines self-consistent errors and evaluates mainstream detection methods on\nthem. Our investigation reveals two key findings: (1) Unlike inconsistent\nerrors, whose frequency diminishes significantly as LLM scale increases, the\nfrequency of self-consistent errors remains stable or even increases. (2) All\nfour types of detection methshods significantly struggle to detect\nself-consistent errors. These findings reveal critical limitations in current\ndetection methods and underscore the need for improved methods. Motivated by\nthe observation that self-consistent errors often differ across LLMs, we\npropose a simple but effective cross-model probe method that fuses hidden state\nevidence from an external verifier LLM. Our method significantly enhances\nperformance on self-consistent errors across three LLM families.\n","authors":["Hexiang Tan","Fei Sun","Sha Liu","Du Su","Qi Cao","Xin Chen","Jingang Wang","Xunliang Cai","Yuanzhuo Wang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.17656v1.pdf","comment":"Underreview in EMNLP25"},{"id":"http://arxiv.org/abs/2505.17654v1","updated":"2025-05-23T09:18:01Z","published":"2025-05-23T09:18:01Z","title":"EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce\n  Applications","summary":"  E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.\n","authors":["Ancheng Xu","Zhihao Yang","Jingpeng Li","Guanghu Yuan","Longze Chen","Liang Yan","Jiehui Zhou","Zhen Qin","Hengyun Chang","Hamid Alinejad-Rokny","Bo Zheng","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14238v2","updated":"2025-05-23T09:17:49Z","published":"2025-05-20T11:43:25Z","title":"ABBA: Highly Expressive Hadamard Product Adaptation for Large Language\n  Models","summary":"  Large Language Models have demonstrated strong performance across a wide\nrange of tasks, but adapting them efficiently to new domains remains a key\nchallenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nintroducing lightweight, trainable modules while keeping most pre-trained\nweights fixed. The prevailing approach, LoRA, models updates using a low-rank\ndecomposition, but its expressivity is inherently constrained by the rank.\nRecent methods like HiRA aim to increase expressivity by incorporating a\nHadamard product with the frozen weights, but still rely on the structure of\nthe pre-trained model. We introduce ABBA, a new PEFT architecture that\nreparameterizes the update as a Hadamard product of two independently learnable\nlow-rank matrices. In contrast to prior work, ABBA fully decouples the update\nfrom the pre-trained weights, enabling both components to be optimized freely.\nThis leads to significantly higher expressivity under the same parameter\nbudget. We formally analyze ABBA's expressive capacity and validate its\nadvantages through matrix reconstruction experiments. Empirically, ABBA\nachieves state-of-the-art results on arithmetic and commonsense reasoning\nbenchmarks, consistently outperforming existing PEFT methods by a significant\nmargin across multiple models. Our code is publicly available at:\nhttps://github.com/CERT-Lab/abba.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Rohit Vartak","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2505.14238v2.pdf","comment":"Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2505.17645v1","updated":"2025-05-23T09:06:09Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human\n  Sensing and Reasoning","summary":"  Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.\n","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17645v1.pdf","comment":"18 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.14614v4","updated":"2025-05-23T09:05:04Z","published":"2025-02-20T14:52:36Z","title":"ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented\n  Generation for Disease Diagnosis","summary":"  Retrieval-Augmented Large Language Models~(LLMs), which integrate external\nknowledge, have shown remarkable performance in medical domains, including\nclinical diagnosis. However, existing RAG methods often struggle to tailor\nretrieval strategies to diagnostic difficulty and input sample informativeness.\nThis limitation leads to excessive and often unnecessary retrieval, impairing\ncomputational efficiency and increasing the risk of introducing noise that can\ndegrade diagnostic accuracy. To address this, we propose ICA-RAG\n(\\textbf{I}nformation \\textbf{C}ompleteness Guided \\textbf{A}daptive\n\\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration), a novel framework\nfor enhancing RAG reliability in disease diagnosis. ICA-RAG utilizes an\nadaptive control module to assess the necessity of retrieval based on the\ninput's information completeness. By optimizing retrieval and incorporating\nknowledge filtering, ICA-RAG better aligns retrieval operations with clinical\nrequirements. Experiments on three Chinese electronic medical record datasets\ndemonstrate that ICA-RAG significantly outperforms baseline methods,\nhighlighting its effectiveness in clinical diagnosis.\n","authors":["Mingyi Jia","Zhihao Jia","Junwen Duan","Yan Song","Jianxin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14614v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17643v1","updated":"2025-05-23T09:04:49Z","published":"2025-05-23T09:04:49Z","title":"Bridging Electronic Health Records and Clinical Texts: Contrastive\n  Learning for Enhanced Clinical Tasks","summary":"  Conventional machine learning models, particularly tree-based approaches,\nhave demonstrated promising performance across various clinical prediction\ntasks using electronic health record (EHR) data. Despite their strengths, these\nmodels struggle with tasks that require deeper contextual understanding, such\nas predicting 30-day hospital readmission. This can be primarily due to the\nlimited semantic information available in structured EHR data. To address this\nlimitation, we propose a deep multimodal contrastive learning (CL) framework\nthat aligns the latent representations of structured EHR data with unstructured\ndischarge summary notes. It works by pulling together paired EHR and text\nembeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR\nencoder extracted from this framework significantly boosts downstream task\nperformance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission\nprediction. Such results demonstrate the effect of integrating domain knowledge\nfrom clinical notes into EHR-based pipelines, enabling more accurate and\ncontext-aware clinical decision support systems.\n","authors":["Sara Ketabi","Dhanesh Ramachandram"],"pdf_url":"https://arxiv.org/pdf/2505.17643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17642v1","updated":"2025-05-23T09:03:56Z","published":"2025-05-23T09:03:56Z","title":"Stereotype Detection in Natural Language Processing","summary":"  Stereotypes influence social perceptions and can escalate into discrimination\nand violence. While NLP research has extensively addressed gender bias and hate\nspeech, stereotype detection remains an emerging field with significant\nsocietal implications. In this work is presented a survey of existing research,\nanalyzing definitions from psychology, sociology, and philosophy. A\nsemi-automatic literature review was performed by using Semantic Scholar. We\nretrieved and filtered over 6,000 papers (in the year range 2000-2025),\nidentifying key trends, methodologies, challenges and future directions. The\nfindings emphasize stereotype detection as a potential early-monitoring tool to\nprevent bias escalation and the rise of hate speech. Conclusions highlight the\nneed for a broader, multilingual, and intersectional approach in NLP studies.\n","authors":["Alessandra Teresa Cignarella","Anastasia Giachanou","Els Lefever"],"pdf_url":"https://arxiv.org/pdf/2505.17642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04686v5","updated":"2025-05-23T08:57:37Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Process Reward Models (PRMs) have shown promise in enhancing the mathematical\nreasoning capabilities of Large Language Models (LLMs) through Test-Time\nScaling (TTS). However, their integration into multimodal reasoning remains\nlargely unexplored. In this work, we take the first step toward unlocking the\npotential of PRMs in multimodal mathematical reasoning. We identify three key\nchallenges: (1) the scarcity of high-quality reasoning data constrains the\ncapabilities of foundation Multimodal Large Language Models (MLLMs), which\nimposes further limitations on the upper bounds of TTS and reinforcement\nlearning (RL); (2) a lack of automated methods for process labeling within\nmultimodal contexts persists; (3) the employment of process rewards in unimodal\nRL faces issues like reward hacking, which may extend to multimodal scenarios.\nTo address these issues, we introduce URSA, a three-stage Unfolding multimodal\nProcess-Supervision Aided training framework. We first construct MMathCoT-1M, a\nhigh-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset,\nto build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we\ngo through an automatic process to synthesize process supervision data, which\nemphasizes both logical correctness and perceptual consistency. We introduce\nDualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose\nProcess-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a\nmultimodal PRM-aided online RL method that outperforms vanilla GRPO. With\nPS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4%\nand 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found\nat https://github.com/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Xinzhe Ni","Zicheng Lin","Songtao Jiang","Yiyao Yu","Chufan Shi","Ruihang Chu","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v5.pdf","comment":"Update version. Project url: https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2406.15968v2","updated":"2025-05-23T08:57:14Z","published":"2024-06-23T00:23:13Z","title":"ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods","summary":"  The rapid scaling of large language models (LLMs) has raised concerns about\nthe transparency and fair use of the data used in their pretraining. Detecting\nsuch content is challenging due to the scale of the data and limited exposure\nof each instance during training. We propose ReCaLL (Relative Conditional\nLog-Likelihood), a novel membership inference attack (MIA) to detect LLMs'\npretraining data by leveraging their conditional language modeling\ncapabilities. ReCaLL examines the relative change in conditional\nlog-likelihoods when prefixing target data points with non-member context. Our\nempirical findings show that conditioning member data on non-member prefixes\ninduces a larger decrease in log-likelihood compared to non-member data. We\nconduct comprehensive experiments and show that ReCaLL achieves\nstate-of-the-art performance on the WikiMIA dataset, even with random and\nsynthetic prefixes, and can be further improved using an ensemble approach.\nMoreover, we conduct an in-depth analysis of LLMs' behavior with different\nmembership contexts, providing insights into how LLMs leverage membership\ninformation for effective inference at both the sequence and token level.\n","authors":["Roy Xie","Junlin Wang","Ruomin Huang","Minxing Zhang","Rong Ge","Jian Pei","Neil Zhenqiang Gong","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2406.15968v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2505.13282v3","updated":"2025-05-23T08:53:40Z","published":"2025-05-19T16:06:13Z","title":"Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy\n  Expansion","summary":"  Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.\n","authors":["Sahil Mishra","Kumar Arjun","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2505.13282v3.pdf","comment":"Accepted in the Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2505.17636v1","updated":"2025-05-23T08:53:11Z","published":"2025-05-23T08:53:11Z","title":"Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A\n  Multi-Dimensional Analysis","summary":"  Various AI safety datasets have been developed to measure LLMs against\nevolving interpretations of harm. Our evaluation of five recently published\nopen-source safety benchmarks reveals distinct semantic clusters using UMAP\ndimensionality reduction and kmeans clustering (silhouette score: 0.470). We\nidentify six primary harm categories with varying benchmark representation.\nGretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix\nemphasizes self-harm scenarios. Significant differences in prompt length\ndistribution suggests confounds to data collection and interpretations of harm\nas well as offer possible context. Our analysis quantifies benchmark\northogonality among AI benchmarks, allowing for transparency in coverage gaps\ndespite topical similarities. Our quantitative framework for analyzing semantic\northogonality across safety benchmarks enables more targeted development of\ndatasets that comprehensively address the evolving landscape of harms in AI\nuse, however that is defined in the future.\n","authors":["Jonathan Bennion","Shaona Ghosh","Mantek Singh","Nouha Dziri"],"pdf_url":"https://arxiv.org/pdf/2505.17636v1.pdf","comment":"6th International Conference on Advanced Natural Language Processing\n  (AdNLP 2025), May 17 ~ 18, 2025, Zurich, Switzerland"},{"id":"http://arxiv.org/abs/2505.17630v1","updated":"2025-05-23T08:41:45Z","published":"2025-05-23T08:41:45Z","title":"GIM: Improved Interpretability for Large Language Models","summary":"  Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.\n","authors":["Joakim Edin","Rbert Csords","Tuukka Ruotsalo","Zhengxuan Wu","Maria Maistro","Jing Huang","Lars Maale"],"pdf_url":"https://arxiv.org/pdf/2505.17630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17625v1","updated":"2025-05-23T08:36:22Z","published":"2025-05-23T08:36:22Z","title":"Enhancing Large Vision-Language Models with Layout Modality for Table\n  Question Answering on Japanese Annual Securities Reports","summary":"  With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats.\n","authors":["Hayato Aida","Kosuke Takahashi","Takahiro Omi"],"pdf_url":"https://arxiv.org/pdf/2505.17625v1.pdf","comment":"Accepted at IIAI AAI 2025, the 3rd International Conference on\n  Computational and Data Sciences in Economics and Finance"},{"id":"http://arxiv.org/abs/2501.16975v2","updated":"2025-05-23T08:32:07Z","published":"2025-01-28T14:15:42Z","title":"Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling","summary":"  Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.\n","authors":["Hongzhi Huang","Defa Zhu","Banggu Wu","Yutao Zeng","Ya Wang","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.16975v2.pdf","comment":"accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.11274v2","updated":"2025-05-23T08:31:47Z","published":"2025-05-16T14:08:04Z","title":"SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning","summary":"  Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.\n","authors":["Zheng Li","Qingxiu Dong","Jingyuan Ma","Di Zhang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2505.11274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19557v3","updated":"2025-05-23T08:31:02Z","published":"2024-11-29T09:10:30Z","title":"Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning","summary":"  Low-rank adapters have become standard for efficiently fine-tuning large\nlanguage models (LLMs), but they often fall short of achieving the performance\nof full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that\napproximates full fine-tuning within low-rank subspaces using a carefully\ndesigned initialization strategy. We theoretically demonstrate that the\narchitecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and\nA while keeping other matrices fixed, provides the precise conditions needed\nfor this approximation. We leverage its constrained update space to achieve\noptimal scaling for high-rank gradient updates while removing the need for\nhyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using \\textbf{27-90}\ntimes fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our\nfindings establish that it is possible to simulate full fine-tuning in low-rank\nsubspaces, and achieve significant efficiency gains without sacrificing\nperformance. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/lora-sb.\n","authors":["Kaustubh Ponkshe","Raghav Singhal","Eduard Gorbunov","Alexey Tumanov","Samuel Horvath","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2411.19557v3.pdf","comment":"Kaustubh Ponkshe and Raghav Singhal contributed equally to this work"},{"id":"http://arxiv.org/abs/2409.12887v3","updated":"2025-05-23T08:29:09Z","published":"2024-09-19T16:29:58Z","title":"Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data\n  Augmentation and Gaussian-Decayed Contrastive Learning","summary":"  Recently, using large language models (LLMs) for data augmentation has led to\nconsiderable improvements in unsupervised sentence embedding models. However,\nexisting methods encounter two primary challenges: limited data diversity and\nhigh data noise. Current approaches often neglect fine-grained knowledge, such\nas entities and quantities, leading to insufficient diversity. Besides,\nunsupervised data frequently lacks discriminative information, and the\ngenerated synthetic samples may introduce noise. In this paper, we propose a\npipeline-based data augmentation method via LLMs and introduce the\nGaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model\nto enhance unsupervised sentence embeddings. To tackle the issue of low data\ndiversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and\nquantities, enabling LLMs to generate more diverse samples. To address high\ndata noise, the GCSE model uses a Gaussian-decayed function to limit the impact\nof false hard negative samples, enhancing the model's discriminative\ncapability. Experimental results show that our approach achieves\nstate-of-the-art performance in semantic textual similarity (STS) tasks, using\nfewer data samples and smaller LLMs, demonstrating its efficiency and\nrobustness across various models.\n","authors":["Peichao Lai","Zhengfeng Zhang","Wentao Zhang","Fangcheng Fu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2409.12887v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11365v3","updated":"2025-05-23T08:25:40Z","published":"2025-05-16T15:31:08Z","title":"Phare: A Safety Probe for Large Language Models","summary":"  Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.\n","authors":["Pierre Le Jeune","Benot Malzieux","Weixuan Xiao","Matteo Dora"],"pdf_url":"https://arxiv.org/pdf/2505.11365v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17616v1","updated":"2025-05-23T08:23:36Z","published":"2025-05-23T08:23:36Z","title":"Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large\n  Language Model-based Agents in Embodied Environments","summary":"  Agents powered by large language models (LLMs) have demonstrated strong\nplanning and decision-making capabilities in complex embodied environments.\nHowever, such agents often suffer from inefficiencies in multi-turn\ninteractions, frequently trapped in repetitive loops or issuing ineffective\ncommands, leading to redundant computational overhead. Instead of relying\nsolely on learning from trajectories, we take a first step toward exploring the\nearly-exit behavior for LLM-based agents. We propose two complementary\napproaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions\nduring generation, and 2. an $\\textbf{extrinsic}$ method that verifies task\ncompletion to determine when to halt an agent's trial. To evaluate early-exit\nmechanisms, we introduce two metrics: one measures the reduction of\n$\\textbf{redundant steps}$ as a positive effect, and the other evaluates\n$\\textbf{progress degradation}$ as a negative effect. Experiments with 4\ndifferent LLMs across 5 embodied environments show significant efficiency\nimprovements, with only minor drops in agent performance. We also validate a\npractical strategy where a stronger agent assists after an early-exit agent,\nachieving better performance with the same total steps. We will release our\ncode to support further research.\n","authors":["Qingyu Lu","Liang Ding","Siyi Cao","Xuebo Liu","Kanjian Zhang","Jinxia Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2505.17616v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.17615v1","updated":"2025-05-23T08:22:09Z","published":"2025-05-23T08:22:09Z","title":"Large language model as user daily behavior data generator: balancing\n  population diversity and individual personality","summary":"  Predicting human daily behavior is challenging due to the complexity of\nroutine patterns and short-term fluctuations. While data-driven models have\nimproved behavior prediction by leveraging empirical data from various\nplatforms and devices, the reliance on sensitive, large-scale user data raises\nprivacy concerns and limits data availability. Synthetic data generation has\nemerged as a promising solution, though existing methods are often limited to\nspecific applications. In this work, we introduce BehaviorGen, a framework that\nuses large language models (LLMs) to generate high-quality synthetic behavior\ndata. By simulating user behavior based on profiles and real events,\nBehaviorGen supports data augmentation and replacement in behavior prediction\nmodels. We evaluate its performance in scenarios such as pertaining\naugmentation, fine-tuning replacement, and fine-tuning augmentation, achieving\nsignificant improvements in human mobility and smartphone usage predictions,\nwith gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen\nto enhance user behavior modeling through flexible and privacy-preserving\nsynthetic data generation.\n","authors":["Haoxin Li","Jingtao Ding","Jiahui Gong","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.17615v1.pdf","comment":"14 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.17613v1","updated":"2025-05-23T08:21:28Z","published":"2025-05-23T08:21:28Z","title":"MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation","summary":"  Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.\n","authors":["Jihan Yao","Yushi Hu","Yujie Yi","Bin Han","Shangbin Feng","Guang Yang","Bingbing Wen","Ranjay Krishna","Lucy Lu Wang","Yulia Tsvetkov","Noah A. Smith","Banghua Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.17613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17612v1","updated":"2025-05-23T08:20:15Z","published":"2025-05-23T08:20:15Z","title":"Distilling LLM Agent into Small Models with Retrieval and Code Tools","summary":"  Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.\n","authors":["Minki Kang","Jongwon Jeong","Seanie Lee","Jaewoong Cho","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2505.17612v1.pdf","comment":"preprint, v1"},{"id":"http://arxiv.org/abs/2505.17607v1","updated":"2025-05-23T08:16:32Z","published":"2025-05-23T08:16:32Z","title":"Controlled Agentic Planning & Reasoning for Mechanism Synthesis","summary":"  This work presents a dual-agent Large Language Model (LLM)-based reasoning\nmethod for mechanism synthesis, capable of reasoning at both linguistic and\nsymbolic levels to generate geometrical and dynamic outcomes. The model\nconsists of a composition of well-defined functions that, starting from a\nnatural language specification, references abstract properties through\nsupporting equations, generates and parametrizes simulation code, and elicits\nfeedback anchor points using symbolic regression and distance functions. This\nprocess closes an actionable refinement loop at the linguistic and symbolic\nlayers. The approach is shown to be both effective and convergent in the\ncontext of planar mechanisms. Additionally, we introduce MSynth, a novel\nbenchmark for planar mechanism synthesis, and perform a comprehensive analysis\nof the impact of the model components. We further demonstrate that symbolic\nregression prompts unlock mechanistic insights only when applied to\nsufficiently large architectures.\n","authors":["Joo Pedro Gandarela","Thiago Rios","Stefan Menzel","Andr Freitas"],"pdf_url":"https://arxiv.org/pdf/2505.17607v1.pdf","comment":"24 pages, 16 figures"},{"id":"http://arxiv.org/abs/2505.17601v1","updated":"2025-05-23T08:13:59Z","published":"2025-05-23T08:13:59Z","title":"Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based\n  Backdoor Attacks for Jailbreaking Large Language Models","summary":"  Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o.\n","authors":["Jiawei Kong","Hao Fang","Xiaochen Yang","Kuofeng Gao","Bin Chen","Shu-Tao Xia","Yaowei Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.17601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11820v2","updated":"2025-05-23T08:12:10Z","published":"2025-05-17T04:06:12Z","title":"Chain-of-Model Learning for Language Model","summary":"  In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.\n","authors":["Kaitao Song","Xiaohua Wang","Xu Tan","Huiqiang Jiang","Chengruidong Zhang","Yongliang Shen","Cen LU","Zihao Li","Zifan Song","Caihua Shan","Yansen Wang","Kan Ren","Xiaoqing Zheng","Tao Qin","Yuqing Yang","Dongsheng Li","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.11820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10882v8","updated":"2025-05-23T08:08:51Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Data Selection via Style Consistency-Aware Response Ranking for\n  Efficient Instruction-Tuning of Large Language Models","summary":"  Recent studies emphasize that manually ensuring a consistent response style\nand maintaining high data quality in training sets can significantly improve\nthe performance of fine-tuned Large Language Models (LLMs) while reducing the\nnumber of training examples needed. However, the precise definition of style\nand the relationship between style, data quality, and LLM performance remains\nunclear. This research identifies two key stylistic elements in responses:\nlinguistic form and instructional surprisal. We find that, among training data\nof comparable quality, higher consistency in these response elements leads to\nbetter LLM performance. Inspired by this, we introduce Style Consistency-Aware\nResponse Ranking (SCAR), which automatically prioritizes instruction-response\npairs in the training set based on their response stylistic consistency. By\nselecting the most style-consistent examples, using 0.7% of the full dataset in\ncertain cases, the fine-tuned LLMs can match or even surpass the performance of\nmodels trained on the entire dataset in coding and open-ended\nquestion-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v8.pdf","comment":"34 pages (8th version), developed over 1.5 years. Previously\n  submitted to EMNLP 2024 and ICLR 2025, with revisions based on extensive\n  review feedback. Now accepted to ACL 2025 main"},{"id":"http://arxiv.org/abs/2505.17598v1","updated":"2025-05-23T08:02:38Z","published":"2025-05-23T08:02:38Z","title":"One Model Transfer to All: On Robust Jailbreak Prompts Generation\n  against LLMs","summary":"  Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack.\n","authors":["Linbao Li","Yannan Liu","Daojing He","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2505.17598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17595v1","updated":"2025-05-23T07:59:46Z","published":"2025-05-23T07:59:46Z","title":"NeUQI: Near-Optimal Uniform Quantization Parameter Initialization","summary":"  Large language models (LLMs) achieve impressive performance across domains\nbut face significant challenges when deployed on consumer-grade GPUs or\npersonal devices such as laptops, due to high memory consumption and inference\ncosts. Post-training quantization (PTQ) of LLMs offers a promising solution\nthat reduces their memory footprint and decoding latency. In practice, PTQ with\nuniform quantization representation is favored for its efficiency and ease of\ndeployment since uniform quantization is widely supported by mainstream\nhardware and software libraries. Recent studies on $\\geq 2$-bit uniform\nquantization have led to noticeable improvements in post-quantization model\nperformance; however, they primarily focus on quantization methodologies, while\nthe initialization of quantization parameters is underexplored and still relies\non the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method\ndevoted to efficiently determining near-optimal initial parameters for uniform\nquantization. NeUQI is orthogonal to prior quantization methodologies and can\nseamlessly integrate with them. The experiments with the LLaMA and Qwen\nfamilies on various tasks demonstrate that our NeUQI consistently outperforms\nexisting methods. Furthermore, when combined with a lightweight distillation\nstrategy, NeUQI can achieve superior performance to PV-tuning, a much more\nresource-intensive approach.\n","authors":["Li Lin","Xinyu Hu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2505.17595v1.pdf","comment":"9 pages, under review"},{"id":"http://arxiv.org/abs/2503.03303v2","updated":"2025-05-23T07:51:47Z","published":"2025-03-05T09:37:05Z","title":"SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open\n  Domain Event Detection","summary":"  Automatic evaluation for Open Domain Event Detection (ODED) is a highly\nchallenging task, because ODED is characterized by a vast diversity of\nun-constrained output labels from various domains. Nearly all existing\nevaluation methods for ODED usually first construct evaluation benchmarks with\nlimited labels and domain coverage, and then evaluate ODED methods using\nmetrics based on token-level label matching rules. However, this kind of\nevaluation framework faces two issues: (1) The limited evaluation benchmarks\nlack representatives of the real world, making it difficult to accurately\nreflect the performance of various ODED methods in real-world scenarios; (2)\nEvaluation metrics based on token-level matching rules fail to capture semantic\nsimilarity between predictions and golden labels. To address these two problems\nabove, we propose a scalable and reliable Semantic-level Evaluation framework\nfor Open domain Event detection (SEOE) by constructing a more representative\nevaluation benchmark and introducing a semantic evaluation metric.\nSpecifically, our proposed framework first constructs a scalable evaluation\nbenchmark that currently includes 564 event types covering 7 major domains,\nwith a cost-effective supplementary annotation strategy to ensure the\nbenchmark's representativeness. The strategy also allows for the supplement of\nnew event types and domains in the future. Then, the proposed SEOE leverages\nlarge language models (LLMs) as automatic evaluation agents to compute a\nsemantic F1-score, incorporating fine-grained definitions of semantically\nsimilar labels to enhance the reliability of the evaluation. Extensive\nexperiments validate the representatives of the benchmark and the reliability\nof the semantic evaluation metric. Existing ODED methods are thoroughly\nevaluated, and the error patterns of predictions are analyzed, revealing\nseveral insightful findings.\n","authors":["Yi-Fan Lu","Xian-Ling Mao","Tian Lan","Tong Zhang","Yu-Shi Zhu","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2503.03303v2.pdf","comment":"Accepted by ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2505.16078v2","updated":"2025-05-23T07:49:10Z","published":"2025-05-21T23:39:24Z","title":"Small Language Models in the Real World: Insights from Industrial Text\n  Classification","summary":"  With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings.\n","authors":["Lujun Li","Lama Sleem","Niccolo' Gentile","Geoffrey Nichil","Radu State"],"pdf_url":"https://arxiv.org/pdf/2505.16078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13237v2","updated":"2025-05-23T07:41:15Z","published":"2025-05-19T15:20:32Z","title":"SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based\n  on Speech and Audio Information","summary":"  Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research.\n","authors":["Chih-Kai Yang","Neo Ho","Yen-Ting Piao","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2505.13237v2.pdf","comment":"Accepted to Interspeech 2025. Project page:\n  https://github.com/ckyang1124/SAKURA"},{"id":"http://arxiv.org/abs/2505.15957v2","updated":"2025-05-23T07:36:27Z","published":"2025-05-21T19:17:29Z","title":"Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey","summary":"  With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.\n","authors":["Chih-Kai Yang","Neo S. Ho","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2505.15957v2.pdf","comment":"Project Website: https://github.com/ckyang1124/LALM-Evaluation-Survey"},{"id":"http://arxiv.org/abs/2505.16592v2","updated":"2025-05-23T07:33:48Z","published":"2025-05-22T12:27:12Z","title":"What Media Frames Reveal About Stance: A Dataset and Study about Memes\n  in Climate Change Discourse","summary":"  Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.\n","authors":["Shijia Zhou","Siyao Peng","Simon Luebke","Jrg Haler","Mario Haim","Saif M. Mohammad","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2505.16592v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.17571v1","updated":"2025-05-23T07:30:13Z","published":"2025-05-23T07:30:13Z","title":"Reasoning Meets Personalization: Unleashing the Potential of Large\n  Reasoning Model for Personalized Generation","summary":"  Personalization is a critical task in modern intelligent systems, with\napplications spanning diverse domains, including interactions with large\nlanguage models (LLMs). Recent advances in reasoning capabilities have\nsignificantly enhanced LLMs, enabling unprecedented performance in tasks such\nas mathematics and coding. However, their potential for personalization tasks\nremains underexplored.\n  In this paper, we present the first systematic evaluation of large reasoning\nmodels (LRMs) for personalization tasks. Surprisingly, despite generating more\ntokens, LRMs do not consistently outperform general-purpose LLMs, especially in\nretrieval-intensive scenarios where their advantages diminish. Our analysis\nidentifies three key limitations: divergent thinking, misalignment of response\nformats, and ineffective use of retrieved information. To address these\nchallenges, we propose Reinforced Reasoning for Personalization (\\model), a\nnovel framework that incorporates a hierarchical reasoning thought template to\nguide LRMs in generating structured outputs. Additionally, we introduce a\nreasoning process intervention method to enforce adherence to designed\nreasoning patterns, enhancing alignment. We also propose a cross-referencing\nmechanism to ensure consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing techniques.\n","authors":["Sichun Luo","Guanzhi Deng","Jian Xu","Xiaojie Zhang","Hanxu Hou","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2505.17571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17565v1","updated":"2025-05-23T07:24:53Z","published":"2025-05-23T07:24:53Z","title":"PPT: A Process-based Preference Learning Framework for Self Improving\n  Table Question Answering Models","summary":"  Improving large language models (LLMs) with self-generated data has\ndemonstrated success in tasks such as mathematical reasoning and code\ngeneration. Yet, no exploration has been made on table question answering\n(TQA), where a system answers questions based on tabular data. Addressing this\ngap is crucial for TQA, as effective self-improvement can boost performance\nwithout requiring costly or manually annotated data. In this work, we propose\nPPT, a Process-based Preference learning framework for TQA. It decomposes\nreasoning chains into discrete states, assigns scores to each state, and\nsamples contrastive steps for preference learning. Experimental results show\nthat PPT effectively improves TQA models by up to 5% on in-domain datasets and\n2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,\nthe resulting models achieve competitive results compared to more complex and\nlarger state-of-the-art TQA systems, while being five times more efficient\nduring inference.\n","authors":["Wei Zhou","Mohsen Mesgar","Heike Adel","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2505.17565v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.18153v1","updated":"2025-05-23T17:59:33Z","published":"2025-05-23T17:59:33Z","title":"REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders","summary":"  We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN.\n","authors":["Savya Khosla","Sethuraman TV","Barnett Lee","Alexander Schwing","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2505.18153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18151v1","updated":"2025-05-23T17:59:24Z","published":"2025-05-23T17:59:24Z","title":"WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions","summary":"  WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/\n","authors":["Zizhang Li","Hong-Xing Yu","Wei Liu","Yin Yang","Charles Herrmann","Gordon Wetzstein","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.18151v1.pdf","comment":"The first two authors contributed equally. Project website:\n  https://kyleleey.github.io/WonderPlay/"},{"id":"http://arxiv.org/abs/2505.18142v1","updated":"2025-05-23T17:52:16Z","published":"2025-05-23T17:52:16Z","title":"TokBench: Evaluating Your Visual Tokenizer before Visual Generation","summary":"  In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.\n","authors":["Junfeng Wu","Dongliang Luo","Weizhi Zhao","Zhihao Xie","Yuanhao Wang","Junyi Li","Xudong Xie","Yuliang Liu","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2505.18142v1.pdf","comment":"Benchmark, homepagee: https://wjf5203.github.io/TokBench"},{"id":"http://arxiv.org/abs/2505.18137v1","updated":"2025-05-23T17:47:20Z","published":"2025-05-23T17:47:20Z","title":"Boosting Open Set Recognition Performance through Modulated\n  Representation Learning","summary":"  The open set recognition (OSR) problem aims to identify test samples from\nnovel semantic classes that are not part of the training classes, a task that\nis crucial in many practical scenarios. However, existing OSR methods use a\nconstant scaling factor (the temperature) to the logits before applying a loss\nfunction, which hinders the model from exploring both ends of the spectrum in\nrepresentation learning -- from instance-level to semantic-level features. In\nthis paper, we address this problem by enabling temperature-modulated\nrepresentation learning using our novel negative cosine scheduling scheme. Our\nscheduling lets the model form a coarse decision boundary at the beginning of\ntraining by focusing on fewer neighbors, and gradually prioritizes more\nneighbors to smooth out rough edges. This gradual task switching leads to a\nricher and more generalizable representation space. While other OSR methods\nbenefit by including regularization or auxiliary negative samples, such as with\nmix-up, thereby adding a significant computational overhead, our scheme can be\nfolded into any existing OSR method with no overhead. We implement the proposed\nscheme on top of a number of baselines, using both cross-entropy and\ncontrastive loss functions as well as a few other OSR methods, and find that\nour scheme boosts both the OSR performance and the closed set performance in\nmost cases, especially on the tougher semantic shift benchmarks.\n","authors":["Amit Kumar Kundu","Vaishnavi Patil","Joseph Jaja"],"pdf_url":"https://arxiv.org/pdf/2505.18137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18134v1","updated":"2025-05-23T17:43:27Z","published":"2025-05-23T17:43:27Z","title":"VideoGameBench: Can Vision-Language Models complete popular video games?","summary":"  Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.\n","authors":["Alex L. Zhang","Thomas L. Griffiths","Karthik R. Narasimhan","Ofir Press"],"pdf_url":"https://arxiv.org/pdf/2505.18134v1.pdf","comment":"9 pages, 33 pages including supplementary"},{"id":"http://arxiv.org/abs/2505.18132v1","updated":"2025-05-23T17:41:54Z","published":"2025-05-23T17:41:54Z","title":"BiggerGait: Unlocking Gait Recognition with Layer-wise Representations\n  from Large Vision Models","summary":"  Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.\n","authors":["Dingqing Ye","Chao Fan","Zhanbo Huang","Chengwen Luo","Jianqiang Li","Shiqi Yu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2505.18132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18129v1","updated":"2025-05-23T17:41:14Z","published":"2025-05-23T17:41:14Z","title":"One RL to See Them All: Visual Triple Unified Reinforcement Learning","summary":"  Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.\n","authors":["Yan Ma","Linge Du","Xuyang Shen","Shaoxiang Chen","Pengfei Li","Qibing Ren","Lizhuang Ma","Yuchao Dai","Pengfei Liu","Junjie Yan"],"pdf_url":"https://arxiv.org/pdf/2505.18129v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2503.22174v2","updated":"2025-05-23T17:38:17Z","published":"2025-03-28T06:27:55Z","title":"Synergistic Bleeding Region and Point Detection in Laparoscopic Surgical\n  Videos","summary":"  Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process and increases the risk of\npostoperative complications. Intelligent detection of bleeding areas can\nquantify the blood loss to assist decision-making, while locating bleeding\npoints helps surgeons quickly identify the source of bleeding and achieve\nhemostasis in time to improve surgical success rates. In this study, we first\nconstruct a real-world laparoscopic surgical bleeding detection dataset, named\nSurgBlood, comprising 5,330 frames from 95 surgical video clips with bleeding\nregion and point annotations. Accordingly, we develop a dual-task synergistic\nonline detector called BlooDet, designed to perform simultaneous detection of\nbleeding regions and points in laparoscopic surgery. Our framework embraces a\ndual-branch bidirectional guidance design based on Segment Anything Model 2\n(SAM 2). The mask branch detects bleeding regions through adaptive edge and\npoint prompt embeddings, and the point branch leverages mask memory to induce\nbleeding point memory modeling and capture the direction of bleed point\nmovement via inter-frame optical flow. By bidirectional guidance, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling to infer the current bleeding condition. Extensive experiments\ndemonstrate that our baseline outperforms 12 counterparts on SurgBlood in both\nbleeding region and point detection.\n","authors":["Jialun Pei","Zhangjun Zhou","Diandian Guo","Zhixi Li","Jing Qin","Bo Du","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2503.22174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18115v1","updated":"2025-05-23T17:14:12Z","published":"2025-05-23T17:14:12Z","title":"Instructify: Demystifying Metadata to Visual Instruction Tuning Data\n  Conversion","summary":"  Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify.\n","authors":["Jacob Hansen","Wei Lin","Junmo Kang","Muhammad Jehanzeb Mirza","Hongyin Luo","Rogerio Feris","Alan Ritter","James Glass","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2505.18115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11651v2","updated":"2025-05-23T17:05:50Z","published":"2025-02-17T10:43:38Z","title":"MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression","summary":"  Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at github:\nhttps://github.com/linjiemu/MMXU.\n","authors":["Linjie Mu","Zhongzhen Huang","Shengqian Qin","Yakun Zhu","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18111v1","updated":"2025-05-23T17:04:56Z","published":"2025-05-23T17:04:56Z","title":"Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR\n  Challenge Multi-Modal Tracking","summary":"  We present an effective approach for adapting the Segment Anything Model 2\n(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the\npowerful pre-trained capabilities of SAM2 and incorporates several key\ntechniques to enhance its performance in VOT applications. By combining SAM2\nwith our proposed optimizations, we achieved a first place AUC score of 89.4 on\nthe 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the\neffectiveness of our approach. This paper details our methodology, the specific\nenhancements made to SAM2, and a comprehensive analysis of our results in the\ncontext of VOT solutions along with the multi-modality aspect of the dataset.\n","authors":["Cheng-Yen Yang","Hsiang-Wei Huang","Pyong-Kun Kim","Chien-Kai Kuo","Jui-Wei Chang","Kwang-Ju Kim","Chung-I Huang","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2505.18111v1.pdf","comment":"Accepted by ICPR Multi-Modal Visual Pattern Recognition Workshop"},{"id":"http://arxiv.org/abs/2504.15928v2","updated":"2025-05-23T17:03:24Z","published":"2025-04-22T14:17:22Z","title":"A Clinician-Friendly Platform for Ophthalmic Image Analysis Without\n  Technical Barriers","summary":"  Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, yet most current models require retraining when applied across\ndifferent clinical settings, limiting their scalability. We introduce\nGlobeReady, a clinician-friendly AI platform that enables fundus disease\ndiagnosis that operates without retraining, fine-tuning, or the needs for\ntechnical expertise. GlobeReady demonstrates high accuracy across imaging\nmodalities: 93.9-98.5% for 11 fundus diseases using color fundus photographs\n(CPFs) and 87.2-92.7% for 15 fundus diseases using optic coherence tomography\n(OCT) scans. By leveraging training-free local feature augmentation, GlobeReady\nplatform effectively mitigates domain shifts across centers and populations,\nachieving accuracies of 88.9-97.4% across five centers on average in China,\n86.3-96.9% in Vietnam, and 73.4-91.0% in Singapore, and 90.2-98.9% in the UK.\nIncorporating a bulit-in confidence-quantifiable diagnostic mechanism further\nenhances the platform's accuracy to 94.9-99.4% with CFPs and 88.2-96.2% with\nOCT, while enabling identification of out-of-distribution cases with 86.3%\naccuracy across 49 common and rare fundus diseases using CFPs, and 90.6%\naccuracy across 13 diseases using OCT. Clinicians from countries rated\nGlobeReady highly for usability and clinical relevance (average score 4.6/5).\nThese findings demonstrate GlobeReady's robustness, generalizability and\npotential to support global ophthalmic care without technical barriers.\n","authors":["Meng Wang","Tian Lin","Qingshan Hou","Aidi Lin","Jingcheng Wang","Qingsheng Peng","Truong X. Nguyen","Danqi Fang","Ke Zou","Ting Xu","Cancan Xue","Ten Cheer Quek","Qinkai Yu","Minxin Liu","Hui Zhou","Zixuan Xiao","Guiqin He","Huiyu Liang","Tingkun Shi","Man Chen","Linna Liu","Yuanyuan Peng","Lianyu Wang","Qiuming Hu","Junhong Chen","Zhenhua Zhang","Cheng Chen","Yitian Zhao","Dianbo Liu","Jianhua Wu","Xinjian Chen","Changqing Zhang","Triet Thanh Nguyen","Yanda Meng","Yalin Zheng","Yih Chung Tham","Carol Y. Cheung","Huazhu Fu","Haoyu Chen","Ching-Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.15928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18107v1","updated":"2025-05-23T17:03:13Z","published":"2025-05-23T17:03:13Z","title":"Accelerating Learned Image Compression Through Modeling Neural Training\n  Dynamics","summary":"  As learned image compression (LIC) methods become increasingly\ncomputationally demanding, enhancing their training efficiency is crucial. This\npaper takes a step forward in accelerating the training of LIC methods by\nmodeling the neural training dynamics. We first propose a Sensitivity-aware\nTrue and Dummy Embedding Training mechanism (STDET) that clusters LIC model\nparameters into few separate modes where parameters are expressed as affine\ntransformations of reference parameters within the same mode. By further\nutilizing the stable intra-mode correlations throughout training and parameter\nsensitivities, we gradually embed non-reference parameters, reducing the number\nof trainable parameters. Additionally, we incorporate a Sampling-then-Moving\nAverage (SMA) technique, interpolating sampled weights from stochastic gradient\ndescent (SGD) training to obtain the moving average weights, ensuring smooth\ntemporal behavior and minimizing training state variances. Overall, our method\nsignificantly reduces training space dimensions and the number of trainable\nparameters without sacrificing model performance, thus accelerating model\nconvergence. We also provide a theoretical analysis on the Noisy quadratic\nmodel, showing that the proposed method achieves a lower training variance than\nstandard SGD. Our approach offers valuable insights for further developing\nefficient training methods for LICs.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.18107v1.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2502.12427v2","updated":"2025-05-23T17:03:00Z","published":"2025-02-18T01:52:41Z","title":"ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image\n  Super-Resolution in Earth System","summary":"  Super-resolution (SR) techniques are essential for improving Earth System\nModel (ESM) data's spatial resolution, which helps better understand complex\nenvironmental processes. This paper presents a new algorithm, ViFOR, which\ncombines Vision Transformers (ViT) and Fourier-based Implicit Neural\nRepresentation Networks (INRs) to generate High-Resolution (HR) images from\nLow-Resolution (LR) inputs. ViFOR introduces a novel integration of\nFourier-based activation functions within the Vision Transformer architecture,\nenabling it to effectively capture global context and high-frequency details\ncritical for accurate SR reconstruction. The results show that ViFOR\noutperforms state-of-the-art methods such as ViT, Sinusoidal Representation\nNetworks (SIREN), and SR Generative Adversarial Networks (SRGANs) based on\nmetrics like Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE)\nboth for global as well as the local imagery. ViFOR improves PSNR of up to 4.18\ndB, 1.56 dB, and 1.73 dB over ViT for full images in the Source Temperature,\nShortwave, and Longwave Flux.\n","authors":["Ehsan Zeraatkar","Salah A Faroughi","Jelena Tei"],"pdf_url":"https://arxiv.org/pdf/2502.12427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18106v1","updated":"2025-05-23T17:02:22Z","published":"2025-05-23T17:02:22Z","title":"F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial\n  Architecture for Synthetic Image Generation of Nanoparticles","summary":"  Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields.\n","authors":["Varun Ajith","Anindya Pal","Saumik Bhattacharya","Sayantari Ghosh"],"pdf_url":"https://arxiv.org/pdf/2505.18106v1.pdf","comment":"11 pages, 9 figures, 2 tables, conference paper"},{"id":"http://arxiv.org/abs/2505.18097v1","updated":"2025-05-23T16:49:20Z","published":"2025-05-23T16:49:20Z","title":"Towards more transferable adversarial attack in black-box manner","summary":"  Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.\n","authors":["Chun Tong Lei","Zhongliang Guo","Hon Chung Lee","Minh Quoc Duong","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2505.18097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18096v1","updated":"2025-05-23T16:49:05Z","published":"2025-05-23T16:49:05Z","title":"DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations","summary":"  In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk.\n","authors":["Ziqiao Peng","Yanbo Fan","Haoyu Wu","Xuan Wang","Hongyan Liu","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2505.18096v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.18087v1","updated":"2025-05-23T16:44:21Z","published":"2025-05-23T16:44:21Z","title":"CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays","summary":"  Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench\n","authors":["Hyungyung Lee","Geon Choi","Jung-Oh Lee","Hangyul Yoon","Hyuk Gi Hong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2505.18087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18079v1","updated":"2025-05-23T16:37:36Z","published":"2025-05-23T16:37:36Z","title":"Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding","summary":"  Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.\n","authors":["Xiaoyi Zhang","Zhaoyang Jia","Zongyu Guo","Jiahao Li","Bin Li","Houqiang Li","Yan Lu"],"pdf_url":"https://arxiv.org/pdf/2505.18079v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.18078v1","updated":"2025-05-23T16:37:14Z","published":"2025-05-23T16:37:14Z","title":"DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation","summary":"  Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.\n","authors":["Junhao Chen","Mingjin Chen","Jianjin Xu","Xiang Li","Junting Dong","Mingze Sun","Puhua Jiang","Hongxiang Li","Yuhang Yang","Hao Zhao","Xiaoxiao Long","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2505.18078v1.pdf","comment":"Our video demos and code are available at https://DanceTog.github.io/"},{"id":"http://arxiv.org/abs/2411.19715v3","updated":"2025-05-23T16:14:40Z","published":"2024-11-29T14:02:11Z","title":"Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery\n  Detection","summary":"  We describe Forensics Adapter, an adapter network designed to transform CLIP\ninto an effective and generalizable face forgery detector. Although CLIP is\nhighly versatile, adapting it for face forgery detection is non-trivial as\nforgery-related knowledge is entangled with a wide range of unrelated\nknowledge. Existing methods treat CLIP merely as a feature extractor, lacking\ntask-specific adaptation, which limits their effectiveness. To address this, we\nintroduce an adapter to learn face forgery traces -- the blending boundaries\nunique to forged faces, guided by task-specific objectives. Then we enhance the\nCLIP visual tokens with a dedicated interaction strategy that communicates\nknowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its\nversatility is highly retained, naturally ensuring strong generalizability in\nface forgery detection. With only 5.7M trainable parameters, our method\nachieves a significant performance boost, improving by approximately 7% on\naverage across five standard datasets. Additionally, we describe Forensics\nAdapter++, an extended method that incorporates textual modality via a newly\nproposed forgery-aware prompt learning strategy. This extension leads to a\nfurther 1.3% performance boost over the original Forensics Adapter. We believe\nthe proposed methods can serve as a baseline for future CLIP-based face forgery\ndetection methods. The codes have been released at\nhttps://github.com/OUC-VAS/ForensicsAdapter.\n","authors":["Xinjie Cui","Yuezun Li","Delong Zhu","Jiaran Zhou","Junyu Dong","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2411.19715v3.pdf","comment":"Extension of CVPR 2025"},{"id":"http://arxiv.org/abs/2505.16854v2","updated":"2025-05-23T16:09:00Z","published":"2025-05-22T16:13:29Z","title":"Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models","summary":"  Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.\n","authors":["Jiaqi Wang","Kevin Qinghong Lin","James Cheng","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2505.16854v2.pdf","comment":"update more examples in appendix"},{"id":"http://arxiv.org/abs/2505.18060v1","updated":"2025-05-23T16:07:16Z","published":"2025-05-23T16:07:16Z","title":"Semantic Correspondence: Unified Benchmarking and a Strong Baseline","summary":"  Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.\n","authors":["Kaiyan Zhang","Xinghui Li","Jingyi Lu","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2505.18060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18058v1","updated":"2025-05-23T16:04:27Z","published":"2025-05-23T16:04:27Z","title":"A Foundation Model Framework for Multi-View MRI Classification of\n  Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer","summary":"  Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.\n","authors":["Yumeng Zhang","Zohaib Salahuddin","Danial Khan","Shruti Atul Mali","Henry C. Woodruff","Sina Amirrajab","Eduardo Ibor-Crespo","Ana Jimenez-Pastor","Luis Marti-Bonmati","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2505.18058v1.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14888v3","updated":"2025-05-23T16:04:19Z","published":"2025-02-16T14:51:07Z","title":"Multi-Faceted Multimodal Monosemanticity","summary":"  Humans experience the world through multiple modalities, such as, vision,\nlanguage, and speech, making it natural to explore the commonality and\ndistinctions among them. In this work, we take a data-driven approach to\naddress this question by analyzing interpretable, monosemantic features\nextracted from deep multimodal models. Specifically, we investigate CLIP, a\nprominent visual-language representation model trained on massive image-text\npairs. Building on prior research in single-modal interpretability, we develop\na set of multi-modal interpretability tools and measures designed to\ndisentangle and analyze features learned from CLIP. Specifically, we introduce\nthe Modality Dominance Score (MDS) to attribute each CLIP feature to a specific\nmodality. We then map CLIP features into a more interpretable space, enabling\nus to categorize them into three distinct classes: vision features\n(single-modal), language features (single-modal), and visual-language features\n(cross-modal). Interestingly, this data-driven categorization closely aligns\nwith human intuitive understandings of different modalities. We further show\nthat this modality decomposition can benefit multiple downstream tasks,\nincluding reducing bias in gender detection, generating cross-modal adversarial\nexamples, and enabling modal-specific feature control in text-to-image\ngeneration. These results indicate that large-scale multimodal models, when\nequipped with task-agnostic interpretability tools, can offer valuable insights\ninto the relationships between different data modalities.\n","authors":["Hanqi Yan","Xiangxiang Cui","Lu Yin","Paul Pu Liang","Yulan He","Yifei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14888v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18053v1","updated":"2025-05-23T15:57:16Z","published":"2025-05-23T15:57:16Z","title":"FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware\n  Vision-Language Models Adaptation","summary":"  Prompt learning as a parameter-efficient method that has been widely adopted\nto adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt\ndesign requires domain expertise and iterative optimization, soft-prompt\nmethods rely heavily on task-specific hard labels, limiting their\ngeneralization to unseen categories. Recent popular distillation-based prompt\nlearning methods improve generalization by exploiting larger teacher VLMs and\nunsupervised knowledge transfer, yet their repetitive teacher model online\ninference sacrifices the inherent training efficiency advantage of prompt\nlearning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large\n{\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large\n{\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which\naddresses these issues by sharing soft supervision contexts across multiple\ntraining stages and implementing accelerated I/O. Furthermore, FDBPL introduces\na region-aware prompt learning paradigm with dual positive-negative prompt\nspaces to fully exploit randomly cropped regions that containing multi-level\ninformation. We propose a positive-negative space mutual learning mechanism\nbased on similarity-difference learning, enabling student CLIP models to\nrecognize correct semantics while learning to reject weakly related concepts,\nthereby improving zero-shot performance. Unlike existing distillation-based\nprompt learning methods that sacrifice parameter efficiency for generalization,\nFDBPL maintains dual advantages of parameter efficiency and strong downstream\ngeneralization. Comprehensive evaluations across 11 datasets demonstrate\nsuperior performance in base-to-new generalization, cross-dataset transfer, and\nrobustness tests, achieving $2.2\\times$ faster training speed.\n","authors":["Zherui Zhang","Jiaxin Wu","Changwei Wang","Rongtao Xu","Longzhao Huang","Wenhao Xu","Wenbo Xu","Li Guo","Shibiao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.18053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18052v1","updated":"2025-05-23T15:56:37Z","published":"2025-05-23T15:56:37Z","title":"BOTM: Echocardiography Segmentation via Bi-directional Optimal Token\n  Matching","summary":"  Existed echocardiography segmentation methods often suffer from anatomical\ninconsistency challenge caused by shape variation, partial observation and\nregion ambiguity with similar intensity across 2D echocardiographic sequences,\nresulting in false positive segmentation with anatomical defeated structures in\nchallenging low signal-to-noise ratio conditions. To provide a strong\nanatomical guarantee across different echocardiographic frames, we propose a\nnovel segmentation framework named BOTM (Bi-directional Optimal Token Matching)\nthat performs echocardiography segmentation and optimal anatomy transportation\nsimultaneously. Given paired echocardiographic images, BOTM learns to match two\nsets of discrete image tokens by finding optimal correspondences from a novel\nanatomical transportation perspective. We further extend the token matching\ninto a bi-directional cross-transport attention proxy to regulate the preserved\nanatomical consistency within the cardiac cyclic deformation in temporal\ndomain. Extensive experimental results show that BOTM can generate stable and\naccurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on\nTED), and provide a better matching interpretation with anatomical consistency\nguarantee.\n","authors":["Zhihua Liu","Lei Tong","Xilin He","Che Liu","Rossella Arcucci","Chen Jin","Huiyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.18052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18051v1","updated":"2025-05-23T15:56:35Z","published":"2025-05-23T15:56:35Z","title":"LookWhere? Efficient Visual Recognition by Learning Where to Look and\n  What to See from Self-Supervision","summary":"  Vision transformers are ever larger, more accurate, and more expensive to\ncompute. The expense is even more extreme at high resolution as the number of\ntokens grows quadratically with the image size. We turn to adaptive computation\nto cope with this cost by learning to predict where to compute. Our LookWhere\nmethod divides the computation between a low-resolution selector and a\nhigh-resolution extractor without ever processing the full high-resolution\ninput. We jointly pretrain the selector and extractor without task supervision\nby distillation from a self-supervised teacher, in effect, learning where and\nwhat to compute simultaneously. Unlike prior token reduction methods, which pay\nto save by pruning already-computed tokens, and prior token selection methods,\nwhich require complex and expensive per-task optimization, LookWhere\neconomically and accurately selects and extracts transferrable representations\nof images. We show that LookWhere excels at sparse recognition on\nhigh-resolution inputs (Traffic Signs), maintaining accuracy while reducing\nFLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks\nthat are global (ImageNet classification) or local (ADE20K segmentation),\nimproving accuracy while reducing time by 1.36x.\n","authors":["Anthony Fuller","Yousef Yassin","Junfeng Wen","Daniel G. Kyrollos","Tarek Ibrahim","James R. Green","Evan Shelhamer"],"pdf_url":"https://arxiv.org/pdf/2505.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18049v1","updated":"2025-05-23T15:54:11Z","published":"2025-05-23T15:54:11Z","title":"SpikeGen: Generative Framework for Visual Spike Stream Processing","summary":"  Neuromorphic Visual Systems, such as spike cameras, have attracted\nconsiderable attention due to their ability to capture clear textures under\ndynamic conditions. This capability effectively mitigates issues related to\nmotion and aperture blur. However, in contrast to conventional RGB modalities\nthat provide dense spatial information, these systems generate binary,\nspatially sparse frames as a trade-off for temporally rich visual streams. In\nthis context, generative models emerge as a promising solution to address the\ninherent limitations of sparse data. These models not only facilitate the\nconditional fusion of existing information from both spike and RGB modalities\nbut also enable the conditional generation based on latent priors. In this\nstudy, we introduce a robust generative processing framework named SpikeGen,\ndesigned for visual spike streams captured by spike cameras. We evaluate this\nframework across multiple tasks involving mixed spike-RGB modalities, including\nconditional image/video deblurring, dense frame reconstruction from spike\nstreams, and high-speed scene novel-view synthesis. Supported by comprehensive\nexperimental results, we demonstrate that leveraging the latent space operation\nabilities of generative models allows us to effectively address the sparsity of\nspatial information while fully exploiting the temporal richness of spike\nstreams, thereby promoting a synergistic enhancement of different visual\nmodalities.\n","authors":["Gaole Dai","Menghang Dong","Rongyu Zhang","Ruichuan An","Shanghang Zhang","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2505.18049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18048v1","updated":"2025-05-23T15:52:31Z","published":"2025-05-23T15:52:31Z","title":"SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded\n  Scenarios","summary":"  Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).\n","authors":["Simon Malzard","Nitish Mital","Richard Walters","Victoria Nockles","Raghuveer Rao","Celso M. De Melo"],"pdf_url":"https://arxiv.org/pdf/2505.18048v1.pdf","comment":"19 pages, 2 images"},{"id":"http://arxiv.org/abs/2505.18047v1","updated":"2025-05-23T15:52:26Z","published":"2025-05-23T15:52:26Z","title":"RestoreVAR: Visual Autoregressive Generation for All-in-One Image\n  Restoration","summary":"  The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.\n","authors":["Sudarshan Rajagopalan","Kartik Narayan","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2505.18047v1.pdf","comment":"Project page: https://sudraj2002.github.io/restorevarpage/"},{"id":"http://arxiv.org/abs/2505.01476v2","updated":"2025-05-23T15:45:22Z","published":"2025-05-02T14:52:34Z","title":"CostFilter-AD: Enhancing Anomaly Detection through Matching Cost\n  Filtering","summary":"  Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an\ninput image with respect to normal samples. Either by reconstructing normal\ncounterparts (reconstruction-based) or by learning an image feature embedding\nspace (embedding-based), existing approaches fundamentally rely on image-level\nor feature-level matching to derive anomaly scores. Often, such a matching\nprocess is inaccurate yet overlooked, leading to sub-optimal detection. To\naddress this issue, we introduce the concept of cost filtering, borrowed from\nclassical matching tasks, such as depth and flow estimation, into the UAD\nproblem. We call this approach {\\em CostFilter-AD}. Specifically, we first\nconstruct a matching cost volume between the input and normal samples,\ncomprising two spatial dimensions and one matching dimension that encodes\npotential matches. To refine this, we propose a cost volume filtering network,\nguided by the input observation as an attention query across multiple feature\nlayers, which effectively suppresses matching noise while preserving edge\nstructures and capturing subtle anomalies. Designed as a generic\npost-processing plug-in, CostFilter-AD can be integrated with either\nreconstruction-based or embedding-based methods. Extensive experiments on\nMVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for\nboth single- and multi-class UAD tasks. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.\n","authors":["Zhe Zhang","Mingxiu Cai","Hanxiao Wang","Gaochang Wu","Tianyou Chai","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.01476v2.pdf","comment":"25 pages, 12 figures, 20 tables, accepted by Forty-Second\n  International Conference on Machine Learning ( ICML 2025 ), link:\n  https://icml.cc/virtual/2025/poster/46359"},{"id":"http://arxiv.org/abs/2505.18039v1","updated":"2025-05-23T15:42:52Z","published":"2025-05-23T15:42:52Z","title":"Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via\n  Cross-Architecture CLIP Distillation","summary":"  Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.\n","authors":["Li Zhong","Ahmed Ghazal","Jun-Jun Wan","Frederik Zilly","Patrick Mackens","Joachim E. Vollrath","Bogdan Sorin Coseriu"],"pdf_url":"https://arxiv.org/pdf/2505.18039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16770v2","updated":"2025-05-23T15:40:22Z","published":"2025-05-22T15:11:57Z","title":"RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs","summary":"  The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv\n","authors":["Meng-Hao Guo","Xuanyu Chu","Qianrui Yang","Zhe-Han Mo","Yiqing Shen","Pei-lin Li","Xinjie Lin","Jinnian Zhang","Xin-Sheng Chen","Yi Zhang","Kiyohiro Nakayama","Zhengyang Geng","Houwen Peng","Han Hu","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2505.16770v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.18035v1","updated":"2025-05-23T15:39:07Z","published":"2025-05-23T15:39:07Z","title":"CAMME: Adaptive Deepfake Image Detection with Multi-Modal\n  Cross-Attention","summary":"  The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.\n","authors":["Naseem Khan","Tuan Nguyen","Amine Bermak","Issa Khalil"],"pdf_url":"https://arxiv.org/pdf/2505.18035v1.pdf","comment":"20 pages, 8 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2505.18032v1","updated":"2025-05-23T15:36:22Z","published":"2025-05-23T15:36:22Z","title":"Mahalanobis++: Improving OOD Detection via Feature Normalization","summary":"  Detecting out-of-distribution (OOD) examples is an important task for\ndeploying reliable machine learning models in safety-critial applications.\nWhile post-hoc methods based on the Mahalanobis distance applied to pre-logit\nfeatures are among the most effective for ImageNet-scale OOD detection, their\nperformance varies significantly across models. We connect this inconsistency\nto strong variations in feature norms, indicating severe violations of the\nGaussian assumption underlying the Mahalanobis distance estimation. We show\nthat simple $\\ell_2$-normalization of the features mitigates this problem\neffectively, aligning better with the premise of normally distributed data with\nshared covariance matrix. Extensive experiments on 44 models across diverse\narchitectures and pretraining schemes show that $\\ell_2$-normalization improves\nthe conventional Mahalanobis distance-based approaches significantly and\nconsistently, and outperforms other recently proposed OOD detection methods.\n","authors":["Maximilian Mueller","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18028v1","updated":"2025-05-23T15:34:08Z","published":"2025-05-23T15:34:08Z","title":"Knot So Simple: A Minimalistic Environment for Spatial Reasoning","summary":"  We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym.\n","authors":["Zizhao Chen","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2505.18028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18025v1","updated":"2025-05-23T15:28:43Z","published":"2025-05-23T15:28:43Z","title":"3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair\n  and Fast Method Evaluation","summary":"  Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining.\n","authors":["Evangelos Sariyanidi","Claudio Ferrari","Federico Nocentini","Stefano Berretti","Andrea Cavallaro","Birkan Tunc"],"pdf_url":"https://arxiv.org/pdf/2505.18025v1.pdf","comment":"To be published in IEEE International Conference on Automatic Face\n  and Gesture Recognition, 2025"},{"id":"http://arxiv.org/abs/2505.18024v1","updated":"2025-05-23T15:28:03Z","published":"2025-05-23T15:28:03Z","title":"A Wavelet-based Stereo Matching Framework for Solving Frequency\n  Convergence Inconsistency","summary":"  We find that the EPE evaluation metrics of RAFT-stereo converge\ninconsistently in the low and high frequency regions, resulting high frequency\ndegradation (e.g., edges and thin objects) during the iterative process. The\nunderlying reason for the limited performance of current iterative methods is\nthat it optimizes all frequency components together without distinguishing\nbetween high and low frequencies. We propose a wavelet-based stereo matching\nframework (Wavelet-Stereo) for solving frequency convergence inconsistency.\nSpecifically, we first explicitly decompose an image into high and low\nfrequency components using discrete wavelet transform. Then, the high-frequency\nand low-frequency components are fed into two different multi-scale frequency\nfeature extractors. Finally, we propose a novel LSTM-based high-frequency\npreservation update operator containing an iterative frequency adapter to\nprovide adaptive refined high-frequency features at different iteration steps\nby fine-tuning the initial high-frequency features. By processing high and low\nfrequency components separately, our framework can simultaneously refine\nhigh-frequency information in edges and low-frequency information in smooth\nregions, which is especially suitable for challenging scenes with fine details\nand textures in the distance. Extensive experiments demonstrate that our\nWavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both\nthe KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/SIA-IDE/Wavelet-Stereo).\n","authors":["Xiaobao Wei","Jiawei Liu","Dongbo Yang","Junda Cheng","Changyong Shu","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2505.18024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18022v1","updated":"2025-05-23T15:27:57Z","published":"2025-05-23T15:27:57Z","title":"RemoteSAM: Towards Segment Anything for Earth Observation","summary":"  We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.\n","authors":["Liang Yao","Fan Liu","Delong Chen","Chuanyi Zhang","Yijun Wang","Ziyun Chen","Wei Xu","Shimin Di","Yuhui Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.18022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18021v1","updated":"2025-05-23T15:27:46Z","published":"2025-05-23T15:27:46Z","title":"Building Floor Number Estimation from Crowdsourced Street-Level Images:\n  Munich Dataset and Baseline Method","summary":"  Accurate information on the number of building floors, or above-ground\nstoreys, is essential for household estimation, utility provision, risk\nassessment, evacuation planning, and energy modeling. Yet large-scale\nfloor-count data are rarely available in cadastral and 3D city databases. This\nstudy proposes an end-to-end deep learning framework that infers floor numbers\ndirectly from unrestricted, crowdsourced street-level imagery, avoiding\nhand-crafted features and generalizing across diverse facade styles. To enable\nbenchmarking, we release the Munich Building Floor Dataset, a public set of\nover 6800 geo-tagged images collected from Mapillary and targeted field\nphotography, each paired with a verified storey label. On this dataset, the\nproposed classification-regression network attains 81.2% exact accuracy and\npredicts 97.9% of buildings within +/-1 floor. The method and dataset together\noffer a scalable route to enrich 3D city models with vertical information and\nlay a foundation for future work in urban informatics, remote sensing, and\ngeographic information science. Source code and data will be released under an\nopen license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.\n","authors":["Yao Sun","Sining Chen","Yifan Tian","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.18021v1.pdf","comment":"Code and data: https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark"},{"id":"http://arxiv.org/abs/2505.14043v3","updated":"2025-05-23T15:20:11Z","published":"2025-05-20T07:39:27Z","title":"Selective Structured State Space for Multispectral-fused Small Target\n  Detection","summary":"  Target detection in high-resolution remote sensing imagery faces challenges\ndue to the low recognition accuracy of small targets and high computational\ncosts. The computational complexity of the Transformer architecture increases\nquadratically with image resolution, while Convolutional Neural Networks (CNN)\narchitectures are forced to stack deeper convolutional layers to expand their\nreceptive fields, leading to an explosive growth in computational demands. To\naddress these computational constraints, we leverage Mamba's linear complexity\nfor efficiency. However, Mamba's performance declines for small targets,\nprimarily because small targets occupy a limited area in the image and have\nlimited semantic information. Accurate identification of these small targets\nnecessitates not only Mamba's global attention capabilities but also the\nprecise capture of fine local details. To this end, we enhance Mamba by\ndeveloping the Enhanced Small Target Detection (ESTD) module and the\nConvolutional Attention Residual Gate (CARG) module. The ESTD module bolsters\nlocal attention to capture fine-grained details, while the CARG module, built\nupon Mamba, emphasizes spatial and channel-wise information, collectively\nimproving the model's ability to capture distinctive representations of small\ntargets. Additionally, to highlight the semantic representation of small\ntargets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for\nmultispectral fusion, which enhances target features by effectively fusing\nvisible and infrared multimodal information.\n","authors":["Qianqian Zhang","WeiJun Wang","Yunxing Liu","Li Zhou","Hao Zhao","Junshe An","Zihan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14043v3.pdf","comment":"This work was submitted to CVPR 2025, but was rejected after being\n  reviewed by 7 reviewers. After revision, it is currently under review"},{"id":"http://arxiv.org/abs/2505.18015v1","updated":"2025-05-23T15:17:45Z","published":"2025-05-23T15:17:45Z","title":"SemSegBench & DetecBench: Benchmarking Reliability and Generalization\n  Beyond Classification","summary":"  Reliability and generalization in deep learning are predominantly studied in\nthe context of image classification. Yet, real-world applications in\nsafety-critical domains involve a broader set of semantic tasks, such as\nsemantic segmentation and object detection, which come with a diverse set of\ndedicated model architectures. To facilitate research towards robust model\ndesign in segmentation and detection, our primary objective is to provide\nbenchmarking tools regarding robustness to distribution shifts and adversarial\nmanipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,\nalong with the most extensive evaluation to date on the reliability and\ngeneralization of semantic segmentation and object detection models. In\nparticular, we benchmark 76 segmentation models across four datasets and 61\nobject detectors across two datasets, evaluating their performance under\ndiverse adversarial attacks and common corruptions. Our findings reveal\nsystematic weaknesses in state-of-the-art models and uncover key trends based\non architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are\nopen-sourced in our GitHub repository\n(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)\nalong with our complete set of total 6139 evaluations. We anticipate the\ncollected data to foster and encourage future research towards improved model\nreliability beyond classification.\n","authors":["Shashank Agnihotri","David Schader","Jonas Jakubassa","Nico Sharei","Simon Kral","Mehmet Ege Kaar","Ruben Weber","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2505.18015v1.pdf","comment":"First seven listed authors have equal contribution. GitHub:\n  https://github.com/shashankskagnihotri/benchmarking_reliability_generalization.\n  arXiv admin note: text overlap with arXiv:2505.05091"},{"id":"http://arxiv.org/abs/2505.18010v1","updated":"2025-05-23T15:14:27Z","published":"2025-05-23T15:14:27Z","title":"Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation\n  Estimation Using Spectral Imaging","summary":"  Accurate, real-time monitoring of tissue ischemia is crucial to understand\ntissue health and guide surgery. Spectral imaging shows great potential for\ncontactless and intraoperative monitoring of tissue oxygenation. Due to the\ndifficulty of obtaining direct reference oxygenation values, conventional\nmethods are based on linear unmixing techniques. These are prone to assumptions\nand these linear relations may not always hold in practice. In this work, we\npresent deep learning approaches for real-time tissue oxygenation estimation\nusing Monte-Carlo simulated spectra. We train a fully connected neural network\n(FCN) and a convolutional neural network (CNN) for this task and propose a\ndomain-adversarial training approach to bridge the gap between simulated and\nreal clinical spectral data. Results demonstrate that these deep learning\nmodels achieve a higher correlation with capillary lactate measurements, a\nwell-known marker of hypoxia, obtained during spectral imaging in surgery,\ncompared to traditional linear unmixing. Notably, domain-adversarial training\neffectively reduces the domain gap, optimizing performance in real clinical\nsettings.\n","authors":["Jens De Winne","Siri Willems","Siri Luthman","Danilo Babin","Hiep Luong","Wim Ceelen"],"pdf_url":"https://arxiv.org/pdf/2505.18010v1.pdf","comment":"Provisionally accepted to the MICCAI 2025 conference"},{"id":"http://arxiv.org/abs/2505.17994v1","updated":"2025-05-23T14:59:44Z","published":"2025-05-23T14:59:44Z","title":"Segment Anyword: Mask Prompt Inversion for Open-Set Grounded\n  Segmentation","summary":"  Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field.\n","authors":["Zhihua Liu","Amrutha Saseendran","Lei Tong","Xilin He","Fariba Yousefi","Nikolay Burlutskiy","Dino Oglic","Tom Diethe","Philip Teare","Huiyu Zhou","Chen Jin"],"pdf_url":"https://arxiv.org/pdf/2505.17994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17992v1","updated":"2025-05-23T14:58:34Z","published":"2025-05-23T14:58:34Z","title":"Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid\n  Pose Recovery on Limited Datasets","summary":"  3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods.\n","authors":["Fahd Alhamazani","Yu-Kun Lai","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2505.17992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17982v1","updated":"2025-05-23T14:48:32Z","published":"2025-05-23T14:48:32Z","title":"Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language\n  Alignment and Modeling","summary":"  Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL\n","authors":["Bryan Wong","Jong Woo Kim","Huazhu Fu","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2505.17982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17973v1","updated":"2025-05-23T14:41:41Z","published":"2025-05-23T14:41:41Z","title":"To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile\n  Mapping Cameras to Textured Semantic 3D Building Models","summary":"  Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue\n","authors":["Simone Gaisbauer","Prabin Gyawali","Qilin Zhang","Olaf Wysocki","Boris Jutzi"],"pdf_url":"https://arxiv.org/pdf/2505.17973v1.pdf","comment":"Accepted to MMT, Xiamen, China; ISPRS Annals"},{"id":"http://arxiv.org/abs/2505.17972v1","updated":"2025-05-23T14:40:50Z","published":"2025-05-23T14:40:50Z","title":"MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from\n  Long EEG Recordings","summary":"  Feature engineering for generalized seizure detection models remains a\nsignificant challenge. Recently proposed models show variable performance\ndepending on the training data and remain ineffective at accurately\ndistinguishing artifacts from seizure data. In this study, we propose a novel\nend-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which\nefficiently distinguishes seizure events from background electroencephalogram\n(EEG) and artifacts/noise by capturing both temporal dependencies across\ndifferent time frames and spatial relationships between channels. The model has\nthree modules: convolution, feature extraction, and predictor. The convolution\nmodule extracts features through depth-wise and spatio-temporal convolution.\nThe feature extraction module individually reduces the feature dimension\nextracted from EEG segments and their sub-segments. Subsequently, the extracted\nfeatures are concatenated into a single vector for classification using a fully\nconnected classifier called the predictor module. In addition, an anomaly\nscore-based post-classification processing technique was introduced to reduce\nthe false-positive rates of the model. Experimental results were reported and\nanalyzed using different parameter settings and datasets (Siena (public) and\nJuntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the\nconventional non-multiresolution approach, improving the F1 scores from 0.177\nto 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%\nand 20.62%, respectively.\n","authors":["Kazi Mahmudul Hassan","Xuyang Zhao","Hidenori Sugano","Toshihisa Tanaka"],"pdf_url":"https://arxiv.org/pdf/2505.17972v1.pdf","comment":"26 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2505.17971v1","updated":"2025-05-23T14:40:09Z","published":"2025-05-23T14:40:09Z","title":"Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In\n  Silico Clinical Trials for Virtual Biopsy-based Risk Assessment","summary":"  We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice.\n","authors":["Danial Khan","Zohaib Salahuddin","Yumeng Zhang","Sheng Kuang","Shruti Atul Mali","Henry C. Woodruff","Sina Amirrajab","Rachel Cavill","Eduardo Ibor-Crespo","Ana Jimenez-Pastor","Adrian Galiana-Bordera","Paula Jimenez Gomez","Luis Marti-Bonmati","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2505.17971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15021v2","updated":"2025-05-23T14:36:29Z","published":"2025-02-20T20:16:10Z","title":"Simpler Fast Vision Transformers with a Jumbo CLS Token","summary":"  We introduce a simple enhancement of vision transformers (ViTs) to improve\naccuracy while maintaining throughput. Our approach, Jumbo, creates a wider CLS\ntoken, which is split to match the patch token width before attention,\nprocessed with self-attention, and reassembled. After attention, Jumbo applies\na dedicated, wider FFN to this token. Since there is only one Jumbo token, its\ncost is minimal, and because we share this FFN across layers, its parameter\ncount is controlled. Jumbo significantly improves over ViT+Registers on\nImageNet-1K and ImageNet-21K. These gains are largest at small sizes / high\nspeeds, e.g., ViT-nano+Jumbo outperforms ViT-nano+Registers by 13%. In fact,\nour Jumbo models are so efficient that they outperform specialized\ncompute-efficient models while preserving the architectural advantages of plain\nViTs, such as support for token dropping and other modalities. Accordingly, we\ndemonstrate that Jumbo excels in these two settings via masked autoencoding and\non a suite of time series benchmarks. Code and weights available:\nhttps://github.com/antofuller/jumbo\n","authors":["Anthony Fuller","Yousef Yassin","Daniel G. Kyrollos","Evan Shelhamer","James R. Green"],"pdf_url":"https://arxiv.org/pdf/2502.15021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17966v1","updated":"2025-05-23T14:35:56Z","published":"2025-05-23T14:35:56Z","title":"Is Single-View Mesh Reconstruction Ready for Robotics?","summary":"  This paper evaluates single-view mesh reconstruction models for creating\ndigital twin environments in robot manipulation. Recent advances in computer\nvision for 3D reconstruction from single viewpoints present a potential\nbreakthrough for efficiently creating virtual replicas of physical environments\nfor robotics contexts. However, their suitability for physics simulations and\nrobotics applications remains unexplored. We establish benchmarking criteria\nfor 3D reconstruction in robotics contexts, including handling typical inputs,\nproducing collision-free and stable reconstructions, managing occlusions, and\nmeeting computational constraints. Our empirical evaluation using realistic\nrobotics datasets shows that despite success on computer vision benchmarks,\nexisting approaches fail to meet robotics-specific requirements. We\nquantitively examine limitations of single-view reconstruction for practical\nrobotics implementation, in contrast to prior work that focuses on multi-view\napproaches. Our findings highlight critical gaps between computer vision\nadvances and robotics needs, guiding future research at this intersection.\n","authors":["Frederik Nolte","Bernhard Schlkopf","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2505.17966v1.pdf","comment":"20 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.17959v1","updated":"2025-05-23T14:31:36Z","published":"2025-05-23T14:31:36Z","title":"Mind the Domain Gap: Measuring the Domain Gap Between Real-World and\n  Synthetic Point Clouds for Automated Driving Development","summary":"  Owing to the typical long-tail data distribution issues, simulating\ndomain-gap-free synthetic data is crucial in robotics, photogrammetry, and\ncomputer vision research. The fundamental challenge pertains to credibly\nmeasuring the difference between real and simulated data. Such a measure is\nvital for safety-critical applications, such as automated driving, where\nout-of-domain samples may impact a car's perception and cause fatal accidents.\nPrevious work has commonly focused on simulating data on one scene and\nanalyzing performance on a different, real-world scene, hampering the disjoint\nanalysis of domain gap coming from networks' deficiencies, class definitions,\nand object representation. In this paper, we propose a novel approach to\nmeasuring the domain gap between the real world sensor observations and\nsimulated data representing the same location, enabling comprehensive domain\ngap analysis. To measure such a domain gap, we introduce a novel metric\nDoGSS-PCL and evaluation assessing the geometric and semantic quality of the\nsimulated point cloud. Our experiments corroborate that the introduced approach\ncan be used to measure the domain gap. The tests also reveal that synthetic\nsemantic point clouds may be used for training deep neural networks,\nmaintaining the performance at the 50/50 real-to-synthetic ratio. We strongly\nbelieve that this work will facilitate research on credible data simulation and\nallow for at-scale deployment in automated driving testing and digital\ntwinning.\n","authors":["Nguyen Duc","Yan-Ling Lai","Patrick Madlindl","Xinyuan Zhu","Benedikt Schwab","Olaf Wysocki","Ludwig Hoegner","Thomas H. Kolbe"],"pdf_url":"https://arxiv.org/pdf/2505.17959v1.pdf","comment":"Submitted to PFG Journal of Photogrammetry, Remote Sensing and\n  Geoinformation Science"},{"id":"http://arxiv.org/abs/2502.08634v2","updated":"2025-05-23T14:30:25Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using\n  Multi-scale Implicit Neural Representation","summary":"  High-resolution whole-brain in vivo MR imaging at mesoscale resolutions\nremains challenging due to long scan durations, motion artifacts, and limited\nsignal-to-noise ratio (SNR). This study proposes Rotating-view super-resolution\n(ROVER)-MRI, an unsupervised framework based on multi-scale implicit neural\nrepresentations (INR), enabling efficient recovery of fine anatomical details\nfrom multi-view thick-slice acquisitions. ROVER-MRI employs coordinate-based\nneural networks to implicitly and continuously encode image structures at\nmultiple spatial scales, simultaneously modeling anatomical continuity and\ncorrecting inter-view motion through an integrated registration mechanism.\nValidation on ex-vivo monkey brain data and multiple in-vivo human datasets\ndemonstrates substantially improved reconstruction performance compared to\nbicubic interpolation and state-of-the-art regularized least-squares\nsuper-resolution reconstruction (LS-SRR) with 2-fold reduction in scan time.\nNotably, ROVER-MRI achieves an unprecedented whole-brain in-vivo T2-weighted\nimaging at 180 micron isotropic resolution in only 17 minutes of scan time on a\n7T scanner with 22.4% lower relative error compared to LS-SRR. We also\ndemonstrate improved SNR using ROVER-MRI compared to a time-matched 3D GRE\nacquisition. Quantitative results on several datasets demonstrate better\nsharpness of the reconstructed images with ROVER-MRI for different\nsuper-resolution factors (5 to 11). These findings highlight ROVER-MRI's\npotential as a rapid, accurate, and motion-resilient mesoscale imaging\nsolution, promising substantial advantages for neuroimaging studies.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17955v1","updated":"2025-05-23T14:29:52Z","published":"2025-05-23T14:29:52Z","title":"Diffusion Classifiers Understand Compositionality, but Conditions Apply","summary":"  Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.\n","authors":["Yujin Jeong","Arnas Uselis","Seong Joon Oh","Anna Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2505.17955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17951v1","updated":"2025-05-23T14:27:12Z","published":"2025-05-23T14:27:12Z","title":"SplatCo: Structure-View Collaborative Gaussian Splatting for\n  Detail-Preserving Rendering of Large-Scale Unbounded Scenes","summary":"  We present SplatCo, a structure-view collaborative Gaussian splatting\nframework for high-fidelity rendering of complex outdoor environments. SplatCo\nbuilds upon two novel components: (1) a cross-structure collaboration module\nthat combines global tri-plane representations, which capture coarse scene\nlayouts, with local context grid features that represent fine surface details.\nThis fusion is achieved through a novel hierarchical compensation strategy,\nensuring both global consistency and local detail preservation; and (2) a\ncross-view assisted training strategy that enhances multi-view consistency by\nsynchronizing gradient updates across viewpoints, applying visibility-aware\ndensification, and pruning overfitted or inaccurate Gaussians based on\nstructural consistency. Through joint optimization of structural representation\nand multi-view coherence, SplatCo effectively reconstructs fine-grained\ngeometric structures and complex textures in large-scale scenes. Comprehensive\nevaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,\nTanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo\nconsistently achieves higher reconstruction quality than state-of-the-art\nmethods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These\nresults establish a new benchmark for high-fidelity rendering of large-scale\nunbounded scenes. Code and additional information are available at\nhttps://github.com/SCUT-BIP-Lab/SplatCo.\n","authors":["Haihong Xiao","Jianan Zou","Yuxin Zhou","Ying He","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2505.17951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10541v2","updated":"2025-05-23T14:26:54Z","published":"2025-05-15T17:52:40Z","title":"Exploring Implicit Visual Misunderstandings in Multimodal Large Language\n  Models through Attention Analysis","summary":"  Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.\n","authors":["Pengfei Wang","Guohai Xu","Weinong Wang","Junjie Yang","Jie Lou","Yunhua Xue"],"pdf_url":"https://arxiv.org/pdf/2505.10541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15425v2","updated":"2025-05-23T14:16:48Z","published":"2025-05-21T12:08:31Z","title":"On the Robustness of Medical Vision-Language Models: Are they Truly\n  Generalizable?","summary":"  Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities.\n","authors":["Raza Imam","Rufael Marew","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2505.15425v2.pdf","comment":"Dataset and Code is available at\n  https://github.com/BioMedIA-MBZUAI/RobustMedCLIP Accepted at: Medical Image\n  Understanding and Analysis (MIUA) 2025"},{"id":"http://arxiv.org/abs/2505.17931v1","updated":"2025-05-23T14:07:21Z","published":"2025-05-23T14:07:21Z","title":"AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation\n  of Foundation Models","summary":"  Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.\n","authors":["Xingjian Li","Qifeng Wu","Colleen Que","Yiran Ding","Adithya S. Ubaradka","Jianhua Xing","Tianyang Wang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2505.17931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15176v2","updated":"2025-05-23T13:59:58Z","published":"2025-05-21T06:46:09Z","title":"Exploring Generalized Gait Recognition: Reducing Redundancy and Noise\n  within Indoor and Outdoor Datasets","summary":"  Generalized gait recognition, which aims to achieve robust performance across\ndiverse domains, remains a challenging problem due to severe domain shifts in\nviewpoints, appearances, and environments. While mixed-dataset training is\nwidely used to enhance generalization, it introduces new obstacles including\ninter-dataset optimization conflicts and redundant or noisy samples, both of\nwhich hinder effective representation learning. To address these challenges, we\npropose a unified framework that systematically improves cross-domain gait\nrecognition. First, we design a disentangled triplet loss that isolates\nsupervision signals across datasets, mitigating gradient conflicts during\noptimization. Second, we introduce a targeted dataset distillation strategy\nthat filters out the least informative 20\\% of training samples based on\nfeature redundancy and prediction uncertainty, enhancing data efficiency.\nExtensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that\nour method significantly improves cross-dataset recognition for both GaitBase\nand DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will\nbe released at https://github.com/li1er3/Generalized_Gait.\n","authors":["Qian Zhou","Xianda Guo","Jilong Wang","Chuanfu Shen","Zhongyuan Wang","Hua Zou","Qin Zou","Chao Liang","Long Chen","Gang Wu"],"pdf_url":"https://arxiv.org/pdf/2505.15176v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.17921v1","updated":"2025-05-23T13:59:02Z","published":"2025-05-23T13:59:02Z","title":"Evaluation of Few-Shot Learning Methods for Kidney Stone Type\n  Recognition in Ureteroscopy","summary":"  Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset.\n","authors":["Carlos Salazar-Ruiz","Francisco Lopez-Tiro","Ivan Reyes-Amezcua","Clement Larose","Gilberto Ochoa-Ruiz","Christian Daul"],"pdf_url":"https://arxiv.org/pdf/2505.17921v1.pdf","comment":"6 pages, 3 figures, 3 tables, conference, cbms25"},{"id":"http://arxiv.org/abs/2505.17915v1","updated":"2025-05-23T13:56:40Z","published":"2025-05-23T13:56:40Z","title":"Promptable cancer segmentation using minimal expert-curated data","summary":"  Automated segmentation of cancer on medical images can aid targeted\ndiagnostic and therapeutic procedures. However, its adoption is limited by the\nhigh cost of expert annotations required for training and inter-observer\nvariability in datasets. While weakly-supervised methods mitigate some\nchallenges, using binary histology labels for training as opposed to requiring\nfull segmentation, they require large paired datasets of histology and images,\nwhich are difficult to curate. Similarly, promptable segmentation aims to allow\nsegmentation with no re-training for new tasks at inference, however, existing\nmodels perform poorly on pathological regions, again necessitating large\ndatasets for training. In this work we propose a novel approach for promptable\nsegmentation requiring only 24 fully-segmented images, supplemented by 8\nweakly-labelled images, for training. Curating this minimal data to a high\nstandard is relatively feasible and thus issues with the cost and variability\nof obtaining labels can be mitigated. By leveraging two classifiers, one\nweakly-supervised and one fully-supervised, our method refines segmentation\nthrough a guided search process initiated by a single-point prompt. Our\napproach outperforms existing promptable segmentation methods, and performs\ncomparably with fully-supervised methods, for the task of prostate cancer\nsegmentation, while using substantially less annotated data (up to 100X less).\nThis enables promptable segmentation with very minimal labelled data, such that\nthe labels can be curated to a very high standard.\n","authors":["Lynn Karam","Yipei Wang","Veeru Kasivisvanathan","Mirabela Rusu","Yipeng Hu","Shaheer U. Saeed"],"pdf_url":"https://arxiv.org/pdf/2505.17915v1.pdf","comment":"Accepted at Medical Image Understanding and Analysis (MIUA) 2025"},{"id":"http://arxiv.org/abs/2505.17912v1","updated":"2025-05-23T13:56:06Z","published":"2025-05-23T13:56:06Z","title":"UltraBoneUDF: Self-supervised Bone Surface Reconstruction from\n  Ultrasound Based on Neural Unsigned Distance Functions","summary":"  Background: Bone surface reconstruction plays a critical role in\ncomputer-assisted orthopedic surgery. Compared to traditional imaging\nmodalities such as CT and MRI, ultrasound offers a radiation-free,\ncost-effective, and portable alternative. Continuous bone surface\nreconstruction can be employed for many clinical applications. However, due to\nthe inherent limitations of ultrasound imaging, B-mode ultrasound typically\ncapture only partial bone surfaces. Existing reconstruction methods struggle\nwith such incomplete data, leading to artifacts and increased reconstruction\nerrors. Effective techniques for accurately reconstructing thin and open bone\nsurfaces from real-world 3D ultrasound volumes remain lacking. Methods: We\npropose UltraBoneUDF, a self-supervised framework designed for reconstructing\nopen bone surfaces from ultrasound using neural Unsigned Distance Functions. To\nenhance reconstruction quality, we introduce a novel global feature extractor\nthat effectively fuses ultrasound-specific image characteristics. Additionally,\nwe present a novel loss function based on local tangent plane optimization that\nsubstantially improves surface reconstruction quality. UltraBoneUDF and\nbaseline models are extensively evaluated on four open-source datasets.\nResults: Qualitative results highlight the limitations of the state-of-the-art\nmethods for open bone surface reconstruction and demonstrate the effectiveness\nof UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms\ncompeting methods across all evaluated datasets for both open and closed bone\nsurface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the\nUltraBones100k dataset (39.6\\% improvement compared to the SOTA), 0.23 mm on\nthe OpenBoneCT dataset (69.3\\% improvement), 0.18 mm on the ClosedBoneCT\ndataset (70.2\\% improvement), and 0.05 mm on the Prostate dataset (55.3\\%\nimprovement).\n","authors":["Luohong Wu","Matthias Seibold","Nicola A. Cavalcanti","Giuseppe Loggia","Lisa Reissner","Bastian Sigrist","Jonas Hein","Lilian Calvet","Arnd Viehfer","Philipp Frnstahl"],"pdf_url":"https://arxiv.org/pdf/2505.17912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17911v1","updated":"2025-05-23T13:55:56Z","published":"2025-05-23T13:55:56Z","title":"Object-level Cross-view Geo-localization with Location Enhancement and\n  Multi-Head Cross Attention","summary":"  Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet).\n","authors":["Zheyang Huang","Jagannath Aryal","Saeid Nahavandi","Xuequan Lu","Chee Peng Lim","Lei Wei","Hailing Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17910v1","updated":"2025-05-23T13:53:23Z","published":"2025-05-23T13:53:23Z","title":"DiffusionReward: Enhancing Blind Face Restoration through Reward\n  Feedback Learning","summary":"  Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward.\n","authors":["Bin Wu","Wei Wang","Yahui Liu","Zixiang Li","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.17910v1.pdf","comment":"22 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.17908v1","updated":"2025-05-23T13:53:03Z","published":"2025-05-23T13:53:03Z","title":"ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback","summary":"  With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind\n","authors":["Litao Guo","Xinli Xu","Luozhou Wang","Jiantao Lin","Jinsong Zhou","Zixin Zhang","Bolan Su","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.17908v1.pdf","comment":"Project page: https://github.com/LitaoGuo/ComfyMind"},{"id":"http://arxiv.org/abs/2505.17905v1","updated":"2025-05-23T13:52:15Z","published":"2025-05-23T13:52:15Z","title":"Semantic segmentation with reward","summary":"  In real-world scenarios, pixel-level labeling is not always available.\nSometimes, we need a semantic segmentation network, and even a visual encoder\ncan have a high compatibility, and can be trained using various types of\nfeedback beyond traditional labels, such as feedback that indicates the quality\nof the parsing results. To tackle this issue, we proposed RSS (Reward in\nSemantic Segmentation), the first practical application of reward-based\nreinforcement learning on pure semantic segmentation offered in two granular\nlevels (pixel-level and image-level). RSS incorporates various novel\ntechnologies, such as progressive scale rewards (PSR) and pair-wise spatial\ndifference (PSD), to ensure that the reward facilitates the convergence of the\nsemantic segmentation network, especially under image-level rewards.\nExperiments and visualizations on benchmark datasets demonstrate that the\nproposed RSS can successfully ensure the convergence of the semantic\nsegmentation network on two levels of rewards. Additionally, the RSS, which\nutilizes an image-level reward, outperforms existing weakly supervised methods\nthat also rely solely on image-level signals during training.\n","authors":["Xie Ting","Ye Huang","Zhilin Liu","Lixin Duan"],"pdf_url":"https://arxiv.org/pdf/2505.17905v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2505.17893v1","updated":"2025-05-23T13:41:52Z","published":"2025-05-23T13:41:52Z","title":"Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and\n  Foundation-Model Signatures Across Multicentre NSCLC Data","summary":"  Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.\n","authors":["Shruti Atul Mali","Zohaib Salahuddin","Danial Khan","Yumeng Zhang","Henry C. Woodruff","Eduardo Ibor-Crespo","Ana Jimenez-Pastor","Luis Marti-Bonmati","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2505.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17884v1","updated":"2025-05-23T13:32:20Z","published":"2025-05-23T13:32:20Z","title":"Track Anything Annotate: Video annotation and dataset generation of\n  computer vision models","summary":"  Modern machine learning methods require significant amounts of labelled data,\nmaking the preparation process time-consuming and resource-intensive. In this\npaper, we propose to consider the process of prototyping a tool for annotating\nand generating training datasets based on video tracking and segmentation. We\nexamine different approaches to solving this problem, from technology selection\nthrough to final implementation. The developed prototype significantly\naccelerates dataset generation compared to manual annotation. All resources are\navailable at https://github.com/lnikioffic/track-anything-annotate\n","authors":["Nikita Ivanov","Mark Klimov","Dmitry Glukhikh","Tatiana Chernysheva","Igor Glukhikh"],"pdf_url":"https://arxiv.org/pdf/2505.17884v1.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.17883v1","updated":"2025-05-23T13:31:54Z","published":"2025-05-23T13:31:54Z","title":"FastCAV: Efficient Computation of Concept Activation Vectors for\n  Explaining Deep Neural Networks","summary":"  Concepts such as objects, patterns, and shapes are how humans understand the\nworld. Building on this intuition, concept-based explainability methods aim to\nstudy representations learned by deep neural networks in relation to\nhuman-understandable concepts. Here, Concept Activation Vectors (CAVs) are an\nimportant tool and can identify whether a model learned a concept or not.\nHowever, the computational cost and time requirements of existing CAV\ncomputation pose a significant challenge, particularly in large-scale,\nhigh-dimensional architectures. To address this limitation, we introduce\nFastCAV, a novel approach that accelerates the extraction of CAVs by up to\n63.6x (on average 46.4x). We provide a theoretical foundation for our approach\nand give concrete assumptions under which it is equivalent to established\nSVM-based methods. Our empirical results demonstrate that CAVs calculated with\nFastCAV maintain similar performance while being more efficient and stable. In\ndownstream applications, i.e., concept-based explanation methods, we show that\nFastCAV can act as a replacement leading to equivalent insights. Hence, our\napproach enables previously infeasible investigations of deep models, which we\ndemonstrate by tracking the evolution of concepts during model training.\n","authors":["Laines Schmalwasser","Niklas Penzel","Joachim Denzler","Julia Niebling"],"pdf_url":"https://arxiv.org/pdf/2505.17883v1.pdf","comment":"Accepted at ICML 2025, 27 pages, 20 figures, 9 tables"},{"id":"http://arxiv.org/abs/2505.17881v1","updated":"2025-05-23T13:31:13Z","published":"2025-05-23T13:31:13Z","title":"Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring\n  Factors Regularization","summary":"  In recent years, tensor decomposition-based approaches for hyperspectral\nanomaly detection (HAD) have gained significant attention in the field of\nremote sensing. However, existing methods often fail to fully leverage both the\nglobal correlations and local smoothness of the background components in\nhyperspectral images (HSIs), which exist in both the spectral and spatial\ndomains. This limitation results in suboptimal detection performance. To\nmitigate this critical issue, we put forward a novel HAD method named\nHAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)\nfactors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first\ndecomposed into background and anomaly components. The TR decomposition is then\nemployed to capture the spatial-spectral correlations within the background\ncomponent. Additionally, we introduce a unified and efficient nonconvex\nregularizer, induced by tensor singular value decomposition (TSVD), to\nsimultaneously encode the low-rankness and sparsity of the 3-D gradient TR\nfactors into a unique concise form. The above characterization scheme enables\nthe interpretable gradient TR factors to inherit the low-rankness and\nsmoothness of the original background. To further enhance anomaly detection, we\ndesign a generalized nonconvex regularization term to exploit the group\nsparsity of the anomaly component. To solve the resulting doubly nonconvex\nmodel, we develop a highly efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) framework. Experimental\nresults on several benchmark datasets demonstrate that our proposed method\noutperforms existing state-of-the-art (SOTA) approaches in terms of detection\naccuracy.\n","authors":["Wenjin Qin","Hailin Wang","Hao Shu","Feng Zhang","Jianjun Wang","Xiangyong Cao","Xi-Le Zhao","Gemine Vivone"],"pdf_url":"https://arxiv.org/pdf/2505.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15618v4","updated":"2025-05-23T13:25:56Z","published":"2024-10-21T03:40:29Z","title":"Erasing Undesirable Concepts in Diffusion Models with Adversarial\n  Preservation","summary":"  Diffusion models excel at generating visually striking content from text but\ncan inadvertently produce undesirable or harmful content when trained on\nunfiltered internet data. A practical solution is to selectively removing\ntarget concepts from the model, but this may impact the remaining concepts.\nPrior approaches have tried to balance this by introducing a loss term to\npreserve neutral content or a regularization term to minimize changes in the\nmodel parameters, yet resolving this trade-off remains challenging. In this\nwork, we propose to identify and preserving concepts most affected by parameter\nchanges, termed as \\textit{adversarial concepts}. This approach ensures stable\nerasure with minimal impact on the other concepts. We demonstrate the\neffectiveness of our method using the Stable Diffusion model, showing that it\noutperforms state-of-the-art erasure methods in eliminating unwanted content\nwhile maintaining the integrity of other unrelated elements. Our code is\navailable at https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.\n","authors":["Anh Bui","Long Vuong","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2410.15618v4.pdf","comment":"Erasing Concepts, Generative Unlearning, NeurIPS 2024. arXiv admin\n  note: text overlap with arXiv:2403.12326"},{"id":"http://arxiv.org/abs/2501.18950v3","updated":"2025-05-23T13:23:21Z","published":"2025-01-31T08:17:23Z","title":"Fantastic Targets for Concept Erasure in Diffusion Models and Where To\n  Find Them","summary":"  Concept erasure has emerged as a promising technique for mitigating the risk\nof harmful content generation in diffusion models by selectively unlearning\nundesirable concepts. The common principle of previous works to remove a\nspecific concept is to map it to a fixed generic concept, such as a neutral\nconcept or just an empty text prompt. In this paper, we demonstrate that this\nfixed-target strategy is suboptimal, as it fails to account for the impact of\nerasing one concept on the others. To address this limitation, we model the\nconcept space as a graph and empirically analyze the effects of erasing one\nconcept on the remaining concepts. Our analysis uncovers intriguing geometric\nproperties of the concept space, where the influence of erasing a concept is\nconfined to a local region. Building on this insight, we propose the Adaptive\nGuided Erasure (AGE) method, which \\emph{dynamically} selects optimal target\nconcepts tailored to each undesirable concept, minimizing unintended side\neffects. Experimental results show that AGE significantly outperforms\nstate-of-the-art erasure methods on preserving unrelated concepts while\nmaintaining effective erasure performance. Our code is published at\n{https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\n","authors":["Anh Bui","Trang Vu","Long Vuong","Trung Le","Paul Montague","Tamas Abraham","Junae Kim","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2501.18950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17867v1","updated":"2025-05-23T13:16:27Z","published":"2025-05-23T13:16:27Z","title":"Multi-task Learning For Joint Action and Gesture Recognition","summary":"  In practical applications, computer vision tasks often need to be addressed\nsimultaneously. Multitask learning typically achieves this by jointly training\na single deep neural network to learn shared representations, providing\nefficiency and improving generalization. Although action and gesture\nrecognition are closely related tasks, since they focus on body and hand\nmovements, current state-of-the-art methods handle them separately. In this\npaper, we show that employing a multi-task learning paradigm for action and\ngesture recognition results in more efficient, robust and generalizable visual\nrepresentations, by leveraging the synergies between these tasks. Extensive\nexperiments on multiple action and gesture datasets demonstrate that handling\nactions and gestures in a single architecture can achieve better performance\nfor both tasks in comparison to their single-task learning variants.\n","authors":["Konstantinos Spathis","Nikolaos Kardaris","Petros Maragos"],"pdf_url":"https://arxiv.org/pdf/2505.17867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17860v1","updated":"2025-05-23T13:13:00Z","published":"2025-05-23T13:13:00Z","title":"Multi-Person Interaction Generation from Two-Person Motion Priors","summary":"  Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.\n","authors":["Wenning Xu","Shiyu Fan","Paul Henderson","Edmond S. L. Ho"],"pdf_url":"https://arxiv.org/pdf/2505.17860v1.pdf","comment":"SIGGRAPH 2025 Conference Papers"},{"id":"http://arxiv.org/abs/2503.07435v3","updated":"2025-05-23T13:07:36Z","published":"2025-03-10T15:18:10Z","title":"Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds","summary":"  The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.\n","authors":["Riccardo Mazzieri","Jacopo Pegoraro","Michele Rossi"],"pdf_url":"https://arxiv.org/pdf/2503.07435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17844v1","updated":"2025-05-23T12:58:42Z","published":"2025-05-23T12:58:42Z","title":"Locality-Sensitive Hashing for Efficient Hard Negative Sampling in\n  Contrastive Learning","summary":"  Contrastive learning is a representational learning paradigm in which a\nneural network maps data elements to feature vectors. It improves the feature\nspace by forming lots with an anchor and examples that are either positive or\nnegative based on class similarity. Hard negative examples, which are close to\nthe anchor in the feature space but from a different class, improve learning\nperformance. Finding such examples of high quality efficiently in large,\nhigh-dimensional datasets is computationally challenging. In this paper, we\npropose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes\nreal-valued feature vectors into binary representations for approximate nearest\nneighbor search. We investigate its theoretical properties and evaluate it on\nseveral datasets from textual and visual domain. Our approach achieves\ncomparable or better performance while requiring significantly less computation\nthan existing hard negative mining strategies.\n","authors":["Fabian Deuser","Philipp Hausenblas","Hannah Schieber","Daniel Roth","Martin Werner","Norbert Oswald"],"pdf_url":"https://arxiv.org/pdf/2505.17844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17835v1","updated":"2025-05-23T12:49:09Z","published":"2025-05-23T12:49:09Z","title":"VLM Models and Automated Grading of Atopic Dermatitis","summary":"  The task of grading atopic dermatitis (or AD, a form of eczema) from patient\nimages is difficult even for trained dermatologists. Research on automating\nthis task has progressed in recent years with the development of deep learning\nsolutions; however, the rapid evolution of multimodal models and more\nspecifically vision-language models (VLMs) opens the door to new possibilities\nin terms of explainable assessment of medical images, including dermatology.\nThis report describes experiments carried out to evaluate the ability of seven\nVLMs to assess the severity of AD on a set of test images.\n","authors":["Marc Lalonde","Hamed Ghodrati"],"pdf_url":"https://arxiv.org/pdf/2505.17835v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.17821v1","updated":"2025-05-23T12:38:27Z","published":"2025-05-23T12:38:27Z","title":"ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral\n  Object Re-Identification","summary":"  Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods.\n","authors":["Shihao Li","Chenglong Li","Aihua Zheng","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2505.17821v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2505.17812v1","updated":"2025-05-23T12:29:00Z","published":"2025-05-23T12:29:00Z","title":"Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate\n  Object Hallucinations","summary":"  Large Vision-Language Models (LVLMs) have achieved remarkable success but\ncontinue to struggle with object hallucination (OH), generating outputs\ninconsistent with visual inputs. While previous work has proposed methods to\nreduce OH, the visual decision-making mechanisms that lead to hallucinations\nremain poorly understood. In this paper, we propose VaLSe, a Vision-aware\nLatent Steering framework that adopts an interpretation-then-mitigation\nstrategy to address OH in LVLMs. By tackling dual challenges of modeling\ncomplex vision-language interactions and eliminating spurious activation\nartifacts, VaLSe can generate visual contribution maps that trace how specific\nvisual inputs influence individual output tokens. These maps reveal the model's\nvision-aware focus regions, which are then used to perform latent space\nsteering, realigning internal representations toward semantically relevant\ncontent and reducing hallucinated outputs. Extensive experiments demonstrate\nthat VaLSe is a powerful interpretability tool and an effective method for\nenhancing model robustness against OH across multiple benchmarks. Furthermore,\nour analysis uncovers limitations in existing OH evaluation metrics,\nunderscoring the need for more nuanced, interpretable, and visually grounded OH\nbenchmarks in future work. Code is available at:\nhttps://github.com/Ziwei-Zheng/VaLSe.\n","authors":["Boxu Chen","Ziwei Zheng","Le Yang","Zeyu Geng","Zhengyu Zhao","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2505.17812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17808v1","updated":"2025-05-23T12:25:13Z","published":"2025-05-23T12:25:13Z","title":"An Attention Infused Deep Learning System with Grad-CAM Visualization\n  for Early Screening of Glaucoma","summary":"  This research work reveals the eye opening wisdom of the hybrid labyrinthine\ndeep learning models synergy born out of combining a trailblazing convolutional\nneural network with a disruptive Vision Transformer, both intertwined together\nwith a radical Cross Attention module. Here, two high yielding datasets for\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\nDrishti, are utilized.\n","authors":["Ramanathan Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2505.17808v1.pdf","comment":"6 pages in general IEEE format, 8 figures, 4 tables, pdflatex"},{"id":"http://arxiv.org/abs/2505.17807v1","updated":"2025-05-23T12:24:28Z","published":"2025-05-23T12:24:28Z","title":"Temporal Consistency Constrained Transferable Adversarial Attacks with\n  Background Mixup for Action Recognition","summary":"  Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid.\n","authors":["Ping Li","Jianan Ni","Bo Pang"],"pdf_url":"https://arxiv.org/pdf/2505.17807v1.pdf","comment":"Accepted in IJCAI'25"},{"id":"http://arxiv.org/abs/2505.17799v1","updated":"2025-05-23T12:18:34Z","published":"2025-05-23T12:18:34Z","title":"A Coreset Selection of Coreset Selection Literature: Introduction and\n  Recent Advances","summary":"  Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Stanislav Frolov","Federico Raue","Joachim Folz","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2505.17799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17796v1","updated":"2025-05-23T12:15:23Z","published":"2025-05-23T12:15:23Z","title":"DetailFusion: A Dual-branch Framework with Detail Enhancement for\n  Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images from a gallery\nbased on a reference image and modification text as a combined query. Recent\napproaches focus on balancing global information from two modalities and encode\nthe query into a unified feature for retrieval. However, due to insufficient\nattention to fine-grained details, these coarse fusion methods often struggle\nwith handling subtle visual alterations or intricate textual instructions. In\nthis work, we propose DetailFusion, a novel dual-branch framework that\neffectively coordinates information across global and detailed granularities,\nthereby enabling detail-enhanced CIR. Our approach leverages atomic detail\nvariation priors derived from an image editing dataset, supplemented by a\ndetail-oriented optimization strategy to develop a Detail-oriented Inference\nBranch. Furthermore, we design an Adaptive Feature Compositor that dynamically\nfuses global and detailed features based on fine-grained information of each\nunique multimodal query. Extensive experiments and ablation analyses not only\ndemonstrate that our method achieves state-of-the-art performance on both CIRR\nand FashionIQ datasets but also validate the effectiveness and cross-domain\nadaptability of detail enhancement for CIR.\n","authors":["Yuxin Yang","Yinan Zhou","Yuxin Chen","Ziqi Zhang","Zongyang Ma","Chunfeng Yuan","Bing Li","Lin Song","Jun Gao","Peng Li","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2505.17796v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.10165v3","updated":"2025-05-23T12:10:33Z","published":"2025-04-14T12:21:16Z","title":"WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs","summary":"  Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive - a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17.81fps for HD and 7.53fps on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds. Alongside, we introduce our WildLive dataset,\nwhich comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV\nvideos collected at the Ol Pejeta Conservancy in Kenya. All frames contain\nground truth bounding boxes, segmentation masks, as well as individual\ntracklets and tracking point trajectories. We compare our system against\ncurrent object tracking approaches including OC-SORT, ByteTrack, and SORT. Our\nmulti-animal tracking experiments with onboard hardware confirm that near\nreal-time high-resolution wildlife tracking is possible on UAVs whilst\nmaintaining high accuracy levels as needed for future navigational and\nmission-specific animal-centric operational autonomy. Our materials are\navailable at: https://dat-nguyenvn.github.io/WildLive/\n","authors":["Nguyen Ngoc Dat","Tom Richardson","Matthew Watson","Kilian Meier","Jenna Kline","Sid Reid","Guy Maalouf","Duncan Hine","Majid Mirmehdi","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2504.10165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11497v2","updated":"2025-05-23T12:08:30Z","published":"2025-05-16T17:59:40Z","title":"QVGen: Pushing the Limit of Quantized Video Generative Models","summary":"  Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench.\n","authors":["Yushi Huang","Ruihao Gong","Jing Liu","Yifu Ding","Chengtao Lv","Haotong Qin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11497v2.pdf","comment":"Our code will be released upon acceptance"},{"id":"http://arxiv.org/abs/2505.12549v2","updated":"2025-05-23T11:59:20Z","published":"2025-05-18T21:33:09Z","title":"VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold","summary":"  We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements.\n","authors":["Dominic Maggio","Hyungtae Lim","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2505.12549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17783v1","updated":"2025-05-23T11:56:06Z","published":"2025-05-23T11:56:06Z","title":"Generative Data Augmentation for Object Point Cloud Segmentation","summary":"  Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.\n","authors":["Dekai Zhu","Stefan Gavranovic","Flavien Boussuge","Benjamin Busam","Slobodan Ilic"],"pdf_url":"https://arxiv.org/pdf/2505.17783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17782v1","updated":"2025-05-23T11:55:41Z","published":"2025-05-23T11:55:41Z","title":"Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest\n  Monitoring","summary":"  Ground deformation is regarded in volcanology as a key precursor signal\npreceding volcanic eruptions. Satellite-based Interferometric Synthetic\nAperture Radar (InSAR) enables consistent, global-scale deformation tracking;\nhowever, deep learning methods remain largely unexplored in this domain, mainly\ndue to the lack of a curated machine learning dataset. In this work, we build\non the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a\nglobal collection of 38 spatiotemporal datacubes offering high resolution,\nmulti-source and multi-temporal information, covering 44 of the world's most\nactive volcanoes over a 7-year period. Each spatiotemporal datacube integrates\nInSAR products, topographic data, as well as atmospheric variables which are\nknown to introduce signal delays that can mimic ground deformation in InSAR\nimagery. Furthermore, we provide expert annotations detailing the type,\nintensity and spatial extent of deformation events, along with rich text\ndescriptions of the observed scenes. Finally, we present a comprehensive\nbenchmark, demonstrating Hephaestus Minicubes' ability to support volcanic\nunrest monitoring as a multi-modal, multi-temporal classification and semantic\nsegmentation task, establishing strong baselines with state-of-the-art\narchitectures. This work aims to advance machine learning research in volcanic\nmonitoring, contributing to the growing integration of data-driven methods\nwithin Earth science applications.\n","authors":["Nikolas Papadopoulos","Nikolaos Ioannis Bountos","Maria Sdraka","Andreas Karavias","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2505.17782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17779v1","updated":"2025-05-23T11:48:48Z","published":"2025-05-23T11:48:48Z","title":"U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound\n  Understanding","summary":"  Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging.\n","authors":["Anjie Le","Henan Liu","Yue Wang","Zhenyu Liu","Rongkun Zhu","Taohan Weng","Jinze Yu","Boyang Wang","Yalun Wu","Kaiwen Yan","Quanlin Sun","Meirui Jiang","Jialun Pei","Siya Liu","Haoyun Zheng","Zhoujun Li","Alison Noble","Jacques Souquet","Xiaoqing Guo","Manxi Lin","Hongcheng Guo"],"pdf_url":"https://arxiv.org/pdf/2505.17779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17778v1","updated":"2025-05-23T11:46:46Z","published":"2025-05-23T11:46:46Z","title":"TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene\n  Text Synthesis","summary":"  Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations.\n","authors":["Yu Xie","Jielei Zhang","Pengyu Chen","Ziyue Wang","Weihang Wang","Longwen Gao","Peiyi Li","Huyang Sun","Qiang Zhang","Qian Qiao","Jiaqing Fan","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2505.17778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17771v1","updated":"2025-05-23T11:42:54Z","published":"2025-05-23T11:42:54Z","title":"TopoPoint: Enhance Topology Reasoning via Endpoint Detection in\n  Autonomous Driving","summary":"  Topology reasoning, which unifies perception and structured reasoning, plays\na vital role in understanding intersections for autonomous driving. However,\nits performance heavily relies on the accuracy of lane detection, particularly\nat connected lane endpoints. Existing methods often suffer from lane endpoints\ndeviation, leading to incorrect topology construction. To address this issue,\nwe propose TopoPoint, a novel framework that explicitly detects lane endpoints\nand jointly reasons over endpoints and lanes for robust topology reasoning.\nDuring training, we independently initialize point and lane query, and proposed\nPoint-Lane Merge Self-Attention to enhance global context sharing through\nincorporating geometric distances between points and lanes as an attention mask\n. We further design Point-Lane Graph Convolutional Network to enable mutual\nfeature aggregation between point and lane query. During inference, we\nintroduce Point-Lane Geometry Matching algorithm that computes distances\nbetween detected points and lanes to refine lane endpoints, effectively\nmitigating endpoint deviation. Extensive experiments on the OpenLane-V2\nbenchmark demonstrate that TopoPoint achieves state-of-the-art performance in\ntopology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate\nendpoint detection, under which our method significantly outperforms existing\napproaches (52.6 v.s. 45.2 on DET$_p$). The code is released at\nhttps://github.com/Franpin/TopoPoint.\n","authors":["Yanping Fu","Xinyuan Liu","Tianyu Li","Yike Ma","Yucheng Zhang","Feng Dai"],"pdf_url":"https://arxiv.org/pdf/2505.17771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04963v2","updated":"2025-05-23T11:41:30Z","published":"2025-05-08T05:44:16Z","title":"ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis","summary":"  Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Yalcin Tur","Gorkhem Durak","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2505.04963v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17768v1","updated":"2025-05-23T11:41:26Z","published":"2025-05-23T11:41:26Z","title":"R-Genie: Reasoning-Guided Generative Image Editing","summary":"  While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.\n","authors":["Dong Zhang","Lingfeng He","Rui Yan","Fei Shen","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2505.17768v1.pdf","comment":"https://dongzhang89.github.io/RGenie.github.io/"},{"id":"http://arxiv.org/abs/2505.17748v1","updated":"2025-05-23T11:15:21Z","published":"2025-05-23T11:15:21Z","title":"Soft-CAM: Making black box models self-explainable for high-stakes\n  decisions","summary":"  Convolutional neural networks (CNNs) are widely used for high-stakes\napplications like medicine, often surpassing human performance. However, most\nexplanation methods rely on post-hoc attribution, approximating the\ndecision-making process of already trained black-box models. These methods are\noften sensitive, unreliable, and fail to reflect true model reasoning, limiting\ntheir trustworthiness in critical applications. In this work, we introduce\nSoftCAM, a straightforward yet effective approach that makes standard CNN\narchitectures inherently interpretable. By removing the global average pooling\nlayer and replacing the fully connected classification layer with a\nconvolution-based class evidence layer, SoftCAM preserves spatial information\nand produces explicit class activation maps that form the basis of the model's\npredictions. Evaluated on three medical datasets, SoftCAM maintains\nclassification performance while significantly improving both the qualitative\nand quantitative explanation compared to existing post-hoc methods. Our results\ndemonstrate that CNNs can be inherently interpretable without compromising\nperformance, advancing the development of self-explainable deep learning for\nhigh-stakes decision-making.\n","authors":["Kerol Djoumessi","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2505.17748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08691v2","updated":"2025-05-23T11:11:53Z","published":"2024-09-13T10:19:10Z","title":"Autoregressive Sequence Modeling for 3D Medical Image Representation","summary":"  Three-dimensional (3D) medical images, such as Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI), are essential for clinical applications.\nHowever, the need for diverse and comprehensive representations is particularly\npronounced when considering the variability across different organs, diagnostic\ntasks, and imaging modalities. How to effectively interpret the intricate\ncontextual information and extract meaningful insights from these images\nremains an open challenge to the community. While current self-supervised\nlearning methods have shown potential, they often consider an image as a whole\nthereby overlooking the extensive, complex relationships among local regions\nfrom one or multiple images. In this work, we introduce a pioneering method for\nlearning 3D medical image representations through an autoregressive\npre-training framework. Our approach sequences various 3D medical images based\non spatial, contrast, and semantic correlations, treating them as\ninterconnected visual tokens within a token sequence. By employing an\nautoregressive sequence modeling task, we predict the next visual token in the\nsequence, which allows our model to deeply understand and integrate the\ncontextual information inherent in 3D medical images. Additionally, we\nimplement a random startup strategy to avoid overestimating token relationships\nand to enhance the robustness of learning. The effectiveness of our approach is\ndemonstrated by the superior performance over others on nine downstream tasks\nin public datasets.\n","authors":["Siwen Wang","Churan Wang","Fei Gao","Lixian Su","Fandong Zhang","Yizhou Wang","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2409.08691v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2505.16809v2","updated":"2025-05-23T10:55:45Z","published":"2025-05-22T15:49:25Z","title":"Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor\n  Segmentation with Missing Modalities","summary":"  Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.\n","authors":["Junze Wang","Lei Fan","Weipeng Jing","Donglin Di","Yang Song","Sidong Liu","Cong Cong"],"pdf_url":"https://arxiv.org/pdf/2505.16809v2.pdf","comment":"MICCAI 2025 Early Accept. The code is available at\n  https://github.com/reeive/ReHyDIL"},{"id":"http://arxiv.org/abs/2505.17732v1","updated":"2025-05-23T10:52:34Z","published":"2025-05-23T10:52:34Z","title":"RQR3D: Reparametrizing the regression targets for BEV-based 3D object\n  detection","summary":"  Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach.\n","authors":["Ozsel Kilinc","Cem Tarhan"],"pdf_url":"https://arxiv.org/pdf/2505.17732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17727v1","updated":"2025-05-23T10:45:43Z","published":"2025-05-23T10:45:43Z","title":"SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the\n  Real World Domain","summary":"  Safety-critical scenarios are rare yet pivotal for evaluating and enhancing\nthe robustness of autonomous driving systems. While existing methods generate\nsafety-critical driving trajectories, simulations, or single-view videos, they\nfall short of meeting the demands of advanced end-to-end autonomous systems\n(E2E AD), which require real-world, multi-view video data. To bridge this gap,\nwe introduce SafeMVDrive, the first framework designed to generate\nhigh-quality, safety-critical, multi-view driving videos grounded in real-world\ndomains. SafeMVDrive strategically integrates a safety-critical trajectory\ngenerator with an advanced multi-view video generator. To tackle the challenges\ninherent in this integration, we first enhance scene understanding ability of\nthe trajectory generator by incorporating visual context -- which is previously\nunavailable to such generator -- and leveraging a GRPO-finetuned\nvision-language model to achieve more realistic and context-aware trajectory\ngeneration. Second, recognizing that existing multi-view video generators\nstruggle to render realistic collision events, we introduce a two-stage,\ncontrollable trajectory generation mechanism that produces collision-evasion\ntrajectories, ensuring both video quality and safety-critical fidelity.\nFinally, we employ a diffusion-based multi-view video generator to synthesize\nhigh-quality safety-critical driving videos from the generated trajectories.\nExperiments conducted on an E2E AD planner demonstrate a significant increase\nin collision rate when tested with our generated data, validating the\neffectiveness of SafeMVDrive in stress-testing planning modules. Our code,\nexamples, and datasets are publicly available at:\nhttps://zhoujiawei3.github.io/SafeMVDrive/.\n","authors":["Jiawei Zhou","Linye Lyu","Zhuotao Tian","Cheng Zhuo","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2505.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17726v1","updated":"2025-05-23T10:43:45Z","published":"2025-05-23T10:43:45Z","title":"Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM","summary":"  Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages.\n","authors":["Donghwan Chi","Hyomin Kim","Yoonjin Oh","Yongjin Kim","Donghoon Lee","Daejin Jo","Jongmin Kim","Junyeob Baek","Sungjin Ahn","Sungwoong Kim"],"pdf_url":"https://arxiv.org/pdf/2505.17726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17721v1","updated":"2025-05-23T10:38:05Z","published":"2025-05-23T10:38:05Z","title":"SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D\n  Generation","summary":"  Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.\n","authors":["Dekai Zhu","Yan Di","Stefan Gavranovic","Slobodan Ilic"],"pdf_url":"https://arxiv.org/pdf/2505.17721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06232v2","updated":"2025-05-23T10:16:46Z","published":"2024-11-09T16:49:59Z","title":"RCR: Robust Crowd Reconstruction with Upright Space from a Single\n  Large-scene Image","summary":"  This paper focuses on spatially consistent hundreds of human pose and shape\nreconstruction from a single large-scene image with various human scales under\narbitrary camera FoVs (Fields of View). Due to the small and highly varying 2D\nhuman scales, depth ambiguity, and perspective distortion, no existing methods\ncan achieve globally consistent reconstruction with correct reprojection. To\naddress these challenges, we first propose a new concept, Human-scene Virtual\nInteraction Point (HVIP), to convert the complex 3D human localization into\n2D-pixel localization. We then extend it to RCR (Robust Crowd Reconstruction),\nwhich achieves globally consistent reconstruction and stable generalization on\ndifferent camera FoVs without test-time optimization. To perceive humans in\nvarying pixel sizes, we propose an Iterative Ground-aware Cropping to\nautomatically crop the image and then merge the results. To eliminate the\ninfluence of the camera and cropping process during the reconstruction, we\nintroduce a canonical Upright 3D Space and the corresponding Upright 2D Space.\nTo link the canonical space and the camera space, we propose the Upright\nNormalization, which transforms the local crop input into the Upright 2D Space,\nand transforms the output from the Upright 3D Space into the unified camera\nspace. Besides, we contribute two benchmark datasets, LargeCrowd and SynCrowd,\nfor evaluating crowd reconstruction in large scenes. Experimental results\ndemonstrate the effectiveness of the proposed method. The source code and data\nwill be publicly available for research purposes.\n","authors":["Jing Huang","Hao Wen","Tianyi Zhou","Haozhe Lin","Yu-kun Lai","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2411.06232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17702v1","updated":"2025-05-23T10:11:19Z","published":"2025-05-23T10:11:19Z","title":"Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using\n  Local Inference via DeepSeek","summary":"  The advent of Computer-Aided Design (CAD) generative modeling will\nsignificantly transform the design of industrial products. The recent research\nendeavor has extended into the realm of Large Language Models (LLMs). In\ncontrast to fine-tuning methods, training-free approaches typically utilize the\nadvanced closed-source LLMs, thereby offering enhanced flexibility and\nefficiency in the development of AI agents for generating CAD parametric\nmodels. However, the substantial cost and limitations of local deployment of\nthe top-tier closed-source LLMs pose challenges in practical applications. The\nSeek-CAD is the pioneer exploration of locally deployed open-source inference\nLLM DeepSeek-R1 for CAD parametric model generation with a training-free\nmethodology. This study is the first investigation to incorporate both visual\nand Chain-of-Thought (CoT) feedback within the self-refinement mechanism for\ngenerating CAD models. Specifically, the initial generated parametric CAD model\nis rendered into a sequence of step-wise perspective images, which are\nsubsequently processed by a Vision Language Model (VLM) alongside the\ncorresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.\nThen, the feedback is utilized by DeepSeek-R1 to refine the initial generated\nmodel for the next round of generation. Moreover, we present an innovative 3D\nCAD model dataset structured around the SSR (Sketch, Sketch-based feature, and\nRefinements) triple design paradigm. This dataset encompasses a wide range of\nCAD commands, thereby aligning effectively with industrial application\nrequirements and proving suitable for the generation of LLMs. Extensive\nexperiments validate the effectiveness of Seek-CAD under various metrics.\n","authors":["Xueyang Li","Jiahao Li","Yu Song","Yunzhong Lou","Xiangdong Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16083v2","updated":"2025-05-23T10:09:51Z","published":"2025-04-22T17:59:51Z","title":"MMInference: Accelerating Pre-filling for Long-Context VLMs via\n  Modality-Aware Permutation Sparse Attention","summary":"  The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.\n","authors":["Yucheng Li","Huiqiang Jiang","Chengruidong Zhang","Qianhui Wu","Xufang Luo","Surin Ahn","Amir H. Abdi","Dongsheng Li","Jianfeng Gao","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2504.16083v2.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2503.17668v2","updated":"2025-05-23T10:09:21Z","published":"2025-03-22T06:37:54Z","title":"Camera Movement Estimation and Path Correction using the Combination of\n  Modified A-SIFT and Stereo System for 3D Modelling","summary":"  Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction.\n","authors":["Usha Kumari","Shuvendu Rana"],"pdf_url":"https://arxiv.org/pdf/2503.17668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17695v1","updated":"2025-05-23T10:05:16Z","published":"2025-05-23T10:05:16Z","title":"SynRES: Towards Referring Expression Segmentation in the Wild via\n  Synthetic Data","summary":"  Despite the advances in Referring Expression Segmentation (RES) benchmarks,\ntheir evaluation protocols remain constrained, primarily focusing on either\nsingle targets with short queries (containing minimal attributes) or multiple\ntargets from distinctly different queries on a single domain. This limitation\nsignificantly hinders the assessment of more complex reasoning capabilities in\nRES models. We introduce WildRES, a novel benchmark that incorporates long\nqueries with diverse attributes and non-distinctive queries for multiple\ntargets. This benchmark spans diverse application domains, including autonomous\ndriving environments and robotic manipulation scenarios, thus enabling more\nrigorous evaluation of complex reasoning capabilities in real-world settings.\nOur analysis reveals that current RES models demonstrate substantial\nperformance deterioration when evaluated on WildRES. To address this challenge,\nwe introduce SynRES, an automated pipeline generating densely paired\ncompositional synthetic training data through three innovations: (1) a dense\ncaption-driven synthesis for attribute-rich image-mask-expression triplets, (2)\nreliable semantic alignment mechanisms rectifying caption-pseudo mask\ninconsistencies via Image-Text Aligned Grouping, and (3) domain-aware\naugmentations incorporating mosaic composition and superclass replacement to\nemphasize generalization ability and distinguishing attributes over object\ncategories. Experimental results demonstrate that models trained with SynRES\nachieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and\n3.8% on WildRES-DS. Code and datasets are available at\nhttps://github.com/UTLLab/SynRES.\n","authors":["Dong-Hee Kim","Hyunjee Song","Donghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2505.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17692v1","updated":"2025-05-23T10:01:11Z","published":"2025-05-23T10:01:11Z","title":"ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for\n  Zero-Shot Anomaly Detection","summary":"  Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.\n","authors":["Ziteng Yang","Jingzehua Xu","Yanshu Li","Zepeng Li","Yeqiang Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2505.17692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17690v1","updated":"2025-05-23T09:59:26Z","published":"2025-05-23T09:59:26Z","title":"Semi-Supervised Medical Image Segmentation via Dual Networks","summary":"  Traditional supervised medical image segmentation models require large\namounts of labeled data for training; however, obtaining such large-scale\nlabeled datasets in the real world is extremely challenging. Recent\nsemi-supervised segmentation models also suffer from noisy pseudo-label issue\nand limited supervision in feature space. To solve these challenges, we propose\nan innovative semi-supervised 3D medical image segmentation method to reduce\nthe dependency on large, expert-labeled datasets. Furthermore, we introduce a\ndual-network architecture to address the limitations of existing methods in\nusing contextual information and generating reliable pseudo-labels. In\naddition, a self-supervised contrastive learning strategy is used to enhance\nthe representation of the network and reduce prediction uncertainty by\ndistinguishing between reliable and unreliable predictions. Experiments on\nclinical magnetic resonance imaging demonstrate that our approach outperforms\nstate-of-the-art techniques. Our code is available at\nhttps://github.com/AIPMLab/Semi-supervised-Segmentation.\n","authors":["Yunyao Lu","Yihang Wu","Reem Kateb","Ahmad Chaddad"],"pdf_url":"https://arxiv.org/pdf/2505.17690v1.pdf","comment":"Accepted in ISBI2025"},{"id":"http://arxiv.org/abs/2405.14522v2","updated":"2025-05-23T09:57:41Z","published":"2024-05-23T13:03:26Z","title":"Explaining Black-box Model Predictions via Two-level Nested Feature\n  Attributions with Consistency Property","summary":"  Techniques that explain the predictions of black-box machine learning models\nare crucial to make the models transparent, thereby increasing trust in AI\nsystems. The input features to the models often have a nested structure that\nconsists of high- and low-level features, and each high-level feature is\ndecomposed into multiple low-level features. For such inputs, both high-level\nfeature attributions (HiFAs) and low-level feature attributions (LoFAs) are\nimportant for better understanding the model's decision. In this paper, we\npropose a model-agnostic local explanation method that effectively exploits the\nnested structure of the input to estimate the two-level feature attributions\nsimultaneously. A key idea of the proposed method is to introduce the\nconsistency property that should exist between the HiFAs and LoFAs, thereby\nbridging the separate optimization problems for estimating them. Thanks to this\nconsistency property, the proposed method can produce HiFAs and LoFAs that are\nboth faithful to the black-box models and consistent with each other, using a\nsmaller number of queries to the models. In experiments on image classification\nin multiple instance learning and text classification using language models, we\ndemonstrate that the HiFAs and LoFAs estimated by the proposed method are\naccurate, faithful to the behaviors of the black-box models, and provide\nconsistent explanations.\n","authors":["Yuya Yoshikawa","Masanari Kimura","Ryotaro Shimizu","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2405.14522v2.pdf","comment":"This manuscript is an extended version of our paper accepted at\n  IJCAI2025, with detailed proofs and additional experimental results"},{"id":"http://arxiv.org/abs/2505.17685v1","updated":"2025-05-23T09:55:32Z","published":"2025-05-23T09:55:32Z","title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for\n  Autonomous Driving","summary":"  Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning.\n","authors":["Shuang Zeng","Xinyuan Chang","Mengwei Xie","Xinran Liu","Yifan Bai","Zheng Pan","Mu Xu","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17684v1","updated":"2025-05-23T09:54:58Z","published":"2025-05-23T09:54:58Z","title":"5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for\n  Dynamic 5G Indoor Localization","summary":"  Indoor positioning based on 5G data has achieved high accuracy through the\nadoption of recent machine learning (ML) techniques. However, the performance\nof learning-based methods degrades significantly when environmental conditions\nchange, thereby hindering their applicability to new scenarios. Acquiring new\ntraining data for each environmental change and fine-tuning ML models is both\ntime-consuming and resource-intensive. This paper introduces a domain\nincremental learning (DIL) approach for dynamic 5G indoor localization, called\n5G-DIL, enabling rapid adaptation to environmental changes. We present a novel\nsimilarity-aware sampling technique based on the Chebyshev distance, designed\nto efficiently select specific exemplars from the previous environment while\ntraining only on the modified regions of the new environment. This avoids the\nneed to train on the entire region, significantly reducing the time and\nresources required for adaptation without compromising localization accuracy.\nThis approach requires as few as 50 exemplars from adaptation domains,\nsignificantly reducing training time while maintaining high positioning\naccuracy in previous environments. Comparative evaluations against\nstate-of-the-art DIL techniques on a challenging real-world indoor dataset\ndemonstrate the effectiveness of the proposed sample selection method. Our\napproach is adaptable to real-world non-line-of-sight propagation scenarios and\nachieves an MAE positioning error of 0.261 meters, even under dynamic\nenvironmental conditions. Code:\nhttps://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil\n","authors":["Nisha Lakshmana Raichur","Lucas Heublein","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2505.17684v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.17683v1","updated":"2025-05-23T09:53:57Z","published":"2025-05-23T09:53:57Z","title":"Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation\n  in IVH Detection","summary":"  Intraventricular hemorrhage (IVH) is a severe neurological complication among\npremature infants, necessitating early and accurate detection from brain\nultrasound (US) images to improve clinical outcomes. While recent deep learning\nmethods offer promise for computer-aided diagnosis, challenges remain in\ncapturing both local spatial details and global contextual dependencies\ncritical for segmenting brain anatomies. In this work, we propose an enhanced\nResidual U-Net architecture incorporating two complementary attention\nmechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse\nAttention Layer (SAL). The CBAM improves the model's ability to refine spatial\nand channel-wise features, while the SAL introduces a dual-branch design,\nsparse attention filters out low-confidence query-key pairs to suppress noise,\nand dense attention ensures comprehensive information propagation. Extensive\nexperiments on the Brain US dataset demonstrate that our method achieves\nstate-of-the-art segmentation performance, with a Dice score of 89.04% and IoU\nof 81.84% for ventricle region segmentation. These results highlight the\neffectiveness of integrating spatial refinement and attention sparsity for\nrobust brain anatomy detection. Code is available at:\nhttps://github.com/DanYuan001/BrainImgSegment.\n","authors":["Dan Yuan","Yi Feng","Ziyun Tang"],"pdf_url":"https://arxiv.org/pdf/2505.17683v1.pdf","comment":"10 pages,6 figures and 3 tables"},{"id":"http://arxiv.org/abs/2505.17677v1","updated":"2025-05-23T09:44:02Z","published":"2025-05-23T09:44:02Z","title":"Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in\n  Ophthalmic Surgery","summary":"  Accurate 3D reconstruction of hands and instruments is critical for\nvision-based analysis of ophthalmic microsurgery, yet progress has been\nhampered by the lack of realistic, large-scale datasets and reliable annotation\ntools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic\n3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from\n40 surgeons and totaling 7.1 million frames, with fine-grained annotations of\n12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full\n6-DoF instrument poses. To scalably produce high-fidelity labels, we design a\nmulti-stage automatic annotation pipeline that integrates multi-view data\nobservation, data-driven motion prior with cross-view geometric consistency and\nbiomechanical constraints, along with a combination of collision-aware\ninteraction constraints for instrument interactions. Building upon OphNet-3D,\nwe establish two challenging benchmarks-bimanual hand pose estimation and\nhand-instrument interaction reconstruction-and propose two dedicated\narchitectures: H-Net for dual-hand mesh recovery and OH-Net for joint\nreconstruction of two-hand-two-instrument interactions. These models leverage a\nnovel spatial reasoning module with weak-perspective camera modeling and\ncollision-aware center-based representation. Both architectures outperform\nexisting methods by substantial margins, achieving improvements of over 2mm in\nMean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand\nand instrument reconstruction, respectively.\n","authors":["Ming Hu","Zhendi Yu","Feilong Tang","Kaiwen Chen","Yulong Li","Imran Razzak","Junjun He","Tolga Birdal","Kaijing Zhou","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2505.17677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13716v3","updated":"2025-05-23T09:43:53Z","published":"2023-11-22T22:20:10Z","title":"DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation\n  Networks for Remote Sensing Imagery","summary":"  Semi-supervised learning (SSL) aims to help reduce the cost of the manual\nlabelling process by leveraging a substantial pool of unlabelled data alongside\na limited set of labelled data during the training phase. Since pixel-level\nmanual labelling in large-scale remote sensing imagery is expensive and\ntime-consuming, semi-supervised learning has become a widely used solution to\ndeal with this. However, the majority of existing SSL frameworks, especially\nvarious teacher-student frameworks, are too bulky to run efficiently on a GPU\nwith limited memory. There is still a lack of lightweight SSL frameworks and\nefficient perturbation methods to promote the diversity of training samples and\nenhance the precision of pseudo labels during training. In order to fill this\ngap, we proposed a simple, lightweight, and efficient SSL architecture named\n\\textit{DiverseHead}, which promotes the utilisation of multiple decision heads\ninstead of multiple whole networks. Another limitation of most existing SSL\nframeworks is the insufficient diversity of pseudo labels, as they rely on the\nsame network architecture and fail to explore different structures for\ngenerating pseudo labels. To solve this issue, we propose \\textit{DiverseModel}\nto explore and analyse different networks in parallel for SSL to increase the\ndiversity of pseudo labels. The two proposed methods, namely\n\\textit{DiverseHead} and \\textit{DiverseModel}, both achieve competitive\nsemantic segmentation performance in four widely used remote sensing imagery\ndatasets compared to state-of-the-art semi-supervised learning methods.\nMeanwhile, the proposed lightweight DiverseHead architecture can be easily\napplied to various state-of-the-art SSL methods while further improving their\nperformance. The code is available at\nhttps://github.com/WANLIMA-CARDIFF/DiverseNet.\n","authors":["Wanli Ma","Oktay Karakus","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2311.13716v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17674v1","updated":"2025-05-23T09:41:10Z","published":"2025-05-23T09:41:10Z","title":"SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world\n  Understanding","summary":"  Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D\nspatio-temporal features. However, existing SNNs still exhibit a significant\nperformance gap compared to Artificial Neural Networks (ANNs) due to inadequate\npre-training strategies. These limitations manifest as restricted\ngeneralization ability, task specificity, and a lack of multimodal\nunderstanding, particularly in challenging tasks such as multimodal question\nanswering and zero-shot 3D classification. To overcome these challenges, we\npropose a Spike-based Vision-Language (SVL) pretraining framework that empowers\nSNNs with open-world 3D understanding while maintaining spike-driven\nefficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment\n(MTA) for label-free triplet-based contrastive learning across 3D, image, and\ntext modalities, and (ii) Re-parameterizable Vision-Language Integration\n(Rep-VLI) to enable lightweight inference without relying on large text\nencoders. Extensive experiments show that SVL achieves a top-1 accuracy of\n85.4% in zero-shot 3D classification, surpassing advanced ANN models, and\nconsistently outperforms prior SNNs on downstream tasks, including 3D\nclassification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),\nand 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables\nSNNs to perform open-world 3D question answering, sometimes outperforming ANNs.\nTo the best of our knowledge, SVL represents the first scalable, generalizable,\nand hardware-friendly paradigm for 3D open-world understanding, effectively\nbridging the gap between SNNs and ANNs in complex open-world understanding\ntasks. Code is available https://github.com/bollossom/SVL.\n","authors":["Xuerui Qiu","Peixi Wu","Yaozhi Wen","Shaowei Gu","Yuqi Pan","Xinhao Luo","Bo XU","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2505.17674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17666v1","updated":"2025-05-23T09:31:02Z","published":"2025-05-23T09:31:02Z","title":"Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape\n  Classification","summary":"  Deep learning-based multi-view coarse-grained 3D shape classification has\nachieved remarkable success over the past decade, leveraging the powerful\nfeature learning capabilities of CNN-based and ViT-based backbones. However, as\na challenging research area critical for detailed shape understanding,\nfine-grained 3D classification remains understudied due to the limited\ndiscriminative information captured during multi-view feature aggregation,\nparticularly for subtle inter-class variations, class imbalance, and inherent\ninterpretability limitations of parametric model. To address these problems, we\npropose the first prototype-based framework named Proto-FG3D for fine-grained\n3D shape classification, achieving a paradigm shift from parametric softmax to\nnon-parametric prototype learning. Firstly, Proto-FG3D establishes joint\nmulti-view and multi-category representation learning via Prototype\nAssociation. Secondly, prototypes are refined via Online Clustering, improving\nboth the robustness of multi-view feature allocation and inter-subclass\nbalance. Finally, prototype-guided supervised learning is established to\nenhance fine-grained discrimination via prototype-view correlation analysis and\nenables ad-hoc interpretability through transparent case-based reasoning.\nExperiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art\nmethods in accuracy, transparent predictions, and ad-hoc interpretability with\nvisualizations, challenging conventional fine-grained 3D recognition\napproaches.\n","authors":["Shuxian Ma","Zihao Dong","Runmin Cong","Sam Kwong","Xiuli Shao"],"pdf_url":"https://arxiv.org/pdf/2505.17666v1.pdf","comment":"11 pages, 2 figures, 5 tablets; Submitted to BMVC2025"},{"id":"http://arxiv.org/abs/2411.16598v3","updated":"2025-05-23T09:31:00Z","published":"2024-11-25T17:30:32Z","title":"DiffBreak: Is Diffusion-Based Purification Robust?","summary":"  Diffusion-based purification (DBP) has become a cornerstone defense against\nadversarial examples (AEs), regarded as robust due to its use of diffusion\nmodels (DMs) that project AEs onto the natural data manifold. We refute this\ncore claim, theoretically proving that gradient-based attacks effectively\ntarget the DM rather than the classifier, causing DBP's outputs to align with\nadversarial distributions. This prompts a reassessment of DBP's robustness,\nattributing it to two critical flaws: incorrect gradients and inappropriate\nevaluation protocols that test only a single random purification of the AE. We\nshow that with proper accounting for stochasticity and resubmission risk, DBP\ncollapses. To support this, we introduce DiffBreak, the first reliable toolkit\nfor differentiation through DBP, eliminating gradient flaws that previously\nfurther inflated robustness estimates. We also analyze the current defense\nscheme used for DBP where classification relies on a single purification,\npinpointing its inherent invalidity. We provide a statistically grounded\nmajority-vote (MV) alternative that aggregates predictions across multiple\npurified copies, showing partial but meaningful robustness gain. We then\npropose a novel adaptation of an optimization method against deepfake\nwatermarking, crafting systemic perturbations that defeat DBP even under MV,\nchallenging DBP's viability.\n","authors":["Andre Kassis","Urs Hengartner","Yaoliang Yu"],"pdf_url":"https://arxiv.org/pdf/2411.16598v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17665v1","updated":"2025-05-23T09:30:45Z","published":"2025-05-23T09:30:45Z","title":"EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote\n  Sensing Images with Attention Proxy","summary":"  High-resolution remote sensing (HRRS) image segmentation is challenging due\nto complex spatial layouts and diverse object appearances. While CNNs excel at\ncapturing local features, they struggle with long-range dependencies, whereas\nTransformers can model global context but often neglect local details and are\ncomputationally expensive.We propose a novel approach, Region-Aware Proxy\nNetwork (RAPNet), which consists of two components: Contextual Region Attention\n(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely\non grid-based layouts, RAPNet operates at the region level for more flexible\nsegmentation. The CRA module uses a Transformer to capture region-level\ncontextual dependencies, generating a Semantic Region Mask (SRM). The GCR\nmodule learns a global class attention map to refine multi-class information,\ncombining the SRM and attention map for accurate segmentation.Experiments on\nthree public datasets show that RAPNet outperforms state-of-the-art methods,\nachieving superior multi-class segmentation accuracy.\n","authors":["Yichun Yu","Yuqing Lan","Zhihuan Xing","Xiaoyi Yang","Tingyue Tang","Dan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.17665v1.pdf","comment":"Proceedings of the 20th International Conference on Intelligent\n  Computing (ICIC 2024): Poster Volume I. Tianjin, China, 2024: 538-562"},{"id":"http://arxiv.org/abs/2502.19260v4","updated":"2025-05-23T09:30:11Z","published":"2025-02-26T16:06:35Z","title":"EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving","summary":"  This paper introduces the Emirates Multi-Task (EMT) dataset, designed to\nsupport multi-task benchmarking within a unified framework. It comprises over\n30,000 frames from a dash-camera perspective and 570,000 annotated bounding\nboxes, covering approximately 150 kilometers of driving routes that reflect the\ndistinctive road topology, congestion patterns, and driving behavior of Gulf\nregion traffic. The dataset supports three primary tasks: tracking, trajectory\nforecasting, and intention prediction. Each benchmark is accompanied by\ncorresponding evaluations: (1) multi-agent tracking experiments addressing\nmulti-class scenarios and occlusion handling; (2) trajectory forecasting\nevaluation using deep sequential and interaction-aware models; and (3)\nintention prediction experiments based on observed trajectories. The dataset is\npublicly available at https://avlab.io/emt-dataset, with pre-processing scripts\nand evaluation models at https://github.com/AV-Lab/emt-dataset.\n","authors":["Nadya Abdel Madjid","Murad Mebrahtu","Abdulrahman Ahmad","Abdelmoamen Nasser","Bilal Hassan","Naoufel Werghi","Jorge Dias","Majid Khonji"],"pdf_url":"https://arxiv.org/pdf/2502.19260v4.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.17659v1","updated":"2025-05-23T09:22:19Z","published":"2025-05-23T09:22:19Z","title":"Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling","summary":"  Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance.\n","authors":["Xiaolong Tang","Meina Kan","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2505.17659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17649v1","updated":"2025-05-23T09:12:34Z","published":"2025-05-23T09:12:34Z","title":"Instruct2See: Learning to Remove Any Obstructions Across Distributions","summary":"  Images are often obstructed by various obstacles due to capture limitations,\nhindering the observation of objects of interest. Most existing methods address\nocclusions from specific elements like fences or raindrops, but are constrained\nby the wide range of real-world obstructions, making comprehensive data\ncollection impractical. To overcome these challenges, we propose Instruct2See,\na novel zero-shot framework capable of handling both seen and unseen obstacles.\nThe core idea of our approach is to unify obstruction removal by treating it as\na soft-hard mask restoration problem, where any obstruction can be represented\nusing multi-modal prompts, such as visual semantics and textual instructions,\nprocessed through a cross-attention unit to enhance contextual understanding\nand improve mode control. Additionally, a tunable mask adapter allows for\ndynamic soft masking, enabling real-time adjustment of inaccurate masks.\nExtensive experiments on both in-distribution and out-of-distribution obstacles\nshow that Instruct2See consistently achieves strong performance and\ngeneralization in obstruction removal, regardless of whether the obstacles were\npresent during the training phase. Code and dataset are available at\nhttps://jhscut.github.io/Instruct2See.\n","authors":["Junhang Li","Yu Guo","Chuhua Xian","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2505.17649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.13560v2","updated":"2025-05-23T09:07:30Z","published":"2022-07-27T14:52:32Z","title":"D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive\n  Sensing","summary":"  By mapping iterative optimization algorithms into neural networks (NNs), deep\nunfolding networks (DUNs) exhibit well-defined and interpretable structures and\nachieve remarkable success in the field of compressive sensing (CS). However,\nmost existing DUNs solely rely on the image-domain unfolding, which restricts\nthe information transmission capacity and reconstruction flexibility, leading\nto their loss of image details and unsatisfactory performance. To overcome\nthese limitations, this paper develops a dual-domain optimization framework\nthat combines the priors of (1) image- and (2) convolutional-coding-domains and\noffers generality to CS and other inverse imaging tasks. By converting this\noptimization framework into deep NN structures, we present a Dual-Domain Deep\nConvolutional Coding Network (D3C2-Net), which enjoys the ability to\nefficiently transmit high-capacity self-adaptive convolutional features across\nall its unfolded stages. Our theoretical analyses and experiments on simulated\nand real captured data, covering 2D and 3D natural, medical, and scientific\nsignals, demonstrate the effectiveness, practicality, superior performance, and\ngeneralization ability of our method over other competing approaches and its\nsignificant potential in achieving a balance among accuracy, complexity, and\ninterpretability. Code is available at https://github.com/lwq20020127/D3C2-Net.\n","authors":["Weiqi Li","Bin Chen","Shuai Liu","Shijie Zhao","Bowen Du","Yongbing Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.13560v2.pdf","comment":"accepted by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2505.17645v1","updated":"2025-05-23T09:06:09Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human\n  Sensing and Reasoning","summary":"  Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.\n","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17645v1.pdf","comment":"18 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.17644v1","updated":"2025-05-23T09:05:10Z","published":"2025-05-23T09:05:10Z","title":"Towards Prospective Medical Image Reconstruction via Knowledge-Informed\n  Dynamic Optimal Transport","summary":"  Medical image reconstruction from measurement data is a vital but challenging\ninverse problem. Deep learning approaches have achieved promising results, but\noften requires paired measurement and high-quality images, which is typically\nsimulated through a forward model, i.e., retrospective reconstruction. However,\ntraining on simulated pairs commonly leads to performance degradation on real\nprospective data due to the retrospective-to-prospective gap caused by\nincomplete imaging knowledge in simulation. To address this challenge, this\npaper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),\na novel dynamic optimal transport framework with optimality in the sense of\npreserving consistency with imaging physics in transport, that conceptualizes\nreconstruction as finding a dynamic transport path. KIDOT learns from unpaired\ndata by modeling reconstruction as a continuous evolution path from\nmeasurements to images, guided by an imaging knowledge-informed cost function\nand transport equation. This dynamic and knowledge-aware approach enhances\nrobustness and better leverages unpaired data while respecting acquisition\nphysics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic\noptimal transport, ensuring its mathematical rationale and solution existence.\nExtensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior\nperformance.\n","authors":["Taoran Zheng","Xing Li","Yan Yang","Xiang Gu","Zongben Xu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2505.17644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14151v2","updated":"2025-05-23T08:51:27Z","published":"2025-05-20T10:01:37Z","title":"ReactDiff: Latent Diffusion for Facial Reaction Generation","summary":"  Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.\n","authors":["Jiaming Li","Sheng Wang","Xin Wang","Yitao Zhu","Honglin Xiong","Zixu Zhuang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14151v2.pdf","comment":"Neural Networks"},{"id":"http://arxiv.org/abs/2310.03602v4","updated":"2025-05-23T08:46:56Z","published":"2023-10-05T15:29:52Z","title":"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout\n  Constraints","summary":"  Text-driven 3D indoor scene generation is useful for gaming, the film\nindustry, and AR/VR applications. However, existing methods cannot faithfully\ncapture the room layout, nor do they allow flexible editing of individual\nobjects in the room. To address these problems, we present Ctrl-Room, which can\ngenerate convincing 3D rooms with designer-style layouts and high-fidelity\ntextures from just a text prompt. Moreover, Ctrl-Room enables versatile\ninteractive editing operations such as resizing or moving individual furniture\nitems. Our key insight is to separate the modeling of layouts and appearance.\nOur proposed method consists of two stages: a Layout Generation Stage and an\nAppearance Generation Stage. The Layout Generation Stage trains a\ntext-conditional diffusion model to learn the layout distribution with our\nholistic scene code parameterization. Next, the Appearance Generation Stage\nemploys a fine-tuned ControlNet to produce a vivid panoramic image of the room\nguided by the 3D scene layout and text prompt. We thus achieve a high-quality\n3D room generation with convincing layouts and lively textures. Benefiting from\nthe scene code parameterization, we can easily edit the generated room model\nthrough our mask-guided editing module, without expensive edit-specific\ntraining. Extensive experiments on the Structured3D dataset demonstrate that\nour method outperforms existing methods in producing more reasonable,\nview-consistent, and editable 3D rooms from natural language prompts.\n","authors":["Chuan Fang","Yuan Dong","Kunming Luo","Xiaotao Hu","Rakesh Shrestha","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2310.03602v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22679v2","updated":"2025-05-23T08:41:50Z","published":"2025-03-28T17:59:54Z","title":"Q-Insight: Understanding Image Quality via Visual Reinforcement Learning","summary":"  Image quality assessment (IQA) focuses on the perceptual visual quality of\nimages, playing a crucial role in downstream tasks such as image\nreconstruction, compression, and generation. The rapid advancement of\nmulti-modal large language models (MLLMs) has significantly broadened the scope\nof IQA, moving toward comprehensive image quality understanding that\nincorporates content analysis, degradation perception, and comparison reasoning\nbeyond mere numerical scoring. Previous MLLM-based methods typically either\ngenerate numerical scores lacking interpretability or heavily rely on\nsupervised fine-tuning (SFT) using large-scale annotated datasets to provide\ndescriptive assessments, limiting their flexibility and applicability. In this\npaper, we propose Q-Insight, a reinforcement learning-based model built upon\ngroup relative policy optimization (GRPO), which demonstrates strong visual\nreasoning capability for image quality understanding while requiring only a\nlimited amount of rating scores and degradation labels. By jointly optimizing\nscore regression and degradation perception tasks with carefully designed\nreward functions, our approach effectively exploits their mutual benefits for\nenhanced performance. Extensive experiments demonstrate that Q-Insight\nsubstantially outperforms existing state-of-the-art methods in both score\nregression and degradation perception tasks, while exhibiting impressive\nzero-shot generalization to comparison reasoning tasks. Code will be available\nat https://github.com/lwq20020127/Q-Insight.\n","authors":["Weiqi Li","Xuanyu Zhang","Shijie Zhao","Yabin Zhang","Junlin Li","Li Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.22679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02534v2","updated":"2025-05-23T08:38:45Z","published":"2025-01-05T13:28:37Z","title":"Boosting Edge Detection with Pixel-wise Feature Selection: The\n  Extractor-Selector Paradigm","summary":"  Deep learning has significantly advanced image edge detection (ED), primarily\nthrough improved feature extraction. However, most existing ED models apply\nuniform feature fusion across all pixels, ignoring critical differences between\nregions such as edges and textures. To address this limitation, we propose the\nExtractor-Selector (E-S) paradigm, a novel framework that introduces pixel-wise\nfeature selection for more adaptive and precise fusion. Unlike conventional\nimage-level fusion that applies the same convolutional kernel to all pixels,\nour approach dynamically selects relevant features at each pixel, enabling more\nrefined edge predictions. The E-S framework can be seamlessly integrated with\nexisting ED models without architectural changes, delivering substantial\nperformance gains. It can also be combined with enhanced feature extractors for\nfurther accuracy improvements. Extensive experiments across multiple benchmarks\nconfirm that our method consistently outperforms baseline ED models. For\ninstance, on the BIPED2 dataset, the proposed framework can achieve over 7$\\%$\nimprovements in ODS and OIS, and 22$\\%$ improvements in AP, demonstrating its\neffectiveness and superiority.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2501.02534v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2505.17625v1","updated":"2025-05-23T08:36:22Z","published":"2025-05-23T08:36:22Z","title":"Enhancing Large Vision-Language Models with Layout Modality for Table\n  Question Answering on Japanese Annual Securities Reports","summary":"  With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats.\n","authors":["Hayato Aida","Kosuke Takahashi","Takahiro Omi"],"pdf_url":"https://arxiv.org/pdf/2505.17625v1.pdf","comment":"Accepted at IIAI AAI 2025, the 3rd International Conference on\n  Computational and Data Sciences in Economics and Finance"},{"id":"http://arxiv.org/abs/2308.04137v3","updated":"2025-05-23T08:32:36Z","published":"2023-08-08T08:50:27Z","title":"A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep\n  Learning Image Classifiers","summary":"  Reliable and robust evaluation methods are a necessary first step towards\ndeveloping machine learning models that are themselves robust and reliable.\nUnfortunately, current evaluation protocols typically used to assess\nclassifiers fail to comprehensively evaluate performance as they tend to rely\non limited types of test data, and ignore others. For example, using the\nstandard test data fails to evaluate the predictions made by the classifier to\nsamples from classes it was not trained on. On the other hand, testing with\ndata containing samples from unknown classes fails to evaluate how well the\nclassifier can predict the labels for known classes. This article advocates\nbenchmarking performance using a wide range of different types of data and\nusing a single metric that can be applied to all such data types to produce a\nconsistent evaluation of performance. Using the proposed benchmark it is found\nthat current deep neural networks, including those trained with methods that\nare believed to produce state-of-the-art robustness, are vulnerable to making\nmistakes on certain types of data. This means that such models will be\nunreliable in real-world scenarios where they may encounter data from many\ndifferent domains, and that they are insecure as they can be easily fooled into\nmaking the wrong decisions. It is hoped that these results will motivate the\nwider adoption of more comprehensive testing methods that will, in turn, lead\nto the development of more robust machine learning methods in the future.\n  Code is available at: https://codeberg.org/mwspratling/RobustnessEvaluation\n","authors":["Michael W. Spratling"],"pdf_url":"https://arxiv.org/pdf/2308.04137v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17619v1","updated":"2025-05-23T08:27:05Z","published":"2025-05-23T08:27:05Z","title":"CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography\n  Quality Assessment","summary":"  Synthetic X-ray angiographies generated by modern generative models hold\ngreat potential to reduce the use of contrast agents in vascular interventional\nprocedures. However, low-quality synthetic angiographies can significantly\nincrease procedural risk, underscoring the need for reliable image quality\nassessment (IQA) methods. Existing IQA models, however, fail to leverage\nauxiliary images as references during evaluation and lack fine-grained,\ntask-specific metrics necessary for clinical relevance. To address these\nlimitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based\nframework that predicts fine-grained quality scores by effectively\nincorporating auxiliary information from related images. In the absence of\nangiography datasets, CAS-3K is constructed, comprising 3,565 synthetic\nangiographies along with score annotations. To ensure clinically meaningful\nassessment, three task-specific evaluation metrics are defined. Furthermore, a\nMulti-path featUre fuSion and rouTing (MUST) module is designed to enhance\nimage representations by adaptively fusing and routing visual tokens to\nmetric-specific branches. Extensive experiments on the CAS-3K dataset\ndemonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods\nby a considerable margin.\n","authors":["Bo Wang","De-Xing Huang","Xiao-Hu Zhou","Mei-Jiang Gui","Nu-Fang Xiao","Jian-Long Hao","Ming-Yuan Liu","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2505.17619v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.17618v1","updated":"2025-05-23T08:25:46Z","published":"2025-05-23T08:25:46Z","title":"Scaling Image and Video Generation via Test-Time Evolutionary Search","summary":"  As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\n\\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.\n","authors":["Haoran He","Jiajun Liang","Xintao Wang","Pengfei Wan","Di Zhang","Kun Gai","Ling Pan"],"pdf_url":"https://arxiv.org/pdf/2505.17618v1.pdf","comment":"37 pages. Project: https://tinnerhrhe.github.io/evosearch"},{"id":"http://arxiv.org/abs/2505.17614v1","updated":"2025-05-23T08:21:58Z","published":"2025-05-23T08:21:58Z","title":"PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive\n  Learning and Pathology-Informed Synthetic Embeddings","summary":"  Unsupervised pathology detection trains models on non-pathological data to\nflag deviations as pathologies, offering strong generalizability for\nidentifying novel diseases and avoiding costly annotations. However, building\nreliable normality models requires vast healthy datasets, as hospitals' data is\ninherently biased toward symptomatic populations, while privacy regulations\nhinder the assembly of representative healthy cohorts. To address this\nlimitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection\nframework that requires only a small set of non-pathological samples (minimum 2\nshots), significantly improving data efficiency. We introduce Global-Local\nContrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the\nvariability of non-pathological embeddings and a Global Contrastive Loss to\nenhance the discrimination of pathological regions. We also propose a\nPathology-informed Embedding Generation (PiEG) module that synthesizes\npathological embeddings guided by the global loss, better exploiting the\nlimited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8\ndatasets, PathoSCOPE achieves state-of-the-art performance among unsupervised\nmethods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).\n","authors":["Sinchee Chin","Yinuo Ma","Xiaochen Yang","Jing-Hao Xue","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17613v1","updated":"2025-05-23T08:21:28Z","published":"2025-05-23T08:21:28Z","title":"MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation","summary":"  Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.\n","authors":["Jihan Yao","Yushi Hu","Yujie Yi","Bin Han","Shangbin Feng","Guang Yang","Bingbing Wen","Ranjay Krishna","Lucy Lu Wang","Yulia Tsvetkov","Noah A. Smith","Banghua Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.17613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17591v1","updated":"2025-05-23T07:56:43Z","published":"2025-05-23T07:56:43Z","title":"MinkUNeXt-SI: Improving point cloud-based place recognition including\n  spherical coordinates and LiDAR intensity","summary":"  In autonomous navigation systems, the solution of the place recognition\nproblem is crucial for their safe functioning. But this is not a trivial\nsolution, since it must be accurate regardless of any changes in the scene,\nsuch as seasonal changes and different weather conditions, and it must be\ngeneralizable to other environments. This paper presents our method,\nMinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input\ndata to obtain its spherical coordinates and intensity values normalized within\na range of 0 to 1 for each point, and it produces a robust place recognition\ndescriptor. To that end, a deep learning approach that combines Minkowski\nconvolutions and a U-net architecture with skip connections is used. The\nresults of MinkUNeXt-SI demonstrate that this method reaches and surpasses\nstate-of-the-art performance while it also generalizes satisfactorily to other\ndatasets. Additionally, we showcase the capture of a custom dataset and its use\nin evaluating our solution, which also achieves outstanding results. Both the\ncode of our solution and the runs of our dataset are publicly available for\nreproducibility purposes.\n","authors":["Judith Vilella-Cantos","Juan Jos Cabrera","Luis Pay","Mnica Ballesta","David Valiente"],"pdf_url":"https://arxiv.org/pdf/2505.17591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17590v1","updated":"2025-05-23T07:56:25Z","published":"2025-05-23T07:56:25Z","title":"CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human\n  Head Synthesis","summary":"  Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high\nquality synthesis of human heads. However, existing methods stabilize training\nand enhance rendering quality from steep viewpoints by conditioning the random\nlatent vector on the current camera position. This compromises 3D consistency,\nas we observe significant identity changes when re-synthesizing the 3D head\nwith each camera shift. Conversely, fixing the camera to a single viewpoint\nyields high-quality renderings for that perspective but results in poor\nperformance for novel views. Removing view-conditioning typically destabilizes\nGAN training, often causing the training to collapse. In response to these\nchallenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework\nthat enables stable training and high-quality 3D-consistent synthesis of human\nheads without relying on view-conditioning. To ensure training stability, we\nintroduce a multi-view regularization technique that enhances generator\nconvergence with minimal computational overhead. Additionally, we adapt the\nconditional loss used in existing 3D Gaussian splatting GANs and propose a\ngenerator architecture designed to not only stabilize training but also\nfacilitate efficient rendering and straightforward scaling, enabling output\nresolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate\na new dataset derived from FFHQ. This dataset enables very high resolutions,\nfocuses on larger portions of the human head, reduces view-dependent artifacts\nfor improved 3D consistency, and excludes images where subjects are obscured by\nhands or other objects. As a result, our approach achieves very high rendering\nquality, supported by competitive FID scores, while ensuring consistent 3D\nscene generation. Check our our project page here:\nhttps://fraunhoferhhi.github.io/cgs-gan/\n","authors":["Florian Barthel","Wieland Morgenstern","Paul Hinzer","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2505.17590v1.pdf","comment":"Main paper 12 pages, supplementary materials 8 pages"},{"id":"http://arxiv.org/abs/2505.10595v2","updated":"2025-05-23T07:47:23Z","published":"2025-05-15T09:44:23Z","title":"ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive\n  Hierarchical Network for Infrared Small Target Detection","summary":"  Infrared small target detection (ISTD) is critical in both civilian and\nmilitary applications. However, the limited texture and structural information\nin infrared images makes accurate detection particularly challenging. Although\nrecent deep learning-based methods have improved performance, their use of\nconventional convolution kernels limits adaptability to complex scenes and\ndiverse targets. Moreover, pooling operations often cause feature loss and\ninsufficient exploitation of image information. To address these issues, we\npropose an adaptive receptive field convolution and wavelet-attentive\nhierarchical network for infrared small target detection (ARFC-WAHNet). This\nnetwork incorporates a multi-receptive field feature interaction convolution\n(MRFFIConv) module to adaptively extract discriminative features by integrating\nmultiple convolutional branches with a gated unit. A wavelet frequency\nenhancement downsampling (WFED) module leverages Haar wavelet transform and\nfrequency-domain reconstruction to enhance target features and suppress\nbackground noise. Additionally, we introduce a high-low feature fusion (HLFF)\nmodule for integrating low-level details with high-level semantics, and a\nglobal median enhancement attention (GMEA) module to improve feature diversity\nand expressiveness via global attention. Experiments on public datasets SIRST,\nNUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent\nstate-of-the-art methods in both detection accuracy and robustness,\nparticularly under complex backgrounds. The code is available at\nhttps://github.com/Leaf2001/ARFC-WAHNet.\n","authors":["Xingye Cui","Junhai Luo","Jiakun Deng","Kexuan Li","Xiangyu Qiu","Zhenming Peng"],"pdf_url":"https://arxiv.org/pdf/2505.10595v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17582v1","updated":"2025-05-23T07:44:33Z","published":"2025-05-23T07:44:33Z","title":"Distance Estimation in Outdoor Driving Environments Using Phase-only\n  Correlation Method with Event Cameras","summary":"  With the growing adoption of autonomous driving, the advancement of sensor\ntechnology is crucial for ensuring safety and reliable operation. Sensor fusion\ntechniques that combine multiple sensors such as LiDAR, radar, and cameras have\nproven effective, but the integration of multiple devices increases both\nhardware complexity and cost. Therefore, developing a single sensor capable of\nperforming multiple roles is highly desirable for cost-efficient and scalable\nautonomous driving systems.\n  Event cameras have emerged as a promising solution due to their unique\ncharacteristics, including high dynamic range, low latency, and high temporal\nresolution. These features enable them to perform well in challenging lighting\nconditions, such as low-light or backlit environments. Moreover, their ability\nto detect fine-grained motion events makes them suitable for applications like\npedestrian detection and vehicle-to-infrastructure communication via visible\nlight.\n  In this study, we present a method for distance estimation using a monocular\nevent camera and a roadside LED bar. By applying a phase-only correlation\ntechnique to the event data, we achieve sub-pixel precision in detecting the\nspatial shift between two light sources. This enables accurate\ntriangulation-based distance estimation without requiring stereo vision. Field\nexperiments conducted in outdoor driving scenarios demonstrated that the\nproposed approach achieves over 90% success rate with less than 0.5-meter error\nfor distances ranging from 20 to 60 meters.\n  Future work includes extending this method to full position estimation by\nleveraging infrastructure such as smart poles equipped with LEDs, enabling\nevent-camera-based vehicles to determine their own position in real time. This\nadvancement could significantly enhance navigation accuracy, route\noptimization, and integration into intelligent transportation systems.\n","authors":["Masataka Kobayashi","Shintaro Shiba","Quan Kong","Norimasa Kobori","Tsukasa Shimizu","Shan Lu","Takaya Yamazato"],"pdf_url":"https://arxiv.org/pdf/2505.17582v1.pdf","comment":"6 pages, 7 figures. To appear in IEEE Intelligent Vehicles Symposium\n  (IV) 2025"},{"id":"http://arxiv.org/abs/2505.17581v1","updated":"2025-05-23T07:43:14Z","published":"2025-05-23T07:43:14Z","title":"MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse\n  Weather Image Recovery","summary":"  Restoring images degraded by adverse weather remains a significant challenge\ndue to the highly non-uniform and spatially heterogeneous nature of\nweather-induced artifacts, e.g., fine-grained rain streaks versus widespread\nhaze. Accurately estimating the underlying degradation can intuitively provide\nrestoration models with more targeted and effective guidance, enabling adaptive\nprocessing strategies. To this end, we propose a Morton-Order Degradation\nEstimation Mechanism (MODEM) for adverse weather image restoration. Central to\nMODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates\nMorton-coded spatial ordering with selective state-space models to capture\nlong-range dependencies while preserving local structural coherence.\nComplementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)\nthat disentangles and estimates both global and local degradation priors. These\npriors dynamically condition the MOS2D modules, facilitating adaptive and\ncontext-aware restoration. Extensive experiments and ablation studies\ndemonstrate that MODEM achieves state-of-the-art results across multiple\nbenchmarks and weather types, highlighting its effectiveness in modeling\ncomplex degradation dynamics. Our code will be released at\nhttps://github.com/hainuo-wang/MODEM.git.\n","authors":["Hainuo Wang","Qiming Hu","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2505.17581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17574v1","updated":"2025-05-23T07:33:25Z","published":"2025-05-23T07:33:25Z","title":"InfLVG: Reinforce Inference-Time Consistent Long Video Generation with\n  GRPO","summary":"  Recent advances in text-to-video generation, particularly with autoregressive\nmodels, have enabled the synthesis of high-quality videos depicting individual\nscenes. However, extending these models to generate long, cross-scene videos\nremains a significant challenge. As the context length grows during\nautoregressive decoding, computational costs rise sharply, and the model's\nability to maintain consistency and adhere to evolving textual prompts\ndeteriorates. We introduce InfLVG, an inference-time framework that enables\ncoherent long video generation without requiring additional long-form video\ndata. InfLVG leverages a learnable context selection policy, optimized via\nGroup Relative Policy Optimization (GRPO), to dynamically identify and retain\nthe most semantically relevant context throughout the generation process.\nInstead of accumulating the entire generation history, the policy ranks and\nselects the top-$K$ most contextually relevant tokens, allowing the model to\nmaintain a fixed computational budget while preserving content consistency and\nprompt alignment. To optimize the policy, we design a hybrid reward function\nthat jointly captures semantic alignment, cross-scene consistency, and artifact\nreduction. To benchmark performance, we introduce the Cross-scene Video\nBenchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates\ncomplex multi-scene transitions involving shared subjects and varied\nactions/backgrounds. Experimental results show that InfLVG can extend video\nlength by up to 9$\\times$, achieving strong consistency and semantic fidelity\nacross scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.\n","authors":["Xueji Fang","Liyuan Ma","Zhiyang Chen","Mingyuan Zhou","Guo-jun Qi"],"pdf_url":"https://arxiv.org/pdf/2505.17574v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2505.17567v1","updated":"2025-05-23T07:27:19Z","published":"2025-05-23T07:27:19Z","title":"Enhancing Fourier-based Doppler Resolution with Diffusion Models","summary":"  In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated.\n","authors":["Denisa Qosja","Kilian Barth","Simon Wagner"],"pdf_url":"https://arxiv.org/pdf/2505.17567v1.pdf","comment":"Published at International Radar Symposium (IRS) 2025"},{"id":"http://arxiv.org/abs/2505.17561v1","updated":"2025-05-23T07:09:10Z","published":"2025-05-23T07:09:10Z","title":"Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model","summary":"  The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/\n","authors":["Kwanyoung Kim","Sanghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2505.17561v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.17560v1","updated":"2025-05-23T07:08:09Z","published":"2025-05-23T07:08:09Z","title":"Deeper Diffusion Models Amplify Bias","summary":"  Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically.\n","authors":["Shahin Hakemi","Naveed Akhtar","Ghulam Mubashar Hassan","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2505.17560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16839v2","updated":"2025-05-23T07:07:29Z","published":"2025-05-22T16:07:12Z","title":"LaViDa: A Large Diffusion Language Model for Multimodal Understanding","summary":"  Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.\n","authors":["Shufan Li","Konstantinos Kallidromitis","Hritik Bansal","Akash Gokul","Yusuke Kato","Kazuki Kozuka","Jason Kuen","Zhe Lin","Kai-Wei Chang","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2505.16839v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.17556v1","updated":"2025-05-23T07:01:38Z","published":"2025-05-23T07:01:38Z","title":"Wildfire spread forecasting with Deep Learning","summary":"  Accurate prediction of wildfire spread is crucial for effective risk\nmanagement, emergency response, and strategic resource allocation. In this\nstudy, we present a deep learning (DL)-based framework for forecasting the\nfinal extent of burned areas, using data available at the time of ignition. We\nleverage a spatio-temporal dataset that covers the Mediterranean region from\n2006 to 2022, incorporating remote sensing data, meteorological observations,\nvegetation maps, land cover classifications, anthropogenic factors, topography\ndata, and thermal anomalies. To evaluate the influence of temporal context, we\nconduct an ablation study examining how the inclusion of pre- and post-ignition\ndata affects model performance, benchmarking the temporal-aware DL models\nagainst a baseline trained exclusively on ignition-day inputs. Our results\nindicate that multi-day observational data substantially improve predictive\naccuracy. Particularly, the best-performing model, incorporating a temporal\nwindow of four days before to five days after ignition, improves both the F1\nscore and the Intersection over Union by almost 5% in comparison to the\nbaseline on the test dataset. We publicly release our dataset and models to\nenhance research into data-driven approaches for wildfire modeling and\nresponse.\n","authors":["Nikolaos Anastasiou","Spyros Kondylatos","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2505.17556v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.17555v1","updated":"2025-05-23T07:00:59Z","published":"2025-05-23T07:00:59Z","title":"ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action\n  Localization","summary":"  Temporal Action Localization (TAL) aims to detect the start and end\ntimestamps of actions in a video. However, the training of TAL models requires\na substantial amount of manually annotated data. Data programming is an\nefficient method to create training labels with a series of human-defined\nlabeling functions. However, its application in TAL faces difficulties of\ndefining complex actions in the context of temporal video frames. In this\npaper, we propose ProTAL, a drag-and-link video programming framework for TAL.\nProTAL enables users to define \\textbf{key events} by dragging nodes\nrepresenting body parts and objects and linking them to constrain the relations\n(direction, distance, etc.). These definitions are used to generate action\nlabels for large-scale unlabelled videos. A semi-supervised method is then\nemployed to train TAL models with such labels. We demonstrate the effectiveness\nof ProTAL through a usage scenario and a user study, providing insights into\ndesigning video programming framework.\n","authors":["Yuchen He","Jianbing Lv","Liqi Cheng","Lingyu Meng","Dazhen Deng","Yingcai Wu"],"pdf_url":"https://arxiv.org/pdf/2505.17555v1.pdf","comment":"Accepted at CHI'25"},{"id":"http://arxiv.org/abs/2505.17551v1","updated":"2025-05-23T06:56:44Z","published":"2025-05-23T06:56:44Z","title":"Center-aware Residual Anomaly Synthesis for Multi-class Industrial\n  Anomaly Detection","summary":"  Anomaly detection plays a vital role in the inspection of industrial images.\nMost existing methods require separate models for each category, resulting in\nmultiplied deployment costs. This highlights the challenge of developing a\nunified model for multi-class anomaly detection. However, the significant\nincrease in inter-class interference leads to severe missed detections.\nFurthermore, the intra-class overlap between normal and abnormal samples,\nparticularly in synthesis-based methods, cannot be ignored and may lead to\nover-detection. To tackle these issues, we propose a novel Center-aware\nResidual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.\nCRAS leverages center-aware residual learning to couple samples from different\ncategories into a unified center, mitigating the effects of inter-class\ninterference. To further reduce intra-class overlap, CRAS introduces\ndistance-guided anomaly synthesis that adaptively adjusts noise variance based\non normal data distribution. Experimental results on diverse datasets and\nreal-world industrial applications demonstrate the superior detection accuracy\nand competitive inference speed of CRAS. The source code and the newly\nconstructed dataset are publicly available at\nhttps://github.com/cqylunlun/CRAS.\n","authors":["Qiyu Chen","Huiyuan Luo","Haiming Yao","Wei Luo","Zhen Qu","Chengkan Lv","Zhengtao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.17551v1.pdf","comment":"Accepted by IEEE Transactions on Industrial Informatics (TII)"},{"id":"http://arxiv.org/abs/2505.17550v1","updated":"2025-05-23T06:56:32Z","published":"2025-05-23T06:56:32Z","title":"T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion\n  Models","summary":"  Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.\n","authors":["Xiaoyu Ye","Songjie Cheng","Yongtao Wang","Yajiao Xiong","Yishen Li"],"pdf_url":"https://arxiv.org/pdf/2505.17550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11117v3","updated":"2025-05-23T06:54:57Z","published":"2025-03-14T06:29:47Z","title":"Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied\n  Question Answering","summary":"  Embodied Question Answering (EQA) is a challenging task in embodied\nintelligence that requires agents to dynamically explore 3D environments,\nactively gather visual information, and perform multi-step reasoning to answer\nquestions. However, current EQA approaches suffer from critical limitations in\nexploration efficiency, dataset design, and evaluation metrics. Moreover,\nexisting datasets often introduce biases or prior knowledge, leading to\ndisembodied reasoning, while frontier-based exploration strategies struggle in\ncluttered environments and fail to ensure fine-grained exploration of\ntask-relevant areas. To address these challenges, we construct the\nEXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the\nlargest dataset designed specifically to evaluate both exploration and\nreasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories\nand 2,044 question-trajectory pairs. To improve exploration efficiency, we\npropose Fine-EQA, a hybrid exploration model that integrates frontier-based and\ngoal-oriented navigation to guide agents toward task-relevant regions more\neffectively. Additionally, we introduce a novel evaluation metric,\nExploration-Answer Consistency (EAC), which ensures faithful assessment by\nmeasuring the alignment between answer grounding and exploration reliability.\nExtensive experimental comparisons with state-of-the-art EQA models demonstrate\nthe effectiveness of our EXPRESS-Bench in advancing embodied exploration and\nquestion reasoning.\n","authors":["Kaixuan Jiang","Yang Liu","Weixing Chen","Jingzhou Luo","Ziliang Chen","Ling Pan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.11117v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17544v1","updated":"2025-05-23T06:51:24Z","published":"2025-05-23T06:51:24Z","title":"FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image\n  Segmentation","summary":"  Medical image segmentation faces persistent challenges due to severe class\nimbalance and the frequency-specific distribution of anatomical structures.\nMost conventional CNN-based methods operate in the spatial domain and struggle\nto capture minority class signals, often affected by frequency aliasing and\nlimited spectral selectivity. Transformer-based models, while powerful in\nmodeling global dependencies, tend to overlook critical local details necessary\nfor fine-grained segmentation. To overcome these limitations, we propose\nFreqU-FNet, a novel U-shaped segmentation architecture operating in the\nfrequency domain. Our framework incorporates a Frequency Encoder that leverages\nLow-Pass Frequency Convolution and Daubechies wavelet-based downsampling to\nextract multi-scale spectral features. To reconstruct fine spatial details, we\nintroduce a Spatial Learnable Decoder (SLD) equipped with an adaptive\nmulti-branch upsampling strategy. Furthermore, we design a frequency-aware loss\n(FAL) function to enhance minority class learning. Extensive experiments on\nmultiple medical segmentation benchmarks demonstrate that FreqU-FNet\nconsistently outperforms both CNN and Transformer baselines, particularly in\nhandling under-represented classes, by effectively exploiting discriminative\nfrequency bands.\n","authors":["Ruiqi Xing"],"pdf_url":"https://arxiv.org/pdf/2505.17544v1.pdf","comment":"15 pages, 1 figure"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.18120v1","updated":"2025-05-23T17:21:14Z","published":"2025-05-23T17:21:14Z","title":"Bidirectional Knowledge Distillation for Enhancing Sequential\n  Recommendation with Large Language Models","summary":"  Large language models (LLMs) have demonstrated exceptional performance in\nunderstanding and generating semantic patterns, making them promising\ncandidates for sequential recommendation tasks. However, when combined with\nconventional recommendation models (CRMs), LLMs often face challenges related\nto high inference costs and static knowledge transfer methods. In this paper,\nwe propose a novel mutual distillation framework, LLMD4Rec, that fosters\ndynamic and bidirectional knowledge exchange between LLM-centric and CRM-based\nrecommendation systems. Unlike traditional unidirectional distillation methods,\nLLMD4Rec enables iterative optimization by alternately refining both models,\nenhancing the semantic understanding of CRMs and enriching LLMs with\ncollaborative signals from user-item interactions. By leveraging sample-wise\nadaptive weighting and aligning output distributions, our approach eliminates\nthe need for additional parameters while ensuring effective knowledge transfer.\nExtensive experiments on real-world datasets demonstrate that LLMD4Rec\nsignificantly improves recommendation accuracy across multiple benchmarks\nwithout increasing inference costs. This method provides a scalable and\nefficient solution for combining the strengths of both LLMs and CRMs in\nsequential recommendation systems.\n","authors":["Jiongran Wu","Jiahao Liu","Dongsheng Li","Guangping Zhang","Mingzhe Han","Hansu Gu","Peng Zhang","Li Shang","Tun Lu","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2505.18120v1.pdf","comment":"11 pages, under review"},{"id":"http://arxiv.org/abs/2505.18059v1","updated":"2025-05-23T16:07:14Z","published":"2025-05-23T16:07:14Z","title":"Assessing the performance of 8 AI chatbots in bibliographic reference\n  retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate","summary":"  This study analyzes the performance of eight generative artificial\nintelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le\nChat, and Perplexity -- in their free versions, in the task of generating\nacademic bibliographic references within the university context. A total of 400\nreferences were evaluated across the five major areas of knowledge (Health,\nEngineering, Experimental Sciences, Social Sciences, and Humanities), based on\na standardized prompt. Each reference was assessed according to five key\ncomponents (authorship, year, title, source, and location), along with document\ntype, publication age, and error count. The results show that only 26.5% of the\nreferences were fully correct, 33.8% partially correct, and 39.8% were either\nerroneous or entirely fabricated. Grok and DeepSeek stood out as the only\nchatbots that did not generate false references, while Copilot, Perplexity, and\nClaude exhibited the highest hallucination rates. Furthermore, the chatbots\nshowed a greater tendency to generate book references over journal articles,\nalthough the latter had a significantly higher fabrication rate. A high degree\nof overlap was also detected among the sources provided by several models,\nparticularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal\nstructural limitations in current AI models, highlight the risks of uncritical\nuse by students, and underscore the need to strengthen information and critical\nliteracy regarding the use of AI tools in higher education.\n","authors":["lvaro Cabezas-Clavijo","Pavel Sidorenko-Bautista"],"pdf_url":"https://arxiv.org/pdf/2505.18059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10440v2","updated":"2025-05-23T15:35:10Z","published":"2025-02-10T09:15:56Z","title":"Towards Copyright Protection for Knowledge Bases of Retrieval-augmented\n  Language Models via Reasoning","summary":"  Large language models (LLMs) are increasingly integrated into real-world\npersonalized applications through retrieval-augmented generation (RAG)\nmechanisms to supplement their responses with domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning or backdoor attacks. However, these\nmethods require altering the LLM's results of verification samples, inevitably\nmaking these watermarks susceptible to anomaly detection and even introducing\nnew security risks. To address these challenges, we propose \\name{} for\n`harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct yet benign verification behaviors\nin the space of chain-of-thought (CoT) reasoning, maintaining the correctness\nof the final answer. Our method has three main stages: (1) Generating CoTs: For\neach verification question, we generate two `innocent' CoTs, including a target\nCoT for building watermark behaviors; (2) Optimizing Watermark Phrases and\nTarget CoTs: Inspired by our theoretical analysis, we optimize them to minimize\nretrieval errors under the \\emph{black-box} and \\emph{text-only} setting of\nsuspicious LLM, ensuring that only watermarked verification queries can\nretrieve their correspondingly target CoTs contained in the knowledge base; (3)\nOwnership Verification: We exploit a pairwise Wilcoxon test to verify whether a\nsuspicious LLM is augmented with the protected knowledge base by comparing its\nresponses to watermarked and benign verification queries. Our experiments on\ndiverse benchmarks demonstrate that \\name{} effectively protects knowledge\nbases and its resistance to adaptive attacks.\n","authors":["Junfeng Guo","Yiming Li","Ruibo Chen","Yihan Wu","Chenxi Liu","Yanshuo Chen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.10440v2.pdf","comment":"The first two authors contributed equally to this work. 25 pages"},{"id":"http://arxiv.org/abs/2505.18006v1","updated":"2025-05-23T15:10:28Z","published":"2025-05-23T15:10:28Z","title":"AI Literacy for Legal AI Systems: A practical approach","summary":"  Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.\n","authors":["Gizem Gultekin-Varkonyi"],"pdf_url":"https://arxiv.org/pdf/2505.18006v1.pdf","comment":"Forthcoming in Iustum Aequum Salutare (2025) vol.21"},{"id":"http://arxiv.org/abs/2505.17999v1","updated":"2025-05-23T15:04:16Z","published":"2025-05-23T15:04:16Z","title":"Revisiting Feature Interactions from the Perspective of Quadratic Neural\n  Networks for Click-through Rate Prediction","summary":"  Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Lei Sang","Jieming Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.17999v1.pdf","comment":"KDD'25 accepted"},{"id":"http://arxiv.org/abs/2505.17925v1","updated":"2025-05-23T14:04:38Z","published":"2025-05-23T14:04:38Z","title":"Enhancing CTR Prediction with De-correlated Expert Networks","summary":"  Modeling feature interactions is essential for accurate click-through rate\n(CTR) prediction in advertising systems. Recent studies have adopted the\nMixture-of-Experts (MoE) approach to improve performance by ensembling multiple\nfeature interaction experts. These studies employ various strategies, such as\nlearning independent embedding tables for each expert or utilizing\nheterogeneous expert architectures, to differentiate the experts, which we\nrefer to expert \\emph{de-correlation}. However, it remains unclear whether\nthese strategies effectively achieve de-correlated experts. To address this, we\npropose a De-Correlated MoE (D-MoE) framework, which introduces a Cross-Expert\nDe-Correlation loss to minimize expert correlations.Additionally, we propose a\nnovel metric, termed Cross-Expert Correlation, to quantitatively evaluate the\nexpert de-correlation degree. Based on this metric, we identify a key finding\nfor MoE framework design: \\emph{different de-correlation strategies are\nmutually compatible, and progressively employing them leads to reduced\ncorrelation and enhanced performance}.Extensive experiments have been conducted\nto validate the effectiveness of D-MoE and the de-correlation principle.\nMoreover, online A/B testing on Tencent's advertising platforms demonstrates\nthat D-MoE achieves a significant 1.19\\% Gross Merchandise Volume (GMV) lift\ncompared to the Multi-Embedding MoE baseline.\n","authors":["Jiancheng Wang","Mingjia Yin","Junwei Pan","Ximei Wang","Hao Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2505.17925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17861v1","updated":"2025-05-23T13:13:44Z","published":"2025-05-23T13:13:44Z","title":"Superplatforms Have to Attack AI Agents","summary":"  Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.\n","authors":["Jianghao Lin","Jiachen Zhu","Zheli Zhou","Yunjia Xi","Weiwen Liu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.17861v1.pdf","comment":"Position paper under review"},{"id":"http://arxiv.org/abs/2505.17810v1","updated":"2025-05-23T12:28:10Z","published":"2025-05-23T12:28:10Z","title":"VIBE: Vector Index Benchmark for Embeddings","summary":"  Approximate nearest neighbor (ANN) search is a performance-critical component\nof many machine learning pipelines. Rigorous benchmarking is essential for\nevaluating the performance of vector indexes for ANN search. However, the\ndatasets of the existing benchmarks are no longer representative of the current\napplications of ANN search. Hence, there is an urgent need for an up-to-date\nset of benchmarks. To this end, we introduce Vector Index Benchmark for\nEmbeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE\ncontains a pipeline for creating benchmark datasets using dense embedding\nmodels characteristic of modern applications, such as retrieval-augmented\ngeneration (RAG). To replicate real-world workloads, we also include\nout-of-distribution (OOD) datasets where the queries and the corpus are drawn\nfrom different distributions. We use VIBE to conduct a comprehensive evaluation\nof SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution\nand 6 out-of-distribution datasets.\n","authors":["Elias Jsaari","Ville Hyvnen","Matteo Ceccarello","Teemu Roos","Martin Aumller"],"pdf_url":"https://arxiv.org/pdf/2505.17810v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2505.17796v1","updated":"2025-05-23T12:15:23Z","published":"2025-05-23T12:15:23Z","title":"DetailFusion: A Dual-branch Framework with Detail Enhancement for\n  Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images from a gallery\nbased on a reference image and modification text as a combined query. Recent\napproaches focus on balancing global information from two modalities and encode\nthe query into a unified feature for retrieval. However, due to insufficient\nattention to fine-grained details, these coarse fusion methods often struggle\nwith handling subtle visual alterations or intricate textual instructions. In\nthis work, we propose DetailFusion, a novel dual-branch framework that\neffectively coordinates information across global and detailed granularities,\nthereby enabling detail-enhanced CIR. Our approach leverages atomic detail\nvariation priors derived from an image editing dataset, supplemented by a\ndetail-oriented optimization strategy to develop a Detail-oriented Inference\nBranch. Furthermore, we design an Adaptive Feature Compositor that dynamically\nfuses global and detailed features based on fine-grained information of each\nunique multimodal query. Extensive experiments and ablation analyses not only\ndemonstrate that our method achieves state-of-the-art performance on both CIRR\nand FashionIQ datasets but also validate the effectiveness and cross-domain\nadaptability of detail enhancement for CIR.\n","authors":["Yuxin Yang","Yinan Zhou","Yuxin Chen","Ziqi Zhang","Zongyang Ma","Chunfeng Yuan","Bing Li","Lin Song","Jun Gao","Peng Li","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2505.17796v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.17762v1","updated":"2025-05-23T11:35:03Z","published":"2025-05-23T11:35:03Z","title":"Resolving Conflicting Evidence in Automated Fact-Checking: A Study on\n  Retrieval-Augmented LLMs","summary":"  Large Language Models (LLMs) augmented with retrieval mechanisms have\ndemonstrated significant potential in fact-checking tasks by integrating\nexternal knowledge. However, their reliability decreases when confronted with\nconflicting evidence from sources of varying credibility. This paper presents\nthe first systematic evaluation of Retrieval-Augmented Generation (RAG) models\nfor fact-checking in the presence of conflicting evidence. To support this\nstudy, we introduce \\textbf{CONFACT} (\\textbf{Con}flicting Evidence for\n\\textbf{Fact}-Checking) (Dataset available at\nhttps://github.com/zoeyyes/CONFACT), a novel dataset comprising questions\npaired with conflicting information from various sources. Extensive experiments\nreveal critical vulnerabilities in state-of-the-art RAG methods, particularly\nin resolving conflicts stemming from differences in media source credibility.\nTo address these challenges, we investigate strategies to integrate media\nbackground information into both the retrieval and generation stages. Our\nresults show that effectively incorporating source credibility significantly\nenhances the ability of RAG models to resolve conflicting evidence and improve\nfact-checking performance.\n","authors":["Ziyu Ge","Yuhao Wu","Daniel Wai Kit Chin","Roy Ka-Wei Lee","Rui Cao"],"pdf_url":"https://arxiv.org/pdf/2505.17762v1.pdf","comment":"Camera-ready for IJCAI 2025, AI and Social Good"},{"id":"http://arxiv.org/abs/2505.17736v1","updated":"2025-05-23T10:58:22Z","published":"2025-05-23T10:58:22Z","title":"Modeling Ranking Properties with In-Context Learning","summary":"  While standard IR models are mainly designed to optimize relevance,\nreal-world search often needs to balance additional objectives such as\ndiversity and fairness. These objectives depend on inter-document interactions\nand are commonly addressed using post-hoc heuristics or supervised learning\nmethods, which require task-specific training for each ranking scenario and\ndataset. In this work, we propose an in-context learning (ICL) approach that\neliminates the need for such training. Instead, our method relies on a small\nnumber of example rankings that demonstrate the desired trade-offs between\nobjectives for past queries similar to the current input. We evaluate our\napproach on four IR test collections to investigate multiple auxiliary\nobjectives: group fairness (TREC Fairness), polarity diversity (Touch\\'e), and\ntopical diversity (TREC Deep Learning 2019/2020). We empirically validate that\nour method enables control over ranking behavior through demonstration\nengineering, allowing nuanced behavioral adjustments without explicit\noptimization.\n","authors":["Nilanjan Sinhababu","Andrew Parry","Debasis Ganguly","Pabitra Mitra"],"pdf_url":"https://arxiv.org/pdf/2505.17736v1.pdf","comment":"9 pages, 3 tables, 2 figures"},{"id":"http://arxiv.org/abs/2505.15872v2","updated":"2025-05-23T10:16:01Z","published":"2025-05-21T14:44:40Z","title":"InfoDeepSeek: Benchmarking Agentic Information Seeking for\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research.\n","authors":["Yunjia Xi","Jianghao Lin","Menghui Zhu","Yongzhao Xiao","Zhuoying Ou","Jiaqi Liu","Tong Wan","Bo Chen","Weiwen Liu","Yasheng Wang","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.15872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12486v3","updated":"2025-05-23T09:33:32Z","published":"2024-12-17T02:43:54Z","title":"Boosting Long-Context Management via Query-Guided Activation Refilling","summary":"  Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.\n","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Zhicheng Dou","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2412.12486v3.pdf","comment":"ACL25 Main Conference"},{"id":"http://arxiv.org/abs/2505.17631v1","updated":"2025-05-23T08:43:46Z","published":"2025-05-23T08:43:46Z","title":"BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling","summary":"  In recent years, foundational models have revolutionized the fields of\nlanguage and vision, demonstrating remarkable abilities in understanding and\ngenerating complex data; however, similar advances in user behavior modeling\nhave been limited, largely due to the complexity of behavioral data and the\nchallenges involved in capturing intricate temporal and contextual\nrelationships in user activities. To address this, we propose BehaveGPT, a\nfoundational model designed specifically for large-scale user behavior\nprediction. Leveraging transformer-based architecture and a novel pretraining\nparadigm, BehaveGPT is trained on vast user behavior datasets, allowing it to\nlearn complex behavior patterns and support a range of downstream tasks,\nincluding next behavior prediction, long-term generation, and cross-domain\nadaptation. Our approach introduces the DRO-based pretraining paradigm tailored\nfor user behavior data, which improves model generalization and transferability\nby equitably modeling both head and tail behaviors. Extensive experiments on\nreal-world datasets demonstrate that BehaveGPT outperforms state-of-the-art\nbaselines, achieving more than a 10% improvement in macro and weighted recall,\nshowcasing its ability to effectively capture and predict user behavior.\nFurthermore, we measure the scaling law in the user behavior domain for the\nfirst time on the Honor dataset, providing insights into how model performance\nscales with increased data and parameter sizes.\n","authors":["Jiahui Gong","Jingtao Ding","Fanjin Meng","Chen Yang","Hong Chen","Zuojian Wang","Haisheng Lu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.17631v1.pdf","comment":"22 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.17615v1","updated":"2025-05-23T08:22:09Z","published":"2025-05-23T08:22:09Z","title":"Large language model as user daily behavior data generator: balancing\n  population diversity and individual personality","summary":"  Predicting human daily behavior is challenging due to the complexity of\nroutine patterns and short-term fluctuations. While data-driven models have\nimproved behavior prediction by leveraging empirical data from various\nplatforms and devices, the reliance on sensitive, large-scale user data raises\nprivacy concerns and limits data availability. Synthetic data generation has\nemerged as a promising solution, though existing methods are often limited to\nspecific applications. In this work, we introduce BehaviorGen, a framework that\nuses large language models (LLMs) to generate high-quality synthetic behavior\ndata. By simulating user behavior based on profiles and real events,\nBehaviorGen supports data augmentation and replacement in behavior prediction\nmodels. We evaluate its performance in scenarios such as pertaining\naugmentation, fine-tuning replacement, and fine-tuning augmentation, achieving\nsignificant improvements in human mobility and smartphone usage predictions,\nwith gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen\nto enhance user behavior modeling through flexible and privacy-preserving\nsynthetic data generation.\n","authors":["Haoxin Li","Jingtao Ding","Jiahui Gong","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2505.17615v1.pdf","comment":"14 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.17549v1","updated":"2025-05-23T06:55:02Z","published":"2025-05-23T06:55:02Z","title":"EGA: A Unified End-to-End Generative Framework for Industrial\n  Advertising Systems","summary":"  Online industrial advertising system is fundamentally constrained by the\ninefficiency of multi-stage cascaded architectures, which filter out\nhigh-potential candidates early and fragment business decision logic across\nindependent modules. Although recent advances in generative recommendation\noffer end-to-end solutions, they fall short of practical advertising\nrequirements, lacking explicit modeling of bidding, creative selection,\nallocation mechanism, and payment computation that are essential for real-world\ndeployment. To overcome these limitations, we propose End-to-end Generative\nAdvertising (EGA), a first unified generative framework that seamlessly\nintegrates user interests modeling, POI and creative generation, position\nallocation, and payment optimization within a single model. EGA leverages\nhierarchical tokenization and multi-token prediction to jointly generate\ncandidate POI and creative contents, while a permutation-aware reward model and\ntoken-level bidding strategy ensure alignment with both user experiences and\nadvertiser business objectives. Meanwhile, we decouple allocation from payment\nvia a dedicated POI-level payment network with differentiable ex-post regret\nminimization, guaranteeing incentive compatibility approximately. Extensive\noffline and large-scale online experiments on real-world advertising systems\ndemonstrate its effectiveness and practical advantages over traditional\ncascading architectures, highlighting its potential as one of the industry's\npioneering end-to-end generative advertising solutions.\n","authors":["Zuowu Zheng","Ze Wang","Fan Yang","Jiangke Fan","Teng Zhang","Xingxing Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17507v1","updated":"2025-05-23T06:00:20Z","published":"2025-05-23T06:00:20Z","title":"Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph","summary":"  The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.\n","authors":["Qiaosheng Chen","Kaijia Huang","Xiao Zhou","Weiqing Luo","Yuanning Cui","Gong Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.17507v1.pdf","comment":"10 pages, 5 figures. Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.20118v2","updated":"2025-05-23T02:43:56Z","published":"2025-04-28T08:04:44Z","title":"OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese\n  Medicine Knowledge Retrieval and Diagnosis","summary":"  Traditional Chinese Medicine (TCM) represents a rich repository of ancient\nmedical knowledge that continues to play an important role in modern\nhealthcare. Due to the complexity and breadth of the TCM literature, the\nintegration of AI technologies is critical for its modernization and broader\naccessibility. However, this integration poses considerable challenges,\nincluding the interpretation of obscure classical Chinese texts and the\nmodeling of intricate semantic relationships among TCM concepts. In this paper,\nwe develop OpenTCM, an LLM-based system that combines a domain-specific TCM\nknowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).\nFirst, we extract more than 3.73 million classical Chinese characters from 68\ngynecological books in the Chinese Medical Classics Database, with the help of\nTCM and gynecology experts. Second, we construct a comprehensive\nmulti-relational knowledge graph comprising more than 48,000 entities and\n152,000 interrelationships, using customized prompts and Chinese-oriented LLMs\nsuch as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,\nwe integrate OpenTCM with this knowledge graph, enabling high-fidelity\ningredient knowledge retrieval and diagnostic question-answering without model\nfine-tuning. Experimental evaluations demonstrate that our prompt design and\nmodel selection significantly improve knowledge graph quality, achieving a\nprecision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves\nmean expert scores of 4.5 in ingredient information retrieval and 3.8 in\ndiagnostic question-answering tasks, outperforming state-of-the-art solutions\nin real-world TCM use cases.\n","authors":["Jinglin He","Yunqi Guo","Lai Kwan Lam","Waikei Leung","Lixing He","Yuanan Jiang","Chi Chiu Wang","Guoliang Xing","Hongkai Chen"],"pdf_url":"https://arxiv.org/pdf/2504.20118v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.05804v2","updated":"2025-05-23T02:35:56Z","published":"2025-04-08T08:36:18Z","title":"StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization","summary":"  The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present $\\textbf{StealthRank}$, a novel adversarial attack\nmethod that manipulates LLM-driven ranking systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within item or document descriptions that subtly yet\neffectively influence LLM ranking mechanisms. We evaluate StealthRank across\nmultiple LLMs, demonstrating its ability to covertly boost the ranking of\ntarget items while avoiding explicit manipulation traces. Our results show that\nStealthRank consistently outperforms state-of-the-art adversarial ranking\nbaselines in both effectiveness and stealth, highlighting critical\nvulnerabilities in LLM-driven ranking systems. Our code is publicly available\nat $\\href{https://github.com/Tangyiming205069/controllable-seo}{here}$.\n","authors":["Yiming Tang","Yi Fan","Chenxiao Yu","Tiankai Yang","Yue Zhao","Xiyang Hu"],"pdf_url":"https://arxiv.org/pdf/2504.05804v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2505.18150v1","updated":"2025-05-23T17:58:57Z","published":"2025-05-23T17:58:57Z","title":"Generative Distribution Embeddings","summary":"  Many real-world problems require reasoning across multiple scales, demanding\nmodels which operate not on single data points, but on entire distributions. We\nintroduce generative distribution embeddings (GDE), a framework that lifts\nautoencoders to the space of distributions. In GDEs, an encoder acts on sets of\nsamples, and the decoder is replaced by a generator which aims to match the\ninput distribution. This framework enables learning representations of\ndistributions by coupling conditional generative models with encoder networks\nwhich satisfy a criterion we call distributional invariance. We show that GDEs\nlearn predictive sufficient statistics embedded in the Wasserstein space, such\nthat latent GDE distances approximately recover the $W_2$ distance, and latent\ninterpolation approximately recovers optimal transport trajectories for\nGaussian and Gaussian mixture distributions. We systematically benchmark GDEs\nagainst existing approaches on synthetic datasets, demonstrating consistently\nstronger performance. We then apply GDEs to six key problems in computational\nbiology: learning representations of cell populations from lineage-tracing data\n(150K cells), predicting perturbation effects on single-cell transcriptomes (1M\ncells), predicting perturbation effects on cellular phenotypes (20M single-cell\nimages), modeling tissue-specific DNA methylation patterns (253M sequences),\ndesigning synthetic yeast promoters (34M sequences), and spatiotemporal\nmodeling of viral protein sequences (1M sequences).\n","authors":["Nic Fishman","Gokul Gowri","Peng Yin","Jonathan Gootenberg","Omar Abudayyeh"],"pdf_url":"https://arxiv.org/pdf/2505.18150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18148v1","updated":"2025-05-23T17:57:42Z","published":"2025-05-23T17:57:42Z","title":"Lost in the Haystack: Smaller Needles are More Difficult for LLMs to\n  Find","summary":"  Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.\n","authors":["Owen Bianchi","Mathew J. Koretsky","Maya Willey","Chelsea X. Alvarado","Tanay Nayak","Adi Asija","Nicole Kuznetsov","Mike A. Nalls","Faraz Faghri","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2505.18148v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.09729v2","updated":"2025-05-23T17:51:32Z","published":"2024-12-12T21:36:24Z","title":"Doubly Robust Conformalized Survival Analysis with Right-Censored Data","summary":"  We present a conformal inference method for constructing lower prediction\nbounds for survival times from right-censored data, extending recent approaches\ndesigned for more restrictive type-I censoring scenarios. The proposed method\nimputes unobserved censoring times using a machine learning model, and then\nanalyzes the imputed data using a survival model calibrated via weighted\nconformal inference. This approach is theoretically supported by an asymptotic\ndouble robustness property. Empirical studies on simulated and real data\ndemonstrate that our method leads to relatively informative predictive\ninferences and is especially robust in challenging settings where the survival\nmodel may be inaccurate.\n","authors":["Matteo Sesia","Vladimir Svetnik"],"pdf_url":"https://arxiv.org/pdf/2412.09729v2.pdf","comment":"Revision after ICML review"},{"id":"http://arxiv.org/abs/2505.18137v1","updated":"2025-05-23T17:47:20Z","published":"2025-05-23T17:47:20Z","title":"Boosting Open Set Recognition Performance through Modulated\n  Representation Learning","summary":"  The open set recognition (OSR) problem aims to identify test samples from\nnovel semantic classes that are not part of the training classes, a task that\nis crucial in many practical scenarios. However, existing OSR methods use a\nconstant scaling factor (the temperature) to the logits before applying a loss\nfunction, which hinders the model from exploring both ends of the spectrum in\nrepresentation learning -- from instance-level to semantic-level features. In\nthis paper, we address this problem by enabling temperature-modulated\nrepresentation learning using our novel negative cosine scheduling scheme. Our\nscheduling lets the model form a coarse decision boundary at the beginning of\ntraining by focusing on fewer neighbors, and gradually prioritizes more\nneighbors to smooth out rough edges. This gradual task switching leads to a\nricher and more generalizable representation space. While other OSR methods\nbenefit by including regularization or auxiliary negative samples, such as with\nmix-up, thereby adding a significant computational overhead, our scheme can be\nfolded into any existing OSR method with no overhead. We implement the proposed\nscheme on top of a number of baselines, using both cross-entropy and\ncontrastive loss functions as well as a few other OSR methods, and find that\nour scheme boosts both the OSR performance and the closed set performance in\nmost cases, especially on the tougher semantic shift benchmarks.\n","authors":["Amit Kumar Kundu","Vaishnavi Patil","Joseph Jaja"],"pdf_url":"https://arxiv.org/pdf/2505.18137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18135v1","updated":"2025-05-23T17:43:48Z","published":"2025-05-23T17:43:48Z","title":"Gaming Tool Preferences in Agentic LLMs","summary":"  Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.\n","authors":["Kazem Faghih","Wenxiao Wang","Yize Cheng","Siddhant Bharti","Gaurang Sriramanan","Sriram Balasubramanian","Parsa Hosseini","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2505.18135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18131v1","updated":"2025-05-23T17:41:18Z","published":"2025-05-23T17:41:18Z","title":"Leveraging KANs for Expedient Training of Multichannel MLPs via\n  Preconditioning and Geometric Refinement","summary":"  Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,\nused in a variety of modern deep learning frameworks. However, recently\nKolmogorov-Arnold Networks (KANs) have become increasingly popular due to their\nsuccess on a range of problems, particularly for scientific machine learning\ntasks. In this paper, we exploit the relationship between KANs and multichannel\nMLPs to gain structural insight into how to train MLPs faster. We demonstrate\nthe KAN basis (1) provides geometric localized support, and (2) acts as a\npreconditioned descent in the ReLU basis, overall resulting in expedited\ntraining and improved accuracy. Our results show the equivalence between\nfree-knot spline KAN architectures, and a class of MLPs that are refined\ngeometrically along the channel dimension of each weight tensor. We exploit\nthis structural equivalence to define a hierarchical refinement scheme that\ndramatically accelerates training of the multi-channel MLP architecture. We\nshow further accuracy improvements can be had by allowing the $1$D locations of\nthe spline knots to be trained simultaneously with the weights. These advances\nare demonstrated on a range of benchmark examples for regression and scientific\nmachine learning.\n","authors":["Jonas A. Actor","Graham Harper","Ben Southworth","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2505.18131v1.pdf","comment":"20 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.00556v2","updated":"2025-05-23T17:39:41Z","published":"2024-12-31T17:34:29Z","title":"Finding the Underlying Viscoelastic Constitutive Equation via Universal\n  Differential Equations and Differentiable Physics","summary":"  This research employs Universal Differential Equations (UDEs) alongside\ndifferentiable physics to model viscoelastic fluids, merging conventional\ndifferential equations, neural networks and numerical methods to reconstruct\nmissing terms in constitutive models. This study focuses on analyzing four\nviscoelastic models: Upper Convected Maxwell (UCM), Johnson-Segalman, Giesekus,\nand Exponential Phan-Thien-Tanner (ePTT), through the use of synthetic\ndatasets. The methodology was tested across different experimental conditions,\nincluding oscillatory and startup flows. While the UDE framework effectively\npredicts shear and normal stresses for most models, it demonstrates some\nlimitations when applied to the ePTT model. The findings underscore the\npotential of UDEs in fluid mechanics while identifying critical areas for\nmethodological improvement. Also, a model distillation approach was employed to\nextract simplified models from complex ones, emphasizing the versatility and\nrobustness of UDEs in rheological modeling.\n","authors":["Elias C. Rodrigues","Roney L. Thompson","Drio A. B. Oliveira","Roberto F. Ausas"],"pdf_url":"https://arxiv.org/pdf/2501.00556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18126v1","updated":"2025-05-23T17:36:13Z","published":"2025-05-23T17:36:13Z","title":"Reward Model Overoptimisation in Iterated RLHF","summary":"  Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.\n","authors":["Lorenz Wolf","Robert Kirk","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2505.18126v1.pdf","comment":"20 pages, 17 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.18125v1","updated":"2025-05-23T17:34:28Z","published":"2025-05-23T17:34:28Z","title":"TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations","summary":"  While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.\n","authors":["Alan Arazi","Eilam Shapira","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2505.18125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04863v2","updated":"2025-05-23T17:30:11Z","published":"2024-11-07T16:54:54Z","title":"OneProt: Towards Multi-Modal Protein Foundation Models","summary":"  Recent advances in Artificial Intelligence have enabled multi-modal systems\nto model and translate diverse information spaces. Extending beyond text and\nvision, we introduce OneProt, a multi-modal AI for proteins that integrates\nstructural, sequence, text, and binding site data. Using the ImageBind\nframework, OneProt aligns the latent spaces of protein modality encoders in a\nlightweight fine-tuning scheme that focuses on pairwise alignment with sequence\ndata rather than requiring full matches. This novel approach comprises a mix of\nGraph Neural Networks and transformer architectures. It demonstrates strong\nperformance in retrieval tasks and showcases the efficacy of multi-modal\nsystems in Protein Machine Learning through a broad spectrum of downstream\nbaselines, including enzyme function prediction and binding site analysis.\nFurthermore, OneProt enables the transfer of representational information from\nspecialized encoders to the sequence encoder, enhancing capabilities for\ndistinguishing evolutionarily related and unrelated sequences and exhibiting\nrepresentational properties where evolutionarily related proteins align in\nsimilar directions within the latent space. In addition, we extensively\ninvestigate modality ablations to identify the encoders that contribute most to\npredictive performance, highlighting the significance of the binding site\nencoder, which has not been used in similar models previously. This work\nexpands the horizons of multi-modal protein models, paving the way for\ntransformative applications in drug discovery, biocatalytic reaction planning,\nand protein engineering.\n","authors":["Klemens Flge","Srisruthi Udayakumar","Johanna Sommer","Marie Piraud","Stefan Kesselheim","Vincent Fortuin","Stephan Gnneman","Karel J van der Weg","Holger Gohlke","Erinc Merdivan","Alina Bazarova"],"pdf_url":"https://arxiv.org/pdf/2411.04863v2.pdf","comment":"34 pages, 7 figures, 11 tables"},{"id":"http://arxiv.org/abs/2505.18121v1","updated":"2025-05-23T17:23:11Z","published":"2025-05-23T17:23:11Z","title":"ProgRM: Build Better GUI Agents with Progress Rewards","summary":"  LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.\n","authors":["Danyang Zhang","Situo Zhang","Ziyue Yang","Zichen Zhu","Zihan Zhao","Ruisheng Cao","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2505.18121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05300v2","updated":"2025-05-23T17:22:54Z","published":"2025-02-07T20:10:05Z","title":"Parameter Symmetry Potentially Unifies Deep Learning Theory","summary":"  The dynamics of learning in modern large AI systems is hierarchical, often\ncharacterized by abrupt, qualitative shifts akin to phase transitions observed\nin physical systems. While these phenomena hold promise for uncovering the\nmechanisms behind neural networks and language models, existing theories remain\nfragmented, addressing specific cases. In this position paper, we advocate for\nthe crucial role of the research direction of parameter symmetries in unifying\nthese fragmented theories. This position is founded on a centralizing\nhypothesis for this direction: parameter symmetry breaking and restoration are\nthe unifying mechanisms underlying the hierarchical learning behavior of AI\nmodels. We synthesize prior observations and theories to argue that this\ndirection of research could lead to a unified understanding of three distinct\nhierarchies in neural networks: learning dynamics, model complexity, and\nrepresentation formation. By connecting these hierarchies, our position paper\nelevates symmetry -- a cornerstone of theoretical physics -- to become a\npotential fundamental principle in modern AI.\n","authors":["Liu Ziyin","Yizhou Xu","Tomaso Poggio","Isaac Chuang"],"pdf_url":"https://arxiv.org/pdf/2502.05300v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2505.18118v1","updated":"2025-05-23T17:19:12Z","published":"2025-05-23T17:19:12Z","title":"Scalable Policy Maximization Under Network Interference","summary":"  Many interventions, such as vaccines in clinical trials or coupons in online\nmarketplaces, must be assigned sequentially without full knowledge of their\neffects. Multi-armed bandit algorithms have proven successful in such settings.\nHowever, standard independence assumptions fail when the treatment status of\none individual impacts the outcomes of others, a phenomenon known as\ninterference. We study optimal-policy learning under interference on a dynamic\nnetwork. Existing approaches to this problem require repeated observations of\nthe same fixed network and struggle to scale in sample size beyond as few as\nfifteen connected units -- both limit applications. We show that under common\nassumptions on the structure of interference, rewards become linear. This\nenables us to develop a scalable Thompson sampling algorithm that maximizes\npolicy impact when a new $n$-node network is observed each round. We prove a\nBayesian regret bound that is sublinear in $n$ and the number of rounds.\nSimulation experiments show that our algorithm learns quickly and outperforms\nexisting methods. The results close a key scalability gap between causal\ninference methods for interference and practical bandit algorithms, enabling\npolicy optimization in large-scale networked systems.\n","authors":["Aidan Gleich","Eric Laber","Alexander Volfovsky"],"pdf_url":"https://arxiv.org/pdf/2505.18118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18116v1","updated":"2025-05-23T17:17:40Z","published":"2025-05-23T17:17:40Z","title":"Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning","summary":"  Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.\n","authors":["Huayu Chen","Kaiwen Zheng","Qinsheng Zhang","Ganqu Cui","Yin Cui","Haotian Ye","Tsung-Yi Lin","Ming-Yu Liu","Jun Zhu","Haoxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.18116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18113v1","updated":"2025-05-23T17:11:22Z","published":"2025-05-23T17:11:22Z","title":"Beyond Discreteness: Finite-Sample Analysis of Straight-Through\n  Estimator for Quantization","summary":"  Training quantized neural networks requires addressing the non-differentiable\nand discrete nature of the underlying optimization problem. To tackle this\nchallenge, the straight-through estimator (STE) has become the most widely\nadopted heuristic, allowing backpropagation through discrete operations by\nintroducing surrogate gradients. However, its theoretical properties remain\nlargely unexplored, with few existing works simplifying the analysis by\nassuming an infinite amount of training data. In contrast, this work presents\nthe first finite-sample analysis of STE in the context of neural network\nquantization. Our theoretical results highlight the critical role of sample\nsize in the success of STE, a key insight absent from existing studies.\nSpecifically, by analyzing the quantization-aware training of a two-layer\nneural network with binary weights and activations, we derive the sample\ncomplexity bound in terms of the data dimensionality that guarantees the\nconvergence of STE-based optimization to the global minimum. Moreover, in the\npresence of label noises, we uncover an intriguing recurrence property of\nSTE-gradient method, where the iterate repeatedly escape from and return to the\noptimal binary weights. Our analysis leverages tools from compressed sensing\nand dynamical systems theory.\n","authors":["Halyun Jeong","Jack Xin","Penghang Yin"],"pdf_url":"https://arxiv.org/pdf/2505.18113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18106v1","updated":"2025-05-23T17:02:22Z","published":"2025-05-23T17:02:22Z","title":"F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial\n  Architecture for Synthetic Image Generation of Nanoparticles","summary":"  Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields.\n","authors":["Varun Ajith","Anindya Pal","Saumik Bhattacharya","Sayantari Ghosh"],"pdf_url":"https://arxiv.org/pdf/2505.18106v1.pdf","comment":"11 pages, 9 figures, 2 tables, conference paper"},{"id":"http://arxiv.org/abs/2504.12397v3","updated":"2025-05-23T17:02:05Z","published":"2025-04-16T18:03:21Z","title":"Activated LoRA: Fine-tuned LLMs for Intrinsics","summary":"  Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. We include a codebase implementing aLoRA in the supplementary\nmaterial.\n","authors":["Kristjan Greenewald","Luis Lastras","Thomas Parnell","Vraj Shah","Lucian Popa","Giulio Zizzo","Chulaka Gunasekara","Ambrish Rawat","David Cox"],"pdf_url":"https://arxiv.org/pdf/2504.12397v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07368v4","updated":"2025-05-23T16:59:15Z","published":"2024-07-10T05:03:48Z","title":"Semi-Supervised Model-Free Bayesian State Estimation from Compressed\n  Measurements","summary":"  We consider data-driven Bayesian state estimation from compressed\nmeasurements (BSCM) of a model-free process. The dimension of the temporal\nmeasurement vector is lower than that of the temporal state vector to be\nestimated, leading to an under-determined inverse problem. The underlying\ndynamical model of the state's evolution is unknown for a 'model-free process.'\nHence, it is difficult to use traditional model-driven methods, for example,\nKalman and particle filters. Instead, we consider data-driven methods. We\nexperimentally show that two existing unsupervised learning-based data-driven\nmethods fail to address the BSCM problem in a model-free process. The methods\nare -- data-driven nonlinear state estimation (DANSE) and deep Markov model\n(DMM). While DANSE provides good predictive/forecasting performance to model\nthe temporal measurement data as a time series, its unsupervised learning lacks\nsuitable regularization for tackling the BSCM task. We then propose a\nsemi-supervised learning approach and develop a semi-supervised learning-based\nDANSE method, referred to as SemiDANSE. In SemiDANSE, we use a large amount of\nunlabelled data along with a limited amount of labelled data, i.e., pairwise\nmeasurement-and-state data, which provides the desired regularization. Using\nthree benchmark dynamical systems, we empirically show that the data-driven\nSemiDANSE provides competitive state estimation performance for BSCM using a\nhandful of different measurement systems, against a hybrid method called\nKalmanNet and two model-driven methods (extended Kalman filter and unscented\nKalman filter) that know the dynamical models exactly.\n","authors":["Anubhab Ghosh","Yonina C. Eldar","Saikat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2407.07368v4.pdf","comment":"14 pages, 12 figures, under review at IEEE Transactions on Signal\n  Processing"},{"id":"http://arxiv.org/abs/2505.18102v1","updated":"2025-05-23T16:57:34Z","published":"2025-05-23T16:57:34Z","title":"How Can I Publish My LLM Benchmark Without Giving the True Answers Away?","summary":"  Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.\n","authors":["Takashi Ishida","Thanawat Lodkaew","Ikko Yamane"],"pdf_url":"https://arxiv.org/pdf/2505.18102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18101v1","updated":"2025-05-23T16:57:04Z","published":"2025-05-23T16:57:04Z","title":"Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online\n  Continual Learning","summary":"  Online Continual Learning (OCL) presents a complex learning environment in\nwhich new data arrives in a batch-to-batch online format, and the risk of\ncatastrophic forgetting can significantly impair model efficacy. In this study,\nwe address OCL by introducing an innovative memory framework that incorporates\na short-term memory system to retain dynamic information and a long-term memory\nsystem to archive enduring knowledge. Specifically, the long-term memory system\ncomprises a collection of sub-memory buffers, each linked to a cluster\nprototype and designed to retain data samples from distinct categories. We\npropose a novel $K$-means-based sample selection method to identify cluster\nprototypes for each encountered category. To safeguard essential and critical\nsamples, we introduce a novel memory optimisation strategy that selectively\nretains samples in the appropriate sub-memory buffer by evaluating each cluster\nprototype against incoming samples through an optimal transportation mechanism.\nThis approach specifically promotes each sub-memory buffer to retain data\nsamples that exhibit significant discrepancies from the corresponding cluster\nprototype, thereby ensuring the preservation of semantically rich information.\nIn addition, we propose a novel Divide-and-Conquer (DAC) approach that\nformulates the memory updating as an optimisation problem and divides it into\nseveral subproblems. As a result, the proposed DAC approach can solve these\nsubproblems separately and thus can significantly reduce computations of the\nproposed memory updating process. We conduct a series of experiments across\nstandard and imbalanced learning settings, and the empirical findings indicate\nthat the proposed memory framework achieves state-of-the-art performance in\nboth learning contexts.\n","authors":["Congren Dai","Huichi Zhou","Jiahao Huang","Zhenxuan Zhang","Fanwen Wang","Guang Yang","Fei Ye"],"pdf_url":"https://arxiv.org/pdf/2505.18101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18097v1","updated":"2025-05-23T16:49:20Z","published":"2025-05-23T16:49:20Z","title":"Towards more transferable adversarial attack in black-box manner","summary":"  Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.\n","authors":["Chun Tong Lei","Zhongliang Guo","Hon Chung Lee","Minh Quoc Duong","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2505.18097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18091v1","updated":"2025-05-23T16:46:24Z","published":"2025-05-23T16:46:24Z","title":"Data Mixing Can Induce Phase Transitions in Knowledge Acquisition","summary":"  Large Language Models (LLMs) are typically trained on data mixtures: most\ndata come from web scrapes, while a small portion is curated from high-quality\nsources with dense domain-specific knowledge. In this paper, we show that when\ntraining LLMs on such data mixtures, knowledge acquisition from knowledge-dense\ndatasets, unlike training exclusively on knowledge-dense data\n(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit\nphase transitions with respect to the mixing ratio and model size. Through\ncontrolled experiments on a synthetic biography dataset mixed with web-scraped\ndata, we demonstrate that: (1) as we increase the model size to a critical\nvalue, the model suddenly transitions from memorizing very few to most of the\nbiographies; (2) below a critical mixing ratio, the model memorizes almost\nnothing even with extensive training, but beyond this threshold, it rapidly\nmemorizes more biographies. We attribute these phase transitions to a capacity\nallocation phenomenon: a model with bounded capacity must act like a knapsack\nproblem solver to minimize the overall test loss, and the optimal allocation\nacross datasets can change discontinuously as the model size or mixing ratio\nvaries. We formalize this intuition in an information-theoretic framework and\nreveal that these phase transitions are predictable, with the critical mixing\nratio following a power-law relationship with the model size. Our findings\nhighlight a concrete case where a good mixing recipe for large models may not\nbe optimal for small models, and vice versa.\n","authors":["Xinran Gu","Kaifeng Lyu","Jiazheng Li","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.18091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16587v3","updated":"2025-05-23T16:45:54Z","published":"2025-01-27T23:59:23Z","title":"HopCast: Calibration of Autoregressive Dynamics Models","summary":"  Deep learning models are often trained to approximate dynamical systems that\ncan be modeled using differential equations. Many of these models are optimized\nto predict one step ahead; such approaches produce calibrated one-step\npredictions if the predictive model can quantify uncertainty, such as Deep\nEnsembles. At inference time, multi-step predictions are generated via\nautoregression, which needs a sound uncertainty propagation method to produce\ncalibrated multi-step predictions. This work introduces an alternative\nPredictor-Corrector approach named \\hop{} that uses Modern Hopfield Networks\n(MHN) to learn the errors of a deterministic Predictor that approximates the\ndynamical system. The Corrector predicts a set of errors for the Predictor's\noutput based on a context state at any timestep during autoregression. The set\nof errors creates sharper and well-calibrated prediction intervals with higher\npredictive accuracy compared to baselines without uncertainty propagation. The\ncalibration and prediction performances are evaluated across a set of dynamical\nsystems. This work is also the first to benchmark existing uncertainty\npropagation methods based on calibration errors.\n","authors":["Muhammad Bilal Shahid","Cody Fleming"],"pdf_url":"https://arxiv.org/pdf/2501.16587v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18088v1","updated":"2025-05-23T16:45:14Z","published":"2025-05-23T16:45:14Z","title":"Early-Exit Graph Neural Networks","summary":"  Early-exit mechanisms allow deep neural networks to halt inference as soon as\nclassification confidence is high enough, adaptively trading depth for\nconfidence, and thereby cutting latency and energy on easy inputs while\nretaining full-depth accuracy for harder ones. Similarly, adding early exit\nmechanisms to Graph Neural Networks (GNNs), the go-to models for\ngraph-structured data, allows for dynamic trading depth for confidence on\nsimple graphs while maintaining full-depth accuracy on harder and more complex\ngraphs to capture intricate relationships. Although early exits have proven\neffective across various deep learning domains, their potential within GNNs in\nscenarios that require deep architectures while resisting over-smoothing and\nover-squashing remains largely unexplored. We unlock that potential by first\nintroducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose\nsymmetry-based inductive biases mitigate these issues and yield stable\nintermediate representations that can be useful to allow early exiting in GNNs.\nBuilding on this backbone, we present Early-Exit Graph Neural Networks\n(EEGNNs), which append confidence-aware exit heads that allow on-the-fly\ntermination of propagation based on each node or the entire graph. Experiments\nshow that EEGNNs preserve robust performance as depth grows and deliver\ncompetitive accuracy on heterophilic and long-range benchmarks, matching\nattention-based and asynchronous message-passing models while substantially\nreducing computation and latency. We plan to release the code to reproduce our\nexperiments.\n","authors":["Andrea Giuseppe Di Francesco","Maria Sofia Bucarelli","Franco Maria Nardini","Raffaele Perego","Nicola Tonellotto","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2505.18088v1.pdf","comment":"37 pages, 14 figures"},{"id":"http://arxiv.org/abs/2505.18086v1","updated":"2025-05-23T16:43:03Z","published":"2025-05-23T16:43:03Z","title":"Stable Reinforcement Learning for Efficient Reasoning","summary":"  The success of Deepseek-R1 has drawn the LLM community's attention to\nreinforcement learning (RL) methods like GRPO. However, such rule-based 0/1\noutcome reward methods lack the capability to regulate the intermediate\nreasoning processes during chain-of-thought (CoT) generation, leading to severe\noverthinking phenomena. In response, recent studies have designed reward\nfunctions to reinforce models' behaviors in producing shorter yet correct\ncompletions. Nevertheless, we observe that these length-penalty reward\nfunctions exacerbate RL training instability: as the completion length\ndecreases, model accuracy abruptly collapses, often occurring early in\ntraining. To address this issue, we propose a simple yet effective solution\nGRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically\nadjusts the reward strategy by monitoring the correctness ratio among\ncompletions within each query-sampled group. A low correctness ratio indicates\nthe need to avoid length penalty that compromises CoT quality, triggering a\nswitch to length-agnostic 0/1 rewards that prioritize reasoning capability. A\nhigh ratio maintains length penalties to boost efficiency. Experimental results\nshow that our approach avoids training instability caused by length penalty\nwhile maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,\nGPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average\naccuracy by 1.48% while reducing CoT sequence length by 47.3%.\n","authors":["Muzhi Dai","Shixuan Liu","Qingyi Si"],"pdf_url":"https://arxiv.org/pdf/2505.18086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18083v1","updated":"2025-05-23T16:41:08Z","published":"2025-05-23T16:41:08Z","title":"What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?","summary":"  In planning, stitching is an ability of algorithms to piece together\nsub-trajectories of data they are trained on to generate new and diverse\nbehaviours. While stitching is historically a strength of offline reinforcement\nlearning, recent generative behavioural cloning (BC) methods have also shown\nproficiency at stitching. However, the main factors behind this are poorly\nunderstood, hindering the development of new algorithms that can reliably\nstitch. Focusing on diffusion planners trained via BC, we find two properties\nare needed to compose: \\emph{positional equivariance} and \\emph{local\nreceptiveness}. We use these two properties to explain architecture, data, and\ninference choices in existing generative BC methods based on diffusion\nplanning, including replanning frequency, data augmentation, and data scaling.\nExperimental comparisions show that (1) while locality is more important than\npositional equivariance in creating a diffusion planner capable of composition,\nboth are crucial (2) enabling these properties through relatively simple\narchitecture choices can be competitive with more computationally expensive\nmethods such as replanning or scaling data, and (3) simple inpainting-based\nguidance can guide architecturally compositional models to enable\ngeneralization in goal-conditioned settings.\n","authors":["Quentin Clark","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2505.18083v1.pdf","comment":"9 Pages"},{"id":"http://arxiv.org/abs/2505.18082v1","updated":"2025-05-23T16:40:25Z","published":"2025-05-23T16:40:25Z","title":"An Iterative Framework for Generative Backmapping of Coarse Grained\n  Proteins","summary":"  The techniques of data-driven backmapping from coarse-grained (CG) to\nfine-grained (FG) representation often struggle with accuracy, unstable\ntraining, and physical realism, especially when applied to complex systems such\nas proteins. In this work, we introduce a novel iterative framework by using\nconditional Variational Autoencoders and graph-based neural networks,\nspecifically designed to tackle the challenges associated with such large-scale\nbiomolecules. Our method enables stepwise refinement from CG beads to full\natomistic details. We outline the theory of iterative generative backmapping\nand demonstrate via numerical experiments the advantages of multistep schemes\nby applying them to proteins of vastly different structures with very coarse\nrepresentations. This multistep approach not only improves the accuracy of\nreconstructions but also makes the training process more computationally\nefficient for proteins with ultra-CG representations.\n","authors":["Georgios Kementzidis","Erin Wong","John Nicholson","Ruichen Xu","Yuefan Deng"],"pdf_url":"https://arxiv.org/pdf/2505.18082v1.pdf","comment":"17 pages, 8 figures. For associated code repositories, see: CGVAE:\n  https://github.com/wwang2/CoarseGrainingVAE GenZProT:\n  https://github.com/learningmatter-mit/GenZProt See also arXiv:2201.12176 and\n  arXiv:2303.01569 for related methods"},{"id":"http://arxiv.org/abs/2505.18081v1","updated":"2025-05-23T16:39:21Z","published":"2025-05-23T16:39:21Z","title":"Backpropagation-Free Metropolis-Adjusted Langevin Algorithm","summary":"  Recent work on backpropagation-free learning has shown that it is possible to\nuse forward-mode automatic differentiation (AD) to perform optimization on\ndifferentiable models. Forward-mode AD requires sampling a tangent vector for\neach forward pass of a model. The result is the model evaluation with the\ndirectional derivative along the tangent. In this paper, we illustrate how the\nsampling of this tangent vector can be incorporated into the proposal mechanism\nfor the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the\nfirst to introduce a backpropagation-free gradient-based Markov chain Monte\nCarlo (MCMC) algorithm. We also extend to a novel backpropagation-free\nposition-specific preconditioned forward-mode MALA that leverages Hessian\ninformation. Overall, we propose four new algorithms: Forward MALA; Line\nForward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward\nMALA. We highlight the reduced computational cost of the forward-mode samplers\nand show that forward-mode is competitive with the original MALA, while even\noutperforming it depending on the probabilistic model. We include Bayesian\ninference results on a range of probabilistic models, including hierarchical\ndistributions and Bayesian neural networks.\n","authors":["Adam D. Cobb","Susmit Jha"],"pdf_url":"https://arxiv.org/pdf/2505.18081v1.pdf","comment":"19 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2505.18080v1","updated":"2025-05-23T16:39:07Z","published":"2025-05-23T16:39:07Z","title":"AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for\n  Chaotic System Prediction","summary":"  This paper presents AFD-STA Net, a neural framework integrating adaptive\nfiltering and spatiotemporal dynamics learning for predicting high-dimensional\nchaotic systems governed by partial differential equations. The architecture\ncombines: 1) An adaptive exponential smoothing module with position-aware decay\ncoefficients for robust attractor reconstruction, 2) Parallel attention\nmechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated\nfusion of multiscale features, and 4) Deep projection networks with\ndimension-scaling capabilities. Numerical experiments on nonlinear PDE systems\ndemonstrate the model's effectiveness in maintaining prediction accuracy under\nboth smooth and strongly chaotic regimes while exhibiting noise tolerance\nthrough adaptive filtering. Component ablation studies confirm critical\ncontributions from each module, particularly highlighting the essential role of\nspatiotemporal attention in learning complex dynamical interactions. The\nframework shows promising potential for real-world applications requiring\nsimultaneous handling of measurement uncertainties and high-dimensional\nnonlinear dynamics.\n","authors":["Chunlin Gong","Yin Wang","Jingru Li","Hanleran Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.18080v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.06261v3","updated":"2025-05-23T16:36:12Z","published":"2025-04-08T17:59:41Z","title":"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention","summary":"  Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.\n","authors":["Gleb Rodionov","Roman Garipov","Alina Shutova","George Yakushev","Erik Schultheis","Vage Egiazarian","Anton Sinitsin","Denis Kuznedelev","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2504.06261v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.18077v1","updated":"2025-05-23T16:33:47Z","published":"2025-05-23T16:33:47Z","title":"Bayesian Deep Learning for Discrete Choice","summary":"  Discrete choice models (DCMs) are used to analyze individual decision-making\nin contexts such as transportation choices, political elections, and consumer\npreferences. DCMs play a central role in applied econometrics by enabling\ninference on key economic variables, such as marginal rates of substitution,\nrather than focusing solely on predicting choices on new unlabeled data.\nHowever, while traditional DCMs offer high interpretability and support for\npoint and interval estimation of economic quantities, these models often\nunderperform in predictive tasks compared to deep learning (DL) models. Despite\ntheir predictive advantages, DL models remain largely underutilized in discrete\nchoice due to concerns about their lack of interpretability, unstable parameter\nestimates, and the absence of established methods for uncertainty\nquantification. Here, we introduce a deep learning model architecture\nspecifically designed to integrate with approximate Bayesian inference methods,\nsuch as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model\ncollapses to behaviorally informed hypotheses when data is limited, mitigating\noverfitting and instability in underspecified settings while retaining the\nflexibility to capture complex nonlinear relationships when sufficient data is\navailable. We demonstrate our approach using SGLD through a Monte Carlo\nsimulation study, evaluating both predictive metrics--such as out-of-sample\nbalanced accuracy--and inferential metrics--such as empirical coverage for\nmarginal rates of substitution interval estimates. Additionally, we present\nresults from two empirical case studies: one using revealed mode choice data in\nNYC, and the other based on the widely used Swiss train choice stated\npreference data.\n","authors":["Daniel F. Villarraga","Ricardo A. Daziano"],"pdf_url":"https://arxiv.org/pdf/2505.18077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11704v2","updated":"2025-05-23T16:32:54Z","published":"2024-09-18T05:13:18Z","title":"From Lists to Emojis: How Format Bias Affects Model Alignment","summary":"  In this paper, we study format biases in reinforcement learning from human\nfeedback (RLHF). We observe that many widely-used preference models, including\nhuman evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,\nexhibit strong biases towards specific format patterns, such as lists, links,\nbold text, and emojis. Furthermore, large language models (LLMs) can exploit\nthese biases to achieve higher rankings on popular benchmarks like AlpacaEval\nand LMSYS Chatbot Arena. One notable example of this is verbosity bias, where\ncurrent preference models favor longer responses that appear more\ncomprehensive, even when their quality is equal to or lower than shorter,\ncompeting responses. However, format biases beyond verbosity remain largely\nunderexplored in the literature. In this work, we extend the study of biases in\npreference learning beyond the commonly recognized length bias, offering a\ncomprehensive analysis of a wider range of format biases. Additionally, we show\nthat with a small amount of biased data (less than 1%), we can inject\nsignificant bias into the reward model. Moreover, these format biases can also\nbe easily exploited by downstream alignment algorithms, such as best-of-n\nsampling and online iterative DPO, as it is usually easier to manipulate the\nformat than to improve the quality of responses. Our findings emphasize the\nneed to disentangle format and content both for designing alignment algorithms\nand evaluating models.\n","authors":["Xuanchang Zhang","Wei Xiong","Lichang Chen","Tianyi Zhou","Heng Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.11704v2.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2409.11138v2","updated":"2025-05-23T16:20:36Z","published":"2024-09-17T12:45:49Z","title":"Learning Generalized Hamiltonians using fully Symplectic Mappings","summary":"  Many important physical systems can be described as the evolution of a\nHamiltonian system, which has the important property of being conservative,\nthat is, energy is conserved throughout the evolution. Physics Informed Neural\nNetworks and in particular Hamiltonian Neural Networks have emerged as a\nmechanism to incorporate structural inductive bias into the NN model. By\nensuring physical invariances are conserved, the models exhibit significantly\nbetter sample complexity and out-of-distribution accuracy than standard NNs.\nLearning the Hamiltonian as a function of its canonical variables, typically\nposition and velocity, from sample observations of the system thus becomes a\ncritical task in system identification and long-term prediction of system\nbehavior. However, to truly preserve the long-run physical conservation\nproperties of Hamiltonian systems, one must use symplectic integrators for a\nforward pass of the system's simulation. While symplectic schemes have been\nused in the literature, they are thus far limited to situations when they\nreduce to explicit algorithms, which include the case of separable Hamiltonians\nor augmented non-separable Hamiltonians. We extend it to generalized\nnon-separable Hamiltonians, and noting the self-adjoint property of symplectic\nintegrators, we bypass computationally intensive backpropagation through an ODE\nsolver. We show that the method is robust to noise and provides a good\napproximation of the system Hamiltonian when the state variables are sampled\nfrom a noisy observation. In the numerical results, we show the performance of\nthe method concerning Hamiltonian reconstruction and conservation, indicating\nits particular advantage for non-separable systems.\n","authors":["Harsh Choudhary","Chandan Gupta","Vyacheslav kungrutsev","Melvin Leok","Georgios Korpas"],"pdf_url":"https://arxiv.org/pdf/2409.11138v2.pdf","comment":"Submitted to The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2505.18069v1","updated":"2025-05-23T16:16:09Z","published":"2025-05-23T16:16:09Z","title":"Emergence of Hebbian Dynamics in Regularized Non-Local Learners","summary":"  Stochastic Gradient Descent (SGD) has emerged as a remarkably effective\nlearning algorithm, underpinning nearly all state-of-the-art machine learning\nmodels, from large language models to autonomous vehicles. Despite its\npractical success, SGD appears fundamentally distinct from biological learning\nmechanisms. It is widely believed that the biological brain can not implement\ngradient descent because it is nonlocal, and we have found little (if any)\nexperimental evidence for it. In contrast, the brain is widely thought to learn\nvia local Hebbian learning principles, which have been seen as incompatible\nwith gradient descent. In this paper, we establish a theoretical and empirical\nconnection between the learning signals of neural networks trained using SGD\nwith weight decay and those trained with Hebbian learning near convergence. We\nshow that SGD with regularization can appear to learn according to a Hebbian\nrule, and SGD with injected noise according to an anti-Hebbian rule. We also\nprovide empirical evidence that Hebbian learning properties can emerge in a\nnetwork with weight decay from virtually any learning rule--even random ones.\nThese results may bridge a long-standing gap between artificial and biological\nlearning, revealing Hebbian properties as an epiphenomenon of deeper\noptimization principles and cautioning against interpreting their presence in\nneural data as evidence against more complex hetero-synaptic mechanisms.\n","authors":["David Koplow","Tomaso Poggio","Liu Ziyin"],"pdf_url":"https://arxiv.org/pdf/2505.18069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19715v3","updated":"2025-05-23T16:14:40Z","published":"2024-11-29T14:02:11Z","title":"Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery\n  Detection","summary":"  We describe Forensics Adapter, an adapter network designed to transform CLIP\ninto an effective and generalizable face forgery detector. Although CLIP is\nhighly versatile, adapting it for face forgery detection is non-trivial as\nforgery-related knowledge is entangled with a wide range of unrelated\nknowledge. Existing methods treat CLIP merely as a feature extractor, lacking\ntask-specific adaptation, which limits their effectiveness. To address this, we\nintroduce an adapter to learn face forgery traces -- the blending boundaries\nunique to forged faces, guided by task-specific objectives. Then we enhance the\nCLIP visual tokens with a dedicated interaction strategy that communicates\nknowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its\nversatility is highly retained, naturally ensuring strong generalizability in\nface forgery detection. With only 5.7M trainable parameters, our method\nachieves a significant performance boost, improving by approximately 7% on\naverage across five standard datasets. Additionally, we describe Forensics\nAdapter++, an extended method that incorporates textual modality via a newly\nproposed forgery-aware prompt learning strategy. This extension leads to a\nfurther 1.3% performance boost over the original Forensics Adapter. We believe\nthe proposed methods can serve as a baseline for future CLIP-based face forgery\ndetection methods. The codes have been released at\nhttps://github.com/OUC-VAS/ForensicsAdapter.\n","authors":["Xinjie Cui","Yuezun Li","Delong Zhu","Jiaran Zhou","Junyu Dong","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2411.19715v3.pdf","comment":"Extension of CVPR 2025"},{"id":"http://arxiv.org/abs/2505.18066v1","updated":"2025-05-23T16:12:39Z","published":"2025-05-23T16:12:39Z","title":"Towards Uncertainty Aware Task Delegation and Human-AI Collaborative\n  Decision-Making","summary":"  Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.\n","authors":["Min Hun Lee","Martyn Zhe Yu Tok"],"pdf_url":"https://arxiv.org/pdf/2505.18066v1.pdf","comment":"ACM FAccT 2025"},{"id":"http://arxiv.org/abs/2505.18065v1","updated":"2025-05-23T16:12:12Z","published":"2025-05-23T16:12:12Z","title":"Reward Model Generalization for Compute-Aware Test-Time Reasoning","summary":"  External test-time reasoning enhances large language models (LLMs) by\ndecoupling generation and selection. At inference time, the model generates\nmultiple reasoning paths, and an auxiliary process reward model (PRM) is used\nto score and select the best one. A central challenge in this setting is\ntest-time compute optimality (TCO), i.e., how to maximize answer accuracy under\na fixed inference budget. In this work, we establish a theoretical framework to\nanalyze how the generalization error of the PRM affects compute efficiency and\nreasoning performance. Leveraging PAC-Bayes theory, we derive generalization\nbounds and show that a lower generalization error of PRM leads to fewer samples\nrequired to find correct answers. Motivated by this analysis, we propose\nCompute-Aware Tree Search (CATS), an actor-critic framework that dynamically\ncontrols search behavior. The actor outputs sampling hyperparameters based on\nreward distributions and sparsity statistics, while the critic estimates their\nutility to guide budget allocation. Experiments on the MATH and AIME benchmarks\nwith various LLMs and PRMs demonstrate that CATS consistently outperforms other\nexternal TTS methods, validating our theoretical predictions.\n","authors":["Zeen Song","Wenwen Qiang","Siyu Zhao","Changwen Zheng","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2505.18065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18064v1","updated":"2025-05-23T16:11:39Z","published":"2025-05-23T16:11:39Z","title":"Asymptotically optimal regret in communicating Markov decision processes","summary":"  In this paper, we present a learning algorithm that achieves asymptotically\noptimal regret for Markov decision processes in average reward under a\ncommunicating assumption. That is, given a communicating Markov decision\nprocess $M$, our algorithm has regret $K(M) \\log(T) + \\mathrm{o}(\\log(T))$\nwhere $T$ is the number of learning steps and $K(M)$ is the best possible\nconstant. This algorithm works by explicitly tracking the constant $K(M)$ to\nlearn optimally, then balances the trade-off between exploration (playing\nsub-optimally to gain information), co-exploration (playing optimally to gain\ninformation) and exploitation (playing optimally to score maximally). We\nfurther show that the function $K(M)$ is discontinuous, which is a consequence\nchallenge for our approach. To that end, we describe a regularization mechanism\nto estimate $K(M)$ with arbitrary precision from empirical data.\n","authors":["Victor Boone"],"pdf_url":"https://arxiv.org/pdf/2505.18064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18511v2","updated":"2025-05-23T16:07:21Z","published":"2025-01-30T17:21:44Z","title":"WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training","summary":"  Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.\n","authors":["Benjamin Feuer","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2501.18511v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2504.13241v3","updated":"2025-05-23T15:52:56Z","published":"2025-04-17T17:39:35Z","title":"Recursive Deep Inverse Reinforcement Learning","summary":"  Inferring an adversary's goals from exhibited behavior is crucial for\ncounterplanning and non-cooperative multi-agent systems in domains like\ncybersecurity, military, and strategy games. Deep Inverse Reinforcement\nLearning (IRL) methods based on maximum entropy principles show promise in\nrecovering adversaries' goals but are typically offline, require large batch\nsizes with gradient descent, and rely on first-order updates, limiting their\napplicability in real-time scenarios. We propose an online Recursive Deep\nInverse Reinforcement Learning (RDIRL) approach to recover the cost function\ngoverning the adversary actions and goals. Specifically, we minimize an upper\nbound on the standard Guided Cost Learning (GCL) objective using sequential\nsecond-order Newton updates, akin to the Extended Kalman Filter (EKF), leading\nto a fast (in terms of convergence) learning algorithm. We demonstrate that\nRDIRL is able to recover cost and reward functions of expert agents in standard\nand adversarial benchmark tasks. Experiments on benchmark tasks show that our\nproposed approach outperforms several leading IRL algorithms.\n","authors":["Paul Ghanem","Michael Potter","Owen Howell","Pau Closas","Alireza Ramezani","Deniz Erdogmus","Tales Imbiriba"],"pdf_url":"https://arxiv.org/pdf/2504.13241v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18046v1","updated":"2025-05-23T15:51:46Z","published":"2025-05-23T15:51:46Z","title":"Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD\n  in High Dimensions","summary":"  The Restricted Boltzmann Machine (RBM) is one of the simplest generative\nneural networks capable of learning input distributions. Despite its\nsimplicity, the analysis of its performance in learning from the training data\nis only well understood in cases that essentially reduce to singular value\ndecomposition of the data. Here, we consider the limit of a large dimension of\nthe input space and a constant number of hidden units. In this limit, we\nsimplify the standard RBM training objective into a form that is equivalent to\nthe multi-index model with non-separable regularization. This opens a path to\nanalyze training of the RBM using methods that are established for multi-index\nmodels, such as Approximate Message Passing (AMP) and its state evolution, and\nthe analysis of Gradient Descent (GD) via the dynamical mean-field theory. We\nthen give rigorous asymptotics of the training dynamics of RBM on data\ngenerated by the spiked covariance model as a prototype of a structure suitable\nfor unsupervised learning. We show in particular that RBM reaches the optimal\ncomputational weak recovery threshold, aligning with the BBP transition, in the\nspiked covariance model.\n","authors":["Yizhou Xu","Florent Krzakala","Lenka Zdeborov"],"pdf_url":"https://arxiv.org/pdf/2505.18046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18044v1","updated":"2025-05-23T15:48:11Z","published":"2025-05-23T15:48:11Z","title":"Linear Mixture Distributionally Robust Markov Decision Processes","summary":"  Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.\n","authors":["Zhishuai Liu","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2505.18044v1.pdf","comment":"26 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.18043v1","updated":"2025-05-23T15:46:16Z","published":"2025-05-23T15:46:16Z","title":"Improved Algorithms for Overlapping and Robust Clustering of\n  Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach","summary":"  Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature.\n","authors":["Changyeol Lee","Yongho Shin","Hyung-Chan An"],"pdf_url":"https://arxiv.org/pdf/2505.18043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04556v3","updated":"2025-05-23T15:43:17Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in AI.\nMeasuring the extent of these behaviors requires principled evaluation methods.\nWe explore a unified perspective that considers both behaviors simultaneously,\ntermed compositional causal reasoning (CCR): the ability to infer how causal\nmeasures compose and, equivalently, how causal quantities propagate through\ngraphs. We instantiate a framework for the systematic evaluation of CCR for the\naverage treatment effect and the probability of necessity and sufficiency. As\nproof of concept, we demonstrate CCR evaluation for language models in the\nLLama, Phi, and GPT families. On a math word problem, our framework revealed a\nrange of taxonomically distinct error patterns. CCR errors increased with the\ncomplexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hyk","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06574v2","updated":"2025-05-23T15:42:28Z","published":"2025-02-10T15:42:38Z","title":"On the Impact of the Utility in Semivalue-based Data Valuation","summary":"  Semivalue-based data valuation uses cooperative-game theory intuitions to\nassign each data point a value reflecting its contribution to a downstream\ntask. Still, those values depend on the practitioner's choice of utility,\nraising the question: How robust is semivalue-based data valuation to changes\nin the utility? This issue is critical when the utility is set as a trade-off\nbetween several criteria and when practitioners must select among multiple\nequally valid utilities. We address it by introducing the notion of a dataset's\nspatial signature: given a semivalue, we embed each data point into a\nlower-dimensional space where any utility becomes a linear functional, making\nthe data valuation framework amenable to a simpler geometric picture. Building\non this, we propose a practical methodology centered on an explicit robustness\nmetric that informs practitioners whether and by how much their data valuation\nresults will shift as the utility changes. We validate this approach across\ndiverse datasets and semivalues, demonstrating strong agreement with\nrank-correlation analyses and offering analytical insight into how choosing a\nsemivalue can amplify or diminish robustness.\n","authors":["Mlissa Tamine","Benjamin Heymann","Patrick Loiseau","Maxime Vono"],"pdf_url":"https://arxiv.org/pdf/2502.06574v2.pdf","comment":"32 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.18032v1","updated":"2025-05-23T15:36:22Z","published":"2025-05-23T15:36:22Z","title":"Mahalanobis++: Improving OOD Detection via Feature Normalization","summary":"  Detecting out-of-distribution (OOD) examples is an important task for\ndeploying reliable machine learning models in safety-critial applications.\nWhile post-hoc methods based on the Mahalanobis distance applied to pre-logit\nfeatures are among the most effective for ImageNet-scale OOD detection, their\nperformance varies significantly across models. We connect this inconsistency\nto strong variations in feature norms, indicating severe violations of the\nGaussian assumption underlying the Mahalanobis distance estimation. We show\nthat simple $\\ell_2$-normalization of the features mitigates this problem\neffectively, aligning better with the premise of normally distributed data with\nshared covariance matrix. Extensive experiments on 44 models across diverse\narchitectures and pretraining schemes show that $\\ell_2$-normalization improves\nthe conventional Mahalanobis distance-based approaches significantly and\nconsistently, and outperforms other recently proposed OOD detection methods.\n","authors":["Maximilian Mueller","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2505.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18030v1","updated":"2025-05-23T15:35:39Z","published":"2025-05-23T15:35:39Z","title":"Automata Learning of Preferences over Temporal Logic Formulas from\n  Pairwise Comparisons","summary":"  Many preference elicitation algorithms consider preference over propositional\nlogic formulas or items with different attributes. In sequential decision\nmaking, a user's preference can be a preorder over possible outcomes, each of\nwhich is a temporal sequence of events. This paper considers a class of\npreference inference problems where the user's unknown preference is\nrepresented by a preorder over regular languages (sets of temporal sequences),\nreferred to as temporal goals. Given a finite set of pairwise comparisons\nbetween finite words, the objective is to learn both the set of temporal goals\nand the preorder over these goals. We first show that a preference relation\nover temporal goals can be modeled by a Preference Deterministic Finite\nAutomaton (PDFA), which is a deterministic finite automaton augmented with a\npreorder over acceptance conditions. The problem of preference inference\nreduces to learning the PDFA. This problem is shown to be computationally\nchallenging, with the problem of determining whether there exists a PDFA of\nsize smaller than a given integer $k$, consistent with the sample, being\nNP-Complete. We formalize the properties of characteristic samples and develop\nan algorithm that guarantees to learn, given a characteristic sample, the\nminimal PDFA equivalent to the true PDFA from which the sample is drawn. We\npresent the method through a running example and provide detailed analysis\nusing a robotic motion planning problem.\n","authors":["Hazhar Rahmani","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2505.18030v1.pdf","comment":"16 pages, 11 figures, technical report, submission under review"},{"id":"http://arxiv.org/abs/2502.10440v2","updated":"2025-05-23T15:35:10Z","published":"2025-02-10T09:15:56Z","title":"Towards Copyright Protection for Knowledge Bases of Retrieval-augmented\n  Language Models via Reasoning","summary":"  Large language models (LLMs) are increasingly integrated into real-world\npersonalized applications through retrieval-augmented generation (RAG)\nmechanisms to supplement their responses with domain-specific knowledge.\nHowever, the valuable and often proprietary nature of the knowledge bases used\nin RAG introduces the risk of unauthorized usage by adversaries. Existing\nmethods that can be generalized as watermarking techniques to protect these\nknowledge bases typically involve poisoning or backdoor attacks. However, these\nmethods require altering the LLM's results of verification samples, inevitably\nmaking these watermarks susceptible to anomaly detection and even introducing\nnew security risks. To address these challenges, we propose \\name{} for\n`harmless' copyright protection of knowledge bases. Instead of manipulating\nLLM's final output, \\name{} implants distinct yet benign verification behaviors\nin the space of chain-of-thought (CoT) reasoning, maintaining the correctness\nof the final answer. Our method has three main stages: (1) Generating CoTs: For\neach verification question, we generate two `innocent' CoTs, including a target\nCoT for building watermark behaviors; (2) Optimizing Watermark Phrases and\nTarget CoTs: Inspired by our theoretical analysis, we optimize them to minimize\nretrieval errors under the \\emph{black-box} and \\emph{text-only} setting of\nsuspicious LLM, ensuring that only watermarked verification queries can\nretrieve their correspondingly target CoTs contained in the knowledge base; (3)\nOwnership Verification: We exploit a pairwise Wilcoxon test to verify whether a\nsuspicious LLM is augmented with the protected knowledge base by comparing its\nresponses to watermarked and benign verification queries. Our experiments on\ndiverse benchmarks demonstrate that \\name{} effectively protects knowledge\nbases and its resistance to adaptive attacks.\n","authors":["Junfeng Guo","Yiming Li","Ruibo Chen","Yihan Wu","Chenxi Liu","Yanshuo Chen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.10440v2.pdf","comment":"The first two authors contributed equally to this work. 25 pages"},{"id":"http://arxiv.org/abs/2505.18028v1","updated":"2025-05-23T15:34:08Z","published":"2025-05-23T15:34:08Z","title":"Knot So Simple: A Minimalistic Environment for Spatial Reasoning","summary":"  We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym.\n","authors":["Zizhao Chen","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2505.18028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14438v4","updated":"2025-05-23T15:30:27Z","published":"2024-05-23T11:10:32Z","title":"LoRA-Ensemble: Efficient Uncertainty Modelling for Self-Attention\n  Networks","summary":"  Numerous real-world decisions rely on machine learning algorithms and require\ncalibrated uncertainty estimates. However, modern methods often yield\noverconfident, uncalibrated predictions. The dominant approach to quantifying\nthe uncertainty inherent in the model is to train an ensemble of separate\npredictors and measure their empirical variance. In an explicit implementation,\nthe ensemble has high computational cost and memory footprint, especially if\nthe base model itself is already large, like modern transformers. This\nmotivates efforts to develop implicit ensemble methods that emulate the\nensemble without explicitly instantiating all its members. We introduce\nLoRA-Ensemble, a parameter-efficient ensembling method for self-attention\nnetworks. It is based on Low-Rank Adaptation (LoRA), originally developed for\nefficient LLM fine-tuning, and extends it into an implicit ensembling scheme,\nwhere all ensemble members share the same, pre-trained self-attention network,\nbut have individual low-rank matrices for the attention projections. The\nresulting method not only outperforms state-of-the-art implicit techniques like\nBatchEnsemble, but even matches or exceeds the accuracy of an Explicit\nEnsemble, while at the same time achieving superior calibration.\n","authors":["Dominik J. Mhlematter","Michelle Halbheer","Alexander Becker","Dominik Narnhofer","Helge Aasen","Konrad Schindler","Mehmet Ozgur Turkoglu"],"pdf_url":"https://arxiv.org/pdf/2405.14438v4.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2505.18023v1","updated":"2025-05-23T15:28:00Z","published":"2025-05-23T15:28:00Z","title":"Time to Spike? Understanding the Representational Power of Spiking\n  Neural Networks in Discrete Time","summary":"  Recent years have seen significant progress in developing spiking neural\nnetworks (SNNs) as a potential solution to the energy challenges posed by\nconventional artificial neural networks (ANNs). However, our theoretical\nunderstanding of SNNs remains relatively limited compared to the ever-growing\nbody of literature on ANNs. In this paper, we study a discrete-time model of\nSNNs based on leaky integrate-and-fire (LIF) neurons, referred to as\ndiscrete-time LIF-SNNs, a widely used framework that still lacks solid\ntheoretical foundations. We demonstrate that discrete-time LIF-SNNs with static\ninputs and outputs realize piecewise constant functions defined on polyhedral\nregions, and more importantly, we quantify the network size required to\napproximate continuous functions. Moreover, we investigate the impact of\nlatency (number of time steps) and depth (number of layers) on the complexity\nof the input space partitioning induced by discrete-time LIF-SNNs. Our analysis\nhighlights the importance of latency and contrasts these networks with ANNs\nemploying piecewise linear activation functions. Finally, we present numerical\nexperiments to support our theoretical findings.\n","authors":["Duc Anh Nguyen","Ernesto Araya","Adalbert Fono","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2505.18023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18017v1","updated":"2025-05-23T15:21:10Z","published":"2025-05-23T15:21:10Z","title":"Strictly Constrained Generative Modeling via Split Augmented Langevin\n  Sampling","summary":"  Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.\n","authors":["Matthieu Blanke","Yongquan Qu","Sara Shamekh","Pierre Gentine"],"pdf_url":"https://arxiv.org/pdf/2505.18017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18015v1","updated":"2025-05-23T15:17:45Z","published":"2025-05-23T15:17:45Z","title":"SemSegBench & DetecBench: Benchmarking Reliability and Generalization\n  Beyond Classification","summary":"  Reliability and generalization in deep learning are predominantly studied in\nthe context of image classification. Yet, real-world applications in\nsafety-critical domains involve a broader set of semantic tasks, such as\nsemantic segmentation and object detection, which come with a diverse set of\ndedicated model architectures. To facilitate research towards robust model\ndesign in segmentation and detection, our primary objective is to provide\nbenchmarking tools regarding robustness to distribution shifts and adversarial\nmanipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,\nalong with the most extensive evaluation to date on the reliability and\ngeneralization of semantic segmentation and object detection models. In\nparticular, we benchmark 76 segmentation models across four datasets and 61\nobject detectors across two datasets, evaluating their performance under\ndiverse adversarial attacks and common corruptions. Our findings reveal\nsystematic weaknesses in state-of-the-art models and uncover key trends based\non architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are\nopen-sourced in our GitHub repository\n(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)\nalong with our complete set of total 6139 evaluations. We anticipate the\ncollected data to foster and encourage future research towards improved model\nreliability beyond classification.\n","authors":["Shashank Agnihotri","David Schader","Jonas Jakubassa","Nico Sharei","Simon Kral","Mehmet Ege Kaar","Ruben Weber","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2505.18015v1.pdf","comment":"First seven listed authors have equal contribution. GitHub:\n  https://github.com/shashankskagnihotri/benchmarking_reliability_generalization.\n  arXiv admin note: text overlap with arXiv:2505.05091"},{"id":"http://arxiv.org/abs/2502.06343v2","updated":"2025-05-23T15:16:33Z","published":"2025-02-10T10:52:17Z","title":"Causal Lifting of Neural Representations: Zero-Shot Generalization for\n  Causal Inferences","summary":"  In many scientific domains, the cost of data annotation limits the scale and\npace of experimentation. Yet, modern machine learning systems offer a promising\nalternative, provided their predictions yield correct conclusions. We focus on\nPrediction-Powered Causal Inferences (PPCI), i.e., estimating the treatment\neffect in a target experiment with unlabeled factual outcomes, retrievable\nzero-shot from a pre-trained model. We first identify the conditional\ncalibration property to guarantee valid PPCI at population level. Then, we\nintroduce causal lifting, a new causal lifting constraint transferring validity\nacross experiments, which we propose to enforce in practice in Deconfounded\nEmpirical Risk Minimization, our new model-agnostic training objective. We\nvalidate our method on synthetic and real-world scientific data, offering\nsolutions to instances not solvable by vanilla Empirical Risk Minimization and\ninvariant training. In particular, we solve zero-shot PPCI on the ISTAnt\ndataset for the first time, fine-tuning a foundational model on our replica\ndataset of their ecological experiment with a different recording platform and\ntreatment.\n","authors":["Riccardo Cadei","Ilker Demirel","Piersilvio De Bartolomeis","Lukas Lindorfer","Sylvia Cremer","Cordelia Schmid","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2502.06343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18008v1","updated":"2025-05-23T15:13:19Z","published":"2025-05-23T15:13:19Z","title":"Deep Operator Neural Network Model Predictive Control","summary":"  In this paper, we consider the design of model predictive control (MPC)\nalgorithms based on deep operator neural networks (DeepONets). These neural\nnetworks are capable of accurately approximating real and complex valued\nsolutions of continuous time nonlinear systems without relying on recurrent\narchitectures. The DeepONet architecture is made up of two feedforward neural\nnetworks: the branch network, which encodes the input function space, and the\ntrunk network, which represents dependencies on temporal variables or initial\nconditions. Utilizing the original DeepONet architecture as a predictor within\nMPC for Multi Input Multi Output (MIMO) systems requires multiple branch\nnetworks, to generate multi output predictions, one for each input. Moreover,\nto predict multiple time steps into the future, the network has to be evaluated\nmultiple times. Motivated by this, we introduce a multi step DeepONet\n(MS-DeepONet) architecture that computes in one shot multi step predictions of\nsystem outputs from multi step input sequences, which is better suited for MPC.\nWe prove that the MS DeepONet is a universal approximator in terms of multi\nstep sequence prediction. Additionally, we develop automated hyper parameter\nselection strategies and implement MPC frameworks using both the standard\nDeepONet and the proposed MS DeepONet architectures in PyTorch. The\nimplementation is publicly available on GitHub. Simulation results demonstrate\nthat MS-DeepONet consistently outperforms the standard DeepONet in learning and\npredictive control tasks across several nonlinear benchmark systems: the van\nder Pol oscillator, the quadruple tank process, and a cart pendulum unstable\nsystem, where it successfully learns and executes multiple swing up and\nstabilization policies.\n","authors":["Thomas Oliver de Jong","Khemraj Shukla","Mircea Lazar"],"pdf_url":"https://arxiv.org/pdf/2505.18008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13609v3","updated":"2025-05-23T15:10:37Z","published":"2024-05-22T13:01:37Z","title":"Tackling Decision Processes with Non-Cumulative Objectives using\n  Reinforcement Learning","summary":"  Markov decision processes (MDPs) are used to model a wide variety of\napplications ranging from game playing over robotics to finance. Their optimal\npolicy typically maximizes the expected sum of rewards given at each step of\nthe decision process. However, a large class of problems does not fit\nstraightforwardly into this framework: Non-cumulative Markov decision processes\n(NCMDPs), where instead of the expected sum of rewards, the expected value of\nan arbitrary function of the rewards is maximized. Example functions include\nthe maximum of the rewards or their mean divided by their standard deviation.\nIn this work, we introduce a general mapping of NCMDPs to standard MDPs. This\nallows all techniques developed to find optimal policies for MDPs, such as\nreinforcement learning or dynamic programming, to be directly applied to the\nlarger class of NCMDPs. Focusing on reinforcement learning, we show\napplications in a diverse set of tasks, including classical control, portfolio\noptimization in finance, and discrete optimization problems. Given our\napproach, we can improve both final performance and training time compared to\nrelying on standard MDPs.\n","authors":["Maximilian Ngele","Jan Olle","Thomas Fsel","Remmy Zen","Florian Marquardt"],"pdf_url":"https://arxiv.org/pdf/2405.13609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18005v1","updated":"2025-05-23T15:09:04Z","published":"2025-05-23T15:09:04Z","title":"Distances for Markov chains from sample streams","summary":"  Bisimulation metrics are powerful tools for measuring similarities between\nstochastic processes, and specifically Markov chains. Recent advances have\nuncovered that bisimulation metrics are, in fact, optimal-transport distances,\nwhich has enabled the development of fast algorithms for computing such metrics\nwith provable accuracy and runtime guarantees. However, these recent methods,\nas well as all previously known methods, assume full knowledge of the\ntransition dynamics. This is often an impractical assumption in most real-world\nscenarios, where typically only sample trajectories are available. In this\nwork, we propose a stochastic optimization method that addresses this\nlimitation and estimates bisimulation metrics based on sample access, without\nrequiring explicit transition models. Our approach is derived from a new linear\nprogramming (LP) formulation of bisimulation metrics, which we solve using a\nstochastic primal-dual optimization method. We provide theoretical guarantees\non the sample complexity of the algorithm and validate its effectiveness\nthrough a series of empirical evaluations.\n","authors":["Sergio Calo","Anders Jonsson","Gergely Neu","Ludovic Schwartz","Javier Segovia-Aguas"],"pdf_url":"https://arxiv.org/pdf/2505.18005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18283v2","updated":"2025-05-23T15:07:15Z","published":"2025-01-30T11:46:00Z","title":"Random Feature Representation Boosting","summary":"  We introduce Random Feature Representation Boosting (RFRBoost), a novel\nmethod for constructing deep residual random feature neural networks (RFNNs)\nusing boosting theory. RFRBoost uses random features at each layer to learn the\nfunctional gradient of the network representation, enhancing performance while\npreserving the convex optimization benefits of RFNNs. In the case of MSE loss,\nwe obtain closed-form solutions to greedy layer-wise boosting with random\nfeatures. For general loss functions, we show that fitting random feature\nresidual blocks reduces to solving a quadratically constrained least squares\nproblem. Through extensive numerical experiments on tabular datasets for both\nregression and classification, we show that RFRBoost significantly outperforms\nRFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime\nwhere RFNNs are typically applied. Moreover, RFRBoost offers substantial\ncomputational benefits, and theoretical guarantees stemming from boosting\ntheory.\n","authors":["Nikita Zozoulenko","Thomas Cass","Lukas Gonon"],"pdf_url":"https://arxiv.org/pdf/2501.18283v2.pdf","comment":"To appear in ICML 2025"},{"id":"http://arxiv.org/abs/2505.18003v1","updated":"2025-05-23T15:06:21Z","published":"2025-05-23T15:06:21Z","title":"An Example Safety Case for Safeguards Against Misuse","summary":"  Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.\n","authors":["Joshua Clymer","Jonah Weinbaum","Robert Kirk","Kimberly Mai","Selena Zhang","Xander Davies"],"pdf_url":"https://arxiv.org/pdf/2505.18003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18002v1","updated":"2025-05-23T15:05:56Z","published":"2025-05-23T15:05:56Z","title":"Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View\n  Perspective","summary":"  Graph anomaly detection aims to identify unusual patterns in graph-based\ndata, with wide applications in fields such as web security and financial fraud\ndetection. Existing methods typically rely on contrastive learning, assuming\nthat a lower similarity between a node and its local subgraph indicates\nabnormality. However, these approaches overlook a crucial limitation: the\npresence of interfering edges invalidates this assumption, since it introduces\ndisruptive noise that compromises the contrastive learning process.\nConsequently, this limitation impairs the ability to effectively learn\nmeaningful representations of normal patterns, leading to suboptimal detection\nperformance. To address this issue, we propose a Clean-View Enhanced Graph\nAnomaly Detection framework (CVGAD), which includes a multi-scale anomaly\nawareness module to identify key sources of interference in the contrastive\nlearning process. Moreover, to mitigate bias from the one-step edge removal\nprocess, we introduce a novel progressive purification module. This module\nincrementally refines the graph by iteratively identifying and removing\ninterfering edges, thereby enhancing model performance. Extensive experiments\non five benchmark datasets validate the effectiveness of our approach.\n","authors":["Di Jin","Jingyi Cao","Xiaobao Wang","Bingdao Feng","Dongxiao He","Longbiao Wang","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2505.18002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18000v1","updated":"2025-05-23T15:05:49Z","published":"2025-05-23T15:05:49Z","title":"Anytime-valid, Bayes-assisted,Prediction-Powered Inference","summary":"  Given a large pool of unlabelled data and a smaller amount of labels,\nprediction-powered inference (PPI) leverages machine learning predictions to\nincrease the statistical efficiency of standard confidence interval procedures\nbased solely on labelled data, while preserving their fixed-time validity.\n  In this paper, we extend the PPI framework to the sequential setting, where\nlabelled and unlabelled datasets grow over time.\n  Exploiting Ville's inequality and the method of mixtures, we propose\nprediction-powered confidence sequence procedures that are valid uniformly over\ntime and naturally accommodate prior knowledge on the quality of the\npredictions to further boost efficiency.\n  We carefully illustrate the design choices behind our method and demonstrate\nits effectiveness in real and synthetic examples.\n","authors":["Valentin Kilian","Stefano Cortinovis","Franois Caron"],"pdf_url":"https://arxiv.org/pdf/2505.18000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12593v2","updated":"2025-05-23T15:04:18Z","published":"2025-03-16T17:59:20Z","title":"Fourier-Based 3D Multistage Transformer for Aberration Correction in\n  Multicellular Specimens","summary":"  High-resolution tissue imaging is often compromised by sample-induced optical\naberrations that degrade resolution and contrast. While wavefront sensor-based\nadaptive optics (AO) can measure these aberrations, such hardware solutions are\ntypically complex, expensive to implement, and slow when serially mapping\nspatially varying aberrations across large fields of view. Here, we introduce\nAOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine\nlearning-based aberration sensing framework built around a 3D multistage Vision\nTransformer that operates on Fourier domain embeddings. AOViFT infers\naberrations and restores diffraction-limited performance in puncta-labeled\nspecimens with substantially reduced computational cost, training time, and\nmemory footprint compared to conventional architectures or real-space networks.\nWe validated AOViFT on live gene-edited zebrafish embryos, demonstrating its\nability to correct spatially varying aberrations using either a deformable\nmirror or post-acquisition deconvolution. By eliminating the need for the guide\nstar and wavefront sensing hardware and simplifying the experimental workflow,\nAOViFT lowers technical barriers for high-resolution volumetric microscopy\nacross diverse biological samples.\n","authors":["Thayer Alshaabi","Daniel E. Milkie","Gaoxiang Liu","Cyna Shirazinejad","Jason L. Hong","Kemal Achour","Frederik Grlitz","Ana Milunovic-Jevtic","Cat Simmons","Ibrahim S. Abuzahriyeh","Erin Hong","Samara Erin Williams","Nathanael Harrison","Evan Huang","Eun Seok Bae","Alison N. Killilea","David G. Drubin","Ian A. Swinburne","Srigokul Upadhyayula","Eric Betzig"],"pdf_url":"https://arxiv.org/pdf/2503.12593v2.pdf","comment":"55 pages, 6 figures, 26 si figures, 8 si tables"},{"id":"http://arxiv.org/abs/2505.17999v1","updated":"2025-05-23T15:04:16Z","published":"2025-05-23T15:04:16Z","title":"Revisiting Feature Interactions from the Perspective of Quadratic Neural\n  Networks for Click-through Rate Prediction","summary":"  Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Lei Sang","Jieming Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.17999v1.pdf","comment":"KDD'25 accepted"},{"id":"http://arxiv.org/abs/2503.22228v2","updated":"2025-05-23T15:03:51Z","published":"2025-03-28T08:21:00Z","title":"MFH: A Multi-faceted Heuristic Algorithm Selection Approach for Software\n  Verification","summary":"  Currently, many verification algorithms are available to improve the\nreliability of software systems. Selecting the appropriate verification\nalgorithm typically demands domain expertise and non-trivial manpower. An\nautomated algorithm selector is thus desired. However, existing selectors,\neither depend on machine-learned strategies or manually designed heuristics,\nencounter issues such as reliance on high-quality samples with algorithm labels\nand limited scalability. In this paper, an automated algorithm selection\napproach, namely MFH, is proposed for software verification. Our approach\nleverages the heuristics that verifiers producing correct results typically\nimplement certain appropriate algorithms, and the supported algorithms by these\nverifiers indirectly reflect which ones are potentially applicable.\nSpecifically, MFH embeds the code property graph (CPG) of a semantic-preserving\ntransformed program to enhance the robustness of the prediction model.\nFurthermore, our approach decomposes the selection task into the sub-tasks of\npredicting potentially applicable algorithms and matching the most appropriate\nverifiers. Additionally, MFH also introduces a feedback loop on incorrect\npredictions to improve model prediction accuracy. We evaluate MFH on 20\nverifiers and over 15,000 verification tasks. Experimental results demonstrate\nthe effectiveness of MFH, achieving a prediction accuracy of 91.47% even\nwithout ground truth algorithm labels provided during the training phase.\nMoreover, the prediction accuracy decreases only by 0.84% when introducing 10\nnew verifiers, indicating the strong scalability of the proposed approach.\n","authors":["Jie Su","Liansai Deng","Cheng Wen","Rong Wang","Zhi Ma","Nan Zhang","Cong Tian","Zhenhua Duan","Shengchao Qin"],"pdf_url":"https://arxiv.org/pdf/2503.22228v2.pdf","comment":"The decision to withdraw the paper is driven by two reasons: 1. A\n  conflict of interest arises from the proposed methods overlapping with\n  pending patent applications by other authors. 2. Upon thorough review, it has\n  been discovered that the paper contains ambiguities and inaccuracies in\n  describing the method, potentially hindering readers' comprehension of the\n  content"},{"id":"http://arxiv.org/abs/2505.17997v1","updated":"2025-05-23T15:03:41Z","published":"2025-05-23T15:03:41Z","title":"Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective","summary":"  The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.\n","authors":["Jintian Shao","Yiming Cheng","Hongyi Huang","Beiwen Zhang","Zhiyu Wu","You Shan","Mingkai Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.17997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17989v1","updated":"2025-05-23T14:56:07Z","published":"2025-05-23T14:56:07Z","title":"Outcome-based Reinforcement Learning to Predict the Future","summary":"  Reinforcement learning with verifiable rewards (RLVR) has boosted math and\ncoding in large language models, yet there has been little effort to extend\nRLVR into messier, real-world domains like forecasting. One sticking point is\nthat outcome-based reinforcement learning for forecasting must learn from\nbinary, delayed, and noisy rewards, a regime where standard fine-tuning is\nbrittle. We show that outcome-only online RL on a 14B model can match\nfrontier-scale accuracy and surpass it in calibration and hypothetical\nprediction market betting by adapting two leading algorithms, Group-Relative\nPolicy Optimisation (GRPO) and ReMax, to the forecasting setting. Our\nadaptations remove per-question variance scaling in GRPO, apply\nbaseline-subtracted advantages in ReMax, hydrate training with 100k temporally\nconsistent synthetic questions, and introduce lightweight guard-rails that\npenalise gibberish, non-English responses and missing rationales, enabling a\nsingle stable pass over 110k events. Scaling ReMax to 110k questions and\nensembling seven predictions yields a 14B model that matches frontier baseline\no1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in\ncalibration (ECE = 0.042, p < 0.001). A simple trading rule turns this\ncalibration edge into \\$127 of hypothetical profit versus \\$92 for o1 (p =\n0.037). This demonstrates that refined RLVR methods can convert small-scale\nLLMs into potentially economically valuable forecasting tools, with\nimplications for scaling this to larger models.\n","authors":["Benjamin Turtel","Danny Franklin","Kris Skotheim","Luke Hewitt","Philipp Schoenegger"],"pdf_url":"https://arxiv.org/pdf/2505.17989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17988v1","updated":"2025-05-23T14:55:22Z","published":"2025-05-23T14:55:22Z","title":"Towards Revealing the Effectiveness of Small-Scale Fine-tuning in\n  R1-style Reinforcement Learning","summary":"  R1-style Reinforcement Learning (RL) significantly enhances Large Language\nModels' reasoning capabilities, yet the mechanism behind rule-based RL remains\nunclear. We found that small-scale SFT has significant influence on RL but\nshows poor efficiency. To explain our observations, we propose an analytical\nframework and compare the efficiency of SFT and RL by measuring sample effect.\nHypothetical analysis show that SFT efficiency is limited by training data.\nGuided by our analysis, we propose Re-distillation, a technique that fine-tunes\npretrain model through small-scale distillation from the RL-trained policy.\nExperiments on Knight & Knave and MATH datasets demonstrate re-distillation's\nsurprising efficiency: re-distilled models match RL performance with far fewer\nsamples and less computation. Empirical verification shows that sample effect\nis a good indicator of performance improvements. As a result, on K&K dataset,\nour re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT\nsamples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches\nits instruct-tuned variant without RL. Our work explains several interesting\nphenomena in R1-style RL, shedding light on the mechanisms behind its empirical\nsuccess. Code is available at: https://github.com/on1262/deep-reasoning\n","authors":["Yutong Chen","Jiandong Gao","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2505.17988v1.pdf","comment":"11 figs, 3 table, preprint"},{"id":"http://arxiv.org/abs/2502.16763v2","updated":"2025-05-23T14:53:25Z","published":"2025-02-24T00:50:02Z","title":"Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly\n  with Neural Networks","summary":"  Neural networks are known for their ability to approximate smooth functions,\nyet they fail to generalize perfectly to unseen inputs when trained on discrete\noperations. Such operations lie at the heart of algorithmic tasks such as\narithmetic, which is often used as a test bed for algorithmic execution in\nneural networks. In this work, we ask: can neural networks learn to execute\nbinary-encoded algorithmic instructions exactly? We use the Neural Tangent\nKernel (NTK) framework to study the training dynamics of two-layer fully\nconnected networks in the infinite-width limit and show how a sufficiently\nlarge ensemble of such models can be trained to execute exactly, with high\nprobability, four fundamental tasks: binary permutations, binary addition,\nbinary multiplication, and Subtract and Branch if Negative (SBN) instructions.\nSince SBN is Turing-complete, our framework extends to computable functions. We\nshow how this can be efficiently achieved using only logarithmically many\ntraining data. Our approach relies on two techniques: structuring the training\ndata to isolate bit-level rules, and controlling correlations in the NTK regime\nto align model predictions with the target algorithmic executions.\n","authors":["Artur Back de Luca","George Giapitzakis","Kimon Fountoulakis"],"pdf_url":"https://arxiv.org/pdf/2502.16763v2.pdf","comment":"43 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.17987v1","updated":"2025-05-23T14:52:48Z","published":"2025-05-23T14:52:48Z","title":"ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for\n  Human Activity Modeling","summary":"  Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.\n","authors":["Weihang You","Hanqi Jiang","Zishuai Liu","Zihang Xie","Tianming Liu","Jin Lu","Fei Dou"],"pdf_url":"https://arxiv.org/pdf/2505.17987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01932v2","updated":"2025-05-23T14:52:12Z","published":"2023-07-04T21:36:46Z","title":"Integrating Random Forests and Generalized Linear Models for Improved\n  Accuracy and Interpretability","summary":"  Random forests (RFs) are among the most popular supervised learning\nalgorithms due to their nonlinear flexibility and ease-of-use. However, as\nblack box models, they can only be interpreted via algorithmically-defined\nfeature importance methods, such as Mean Decrease in Impurity (MDI), which have\nbeen observed to be highly unstable and have ambiguous scientific meaning.\nFurthermore, they can perform poorly in the presence of smooth or additive\nstructure. To address this, we reinterpret decision trees and MDI as linear\nregression and $R^2$ values, respectively, with respect to engineered features\nassociated with the tree's decision splits. This allows us to combine the\nrespective strengths of RFs and generalized linear models in a framework called\nRF+, which also yields an improved feature importance method we call MDI+.\nThrough extensive data-inspired simulations and real-world datasets, we show\nthat RF+ improves prediction accuracy over RFs and that MDI+ outperforms\npopular feature importance measures in identifying signal features, often\nyielding more than a 10% improvement over its closest competitor. In case\nstudies on drug response prediction and breast cancer subtyping, we further\nshow that MDI+ extracts well-established genes with significantly greater\nstability compared to existing feature importance measures.\n","authors":["Abhineet Agarwal","Ana M. Kenney","Yan Shuo Tan","Tiffany M. Tang","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2307.01932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17974v1","updated":"2025-05-23T14:41:52Z","published":"2025-05-23T14:41:52Z","title":"Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher\n  Approximation for Compressing Large Language Models","summary":"  The Fisher information is a fundamental concept for characterizing the\nsensitivity of parameters in neural networks. However, leveraging the full\nobserved Fisher information is too expensive for large models, so most methods\nrely on simple diagonal approximations. While efficient, this approach ignores\nparameter correlations, often resulting in reduced performance on downstream\ntasks. In this work, we mitigate these limitations and propose Generalized\nFisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that\naccounts for both diagonal and off-diagonal elements of the Fisher information\nmatrix, providing a more accurate reflection of parameter importance. To make\nthe method tractable, we introduce a scalable adaptation of the\nKronecker-factored approximation algorithm for the observed Fisher information.\nWe demonstrate the effectiveness of our method on LLM compression, showing\nimprovements over existing compression baselines. For example, at a 20\ncompression rate on the MMLU benchmark, our method outperforms FWSVD, which is\nbased on a diagonal approximation of the Fisher information, by 5 percent,\nSVD-LLM by 3 percent, and ASVD by 6 percent compression rate.\n","authors":["Viktoriia Chekalina","Daniil Moskovskiy","Daria Cherniuk","Maxim Kurkin","Andrey Kuznetsov","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2505.17974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17973v1","updated":"2025-05-23T14:41:41Z","published":"2025-05-23T14:41:41Z","title":"To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile\n  Mapping Cameras to Textured Semantic 3D Building Models","summary":"  Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue\n","authors":["Simone Gaisbauer","Prabin Gyawali","Qilin Zhang","Olaf Wysocki","Boris Jutzi"],"pdf_url":"https://arxiv.org/pdf/2505.17973v1.pdf","comment":"Accepted to MMT, Xiamen, China; ISPRS Annals"},{"id":"http://arxiv.org/abs/2505.17972v1","updated":"2025-05-23T14:40:50Z","published":"2025-05-23T14:40:50Z","title":"MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from\n  Long EEG Recordings","summary":"  Feature engineering for generalized seizure detection models remains a\nsignificant challenge. Recently proposed models show variable performance\ndepending on the training data and remain ineffective at accurately\ndistinguishing artifacts from seizure data. In this study, we propose a novel\nend-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which\nefficiently distinguishes seizure events from background electroencephalogram\n(EEG) and artifacts/noise by capturing both temporal dependencies across\ndifferent time frames and spatial relationships between channels. The model has\nthree modules: convolution, feature extraction, and predictor. The convolution\nmodule extracts features through depth-wise and spatio-temporal convolution.\nThe feature extraction module individually reduces the feature dimension\nextracted from EEG segments and their sub-segments. Subsequently, the extracted\nfeatures are concatenated into a single vector for classification using a fully\nconnected classifier called the predictor module. In addition, an anomaly\nscore-based post-classification processing technique was introduced to reduce\nthe false-positive rates of the model. Experimental results were reported and\nanalyzed using different parameter settings and datasets (Siena (public) and\nJuntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the\nconventional non-multiresolution approach, improving the F1 scores from 0.177\nto 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%\nand 20.62%, respectively.\n","authors":["Kazi Mahmudul Hassan","Xuyang Zhao","Hidenori Sugano","Toshihisa Tanaka"],"pdf_url":"https://arxiv.org/pdf/2505.17972v1.pdf","comment":"26 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2505.17968v1","updated":"2025-05-23T14:37:36Z","published":"2025-05-23T14:37:36Z","title":"Are Large Language Models Reliable AI Scientists? Assessing\n  Reverse-Engineering of Black-Box Systems","summary":"  Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.\n","authors":["Jiayi Geng","Howard Chen","Dilip Arumugam","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2505.17968v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2505.17967v1","updated":"2025-05-23T14:37:00Z","published":"2025-05-23T14:37:00Z","title":"SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language\n  Models","summary":"  Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to reduce the memory usage of adaptive optimizers by\nconstraining learning to a lower-dimensional space. Prior work typically\nprojects gradients of linear layers using approaches based on Singular Value\nDecomposition (SVD). However, applying SVD-based procedures individually to\neach layer in large models is computationally expensive and incurs additional\nmemory costs due to storing the projection matrices. In this work, we propose a\ncomputationally efficient and conceptually simple two-step procedure to\napproximate SVD-based gradient projections into lower-dimensional spaces.\nFirst, we construct a complete orthogonal basis using predefined orthogonal\nmatrices of the Discrete Cosine Transform (DCT). Second, we adaptively select\nbasis columns based on their alignment with the gradient of each layer. Each\nprojection matrix in our method is obtained via a single matrix multiplication\nfollowed by a lightweight sorting step to identify the most relevant basis\nvectors. Due to the predefined nature of the orthogonal bases, they are\ncomputed once at the start of training. During training, we store only the\nindices of the selected columns, avoiding the need to store full projection\nmatrices for each layer. Our numerical experiments on both pre-training and\nfine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, matching the performance of costly\nSVD-based methods while achieving faster runtime and reduced memory usage.\n","authors":["Ionut-Vlad Modoranu","Mher Safaryan","Erik Schultheis","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2505.17967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17965v1","updated":"2025-05-23T14:34:46Z","published":"2025-05-23T14:34:46Z","title":"New Tight Bounds for SGD without Variance Assumption: A Computer-Aided\n  Lyapunov Analysis","summary":"  The analysis of Stochastic Gradient Descent (SGD) often relies on making some\nassumption on the variance of the stochastic gradients, which is usually not\nsatisfied or difficult to verify in practice. This paper contributes to a\nrecent line of works which attempt to provide guarantees without making any\nvariance assumption, leveraging only the (strong) convexity and smoothness of\nthe loss functions. In this context, we prove new theoretical bounds derived\nfrom the monotonicity of a simple Lyapunov energy, improving the current\nstate-of-the-art and extending their validity to larger step-sizes. Our\ntheoretical analysis is backed by a Performance Estimation Problem analysis,\nwhich allows us to claim that, empirically, the bias term in our bounds is\ntight within our framework.\n","authors":["Daniel Cortild","Lucas Ketels","Juan Peypouquet","Guillaume Garrigos"],"pdf_url":"https://arxiv.org/pdf/2505.17965v1.pdf","comment":"57 pages, 10 figures. Under review"},{"id":"http://arxiv.org/abs/2505.17962v1","updated":"2025-05-23T14:33:20Z","published":"2025-05-23T14:33:20Z","title":"A Principled Bayesian Framework for Training Binary and Spiking Neural\n  Networks","summary":"  We propose a Bayesian framework for training binary and spiking neural\nnetworks that achieves state-of-the-art performance without normalisation\nlayers. Unlike commonly used surrogate gradient methods -- often heuristic and\nsensitive to hyperparameter choices -- our approach is grounded in a\nprobabilistic model of noisy binary networks, enabling fully end-to-end\ngradient-based optimisation. We introduce importance-weighted straight-through\n(IW-ST) estimators, a unified class generalising straight-through and\nrelaxation-based estimators. We characterise the bias-variance trade-off in\nthis family and derive a bias-minimising objective implemented via an auxiliary\nloss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),\na variational inference framework that uses posterior noise to train Binary and\nSpiking Neural Networks with IW-ST. This Bayesian approach minimises gradient\nbias, regularises parameters, and introduces dropout-like noise. By linking\nlow-bias conditions, vanishing gradients, and the KL term, we enable training\nof deep residual networks without normalisation. Experiments on CIFAR-10, DVS\nGesture, and SHD show our method matches or exceeds existing approaches without\nnormalisation or hand-tuned gradients.\n","authors":["James A. Walker","Moein Khajehnejad","Adeel Razi"],"pdf_url":"https://arxiv.org/pdf/2505.17962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17959v1","updated":"2025-05-23T14:31:36Z","published":"2025-05-23T14:31:36Z","title":"Mind the Domain Gap: Measuring the Domain Gap Between Real-World and\n  Synthetic Point Clouds for Automated Driving Development","summary":"  Owing to the typical long-tail data distribution issues, simulating\ndomain-gap-free synthetic data is crucial in robotics, photogrammetry, and\ncomputer vision research. The fundamental challenge pertains to credibly\nmeasuring the difference between real and simulated data. Such a measure is\nvital for safety-critical applications, such as automated driving, where\nout-of-domain samples may impact a car's perception and cause fatal accidents.\nPrevious work has commonly focused on simulating data on one scene and\nanalyzing performance on a different, real-world scene, hampering the disjoint\nanalysis of domain gap coming from networks' deficiencies, class definitions,\nand object representation. In this paper, we propose a novel approach to\nmeasuring the domain gap between the real world sensor observations and\nsimulated data representing the same location, enabling comprehensive domain\ngap analysis. To measure such a domain gap, we introduce a novel metric\nDoGSS-PCL and evaluation assessing the geometric and semantic quality of the\nsimulated point cloud. Our experiments corroborate that the introduced approach\ncan be used to measure the domain gap. The tests also reveal that synthetic\nsemantic point clouds may be used for training deep neural networks,\nmaintaining the performance at the 50/50 real-to-synthetic ratio. We strongly\nbelieve that this work will facilitate research on credible data simulation and\nallow for at-scale deployment in automated driving testing and digital\ntwinning.\n","authors":["Nguyen Duc","Yan-Ling Lai","Patrick Madlindl","Xinyuan Zhu","Benedikt Schwab","Olaf Wysocki","Ludwig Hoegner","Thomas H. Kolbe"],"pdf_url":"https://arxiv.org/pdf/2505.17959v1.pdf","comment":"Submitted to PFG Journal of Photogrammetry, Remote Sensing and\n  Geoinformation Science"},{"id":"http://arxiv.org/abs/2505.17958v1","updated":"2025-05-23T14:31:14Z","published":"2025-05-23T14:31:14Z","title":"The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized\n  Quadratic Networks","summary":"  We study the high-dimensional asymptotics of empirical risk minimization\n(ERM) in over-parametrized two-layer neural networks with quadratic activations\ntrained on synthetic data. We derive sharp asymptotics for both training and\ntest errors by mapping the $\\ell_2$-regularized learning problem to a convex\nmatrix sensing task with nuclear norm penalization. This reveals that capacity\ncontrol in such networks emerges from a low-rank structure in the learned\nfeature maps. Our results characterize the global minima of the loss and yield\nprecise generalization thresholds, showing how the width of the target function\ngoverns learnability. This analysis bridges and extends ideas from spin-glass\nmethods, matrix factorization, and convex optimization and emphasizes the deep\nlink between low-rank matrix sensing and learning in quadratic neural networks.\n","authors":["Vittorio Erba","Emanuele Troiani","Lenka Zdeborov","Florent Krzakala"],"pdf_url":"https://arxiv.org/pdf/2505.17958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08634v2","updated":"2025-05-23T14:30:25Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using\n  Multi-scale Implicit Neural Representation","summary":"  High-resolution whole-brain in vivo MR imaging at mesoscale resolutions\nremains challenging due to long scan durations, motion artifacts, and limited\nsignal-to-noise ratio (SNR). This study proposes Rotating-view super-resolution\n(ROVER)-MRI, an unsupervised framework based on multi-scale implicit neural\nrepresentations (INR), enabling efficient recovery of fine anatomical details\nfrom multi-view thick-slice acquisitions. ROVER-MRI employs coordinate-based\nneural networks to implicitly and continuously encode image structures at\nmultiple spatial scales, simultaneously modeling anatomical continuity and\ncorrecting inter-view motion through an integrated registration mechanism.\nValidation on ex-vivo monkey brain data and multiple in-vivo human datasets\ndemonstrates substantially improved reconstruction performance compared to\nbicubic interpolation and state-of-the-art regularized least-squares\nsuper-resolution reconstruction (LS-SRR) with 2-fold reduction in scan time.\nNotably, ROVER-MRI achieves an unprecedented whole-brain in-vivo T2-weighted\nimaging at 180 micron isotropic resolution in only 17 minutes of scan time on a\n7T scanner with 22.4% lower relative error compared to LS-SRR. We also\ndemonstrate improved SNR using ROVER-MRI compared to a time-matched 3D GRE\nacquisition. Quantitative results on several datasets demonstrate better\nsharpness of the reconstructed images with ROVER-MRI for different\nsuper-resolution factors (5 to 11). These findings highlight ROVER-MRI's\npotential as a rapid, accurate, and motion-resilient mesoscale imaging\nsolution, promising substantial advantages for neuroimaging studies.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06358v2","updated":"2025-05-23T14:30:04Z","published":"2023-11-10T19:11:13Z","title":"Compact Matrix Quantum Group Equivariant Neural Networks","summary":"  Group equivariant neural networks have proven effective in modelling a wide\nrange of tasks where the data lives in a classical geometric space and exhibits\nwell-defined group symmetries. However, these networks are not suitable for\nlearning from data that lives in a non-commutative geometry, described formally\nby non-commutative $C^{*}$-algebras, since the $C^{*}$-algebra of continuous\nfunctions on a compact matrix group is commutative. To address this limitation,\nwe derive the existence of a new type of equivariant neural network, called\ncompact matrix quantum group equivariant neural networks, which encode\nsymmetries that are described by compact matrix quantum groups. We characterise\nthe weight matrices that appear in these neural networks for the easy compact\nmatrix quantum groups, which are defined by set partitions. As a result, we\nobtain new characterisations of equivariant weight matrices for some compact\nmatrix groups that have not appeared previously in the machine learning\nliterature.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2311.06358v2.pdf","comment":"ICML 2025 Poster; 32 pages"},{"id":"http://arxiv.org/abs/2503.17400v2","updated":"2025-05-23T14:28:05Z","published":"2025-03-19T17:30:57Z","title":"TripNet: Learning Large-scale High-fidelity 3D Car Aerodynamics with\n  Triplane Networks","summary":"  Surrogate modeling has emerged as a powerful tool to accelerate Computational\nFluid Dynamics (CFD) simulations. Existing 3D geometric learning models based\non point clouds, voxels, meshes, or graphs depend on explicit geometric\nrepresentations that are memory-intensive and resolution-limited. For\nlarge-scale simulations with millions of nodes and cells, existing models\nrequire aggressive downsampling due to their dependence on mesh resolution,\nresulting in degraded accuracy. We present TripNet, a triplane-based neural\nframework that implicitly encodes 3D geometry into a compact, continuous\nfeature map with fixed dimension. Unlike mesh-dependent approaches, TripNet\nscales to high-resolution simulations without increasing memory cost, and\nenables CFD predictions at arbitrary spatial locations in a query-based\nfashion, independent of mesh connectivity or predefined nodes. TripNet achieves\nstate-of-the-art performance on the DrivAerNet and DrivAerNet++ datasets,\naccurately predicting drag coefficients, surface pressure, and full 3D flow\nfields. With a unified triplane backbone supporting multiple simulation tasks,\nTripNet offers a scalable, accurate, and efficient alternative to traditional\nCFD solvers and existing surrogate models.\n","authors":["Qian Chen","Mohamed Elrefaie","Angela Dai","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2503.17400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11276v2","updated":"2025-05-23T14:20:09Z","published":"2025-03-14T10:33:13Z","title":"Permutation Equivariant Neural Networks for Symmetric Tensors","summary":"  Incorporating permutation equivariance into neural networks has proven to be\nuseful in ensuring that models respect symmetries that exist in data. Symmetric\ntensors, which naturally appear in statistics, machine learning, and graph\ntheory, are essential for many applications in physics, chemistry, and\nmaterials science, amongst others. However, existing research on permutation\nequivariant models has not explored symmetric tensors as inputs, and most prior\nwork on learning from these tensors has focused on equivariance to Euclidean\ngroups. In this paper, we present two different characterisations of all linear\npermutation equivariant functions between symmetric power spaces of\n$\\mathbb{R}^n$. We show on two tasks that these functions are highly data\nefficient compared to standard MLPs and have potential to generalise well to\nsymmetric tensors of different sizes.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2503.11276v2.pdf","comment":"ICML 2025 Poster; 40 pages"},{"id":"http://arxiv.org/abs/2505.17941v1","updated":"2025-05-23T14:17:56Z","published":"2025-05-23T14:17:56Z","title":"VeriThinker: Learning to Verify Makes Reasoning Model Efficient","summary":"  Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker\n","authors":["Zigeng Chen","Xinyin Ma","Gongfan Fang","Ruonan Yu","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.17941v1.pdf","comment":"Working in progress. Code Repo:\n  https://github.com/czg1225/VeriThinker"},{"id":"http://arxiv.org/abs/2505.17939v1","updated":"2025-05-23T14:15:44Z","published":"2025-05-23T14:15:44Z","title":"Directed Semi-Simplicial Learning with Applications to Brain Activity\n  Decoding","summary":"  Graph Neural Networks (GNNs) excel at learning from pairwise interactions but\noften overlook multi-way and hierarchical relationships. Topological Deep\nLearning (TDL) addresses this limitation by leveraging combinatorial\ntopological spaces. However, existing TDL models are restricted to undirected\nsettings and fail to capture the higher-order directed patterns prevalent in\nmany complex systems, e.g., brain networks, where such interactions are both\nabundant and functionally significant. To fill this gap, we introduce\nSemi-Simplicial Neural Networks (SSNs), a principled class of TDL models that\noperate on semi-simplicial sets -- combinatorial structures that encode\ndirected higher-order motifs and their directional relationships. To enhance\nscalability, we propose Routing-SSNs, which dynamically select the most\ninformative relations in a learnable manner. We prove that SSNs are strictly\nmore expressive than standard graph and TDL models. We then introduce a new\nprincipled framework for brain dynamics representation learning, grounded in\nthe ability of SSNs to provably recover topological descriptors shown to\nsuccessfully characterize brain activity. Empirically, SSNs achieve\nstate-of-the-art performance on brain dynamics classification tasks,\noutperforming the second-best model by up to 27%, and message passing GNNs by\nup to 50% in accuracy. Our results highlight the potential of principled\ntopological models for learning from structured brain data, establishing a\nunique real-world case study for TDL. We also test SSNs on standard node\nclassification and edge regression tasks, showing competitive performance. We\nwill make the code and data publicly available.\n","authors":["Manuel Lecha","Andrea Cavallo","Francesca Dominici","Ran Levi","Alessio Del Bue","Elvin Isufi","Pietro Morerio","Claudio Battiloro"],"pdf_url":"https://arxiv.org/pdf/2505.17939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17938v1","updated":"2025-05-23T14:15:26Z","published":"2025-05-23T14:15:26Z","title":"LMask: Learn to Solve Constrained Routing Problems with Lazy Masking","summary":"  Routing problems are canonical combinatorial optimization tasks with\nwide-ranging applications in logistics, transportation, and supply chain\nmanagement. However, solving these problems becomes significantly more\nchallenging when complex constraints are involved. In this paper, we propose\nLMask, a novel learning framework that utilizes dynamic masking to generate\nhigh-quality feasible solutions for constrained routing problems. LMask\nintroduces the LazyMask decoding method, which lazily refines feasibility masks\nwith the backtracking mechanism. In addition, it employs the refinement\nintensity embedding to encode the search trace into the model, mitigating\nrepresentation ambiguities induced by backtracking. To further reduce sampling\ncost, LMask sets a backtracking budget during decoding, while constraint\nviolations are penalized in the loss function during training to counteract\ninfeasibility caused by this budget. We provide theoretical guarantees for the\nvalidity and probabilistic optimality of our approach. Extensive experiments on\nthe traveling salesman problem with time windows (TSPTW) and TSP with draft\nlimits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility\nrates and solution quality, outperforming existing neural methods.\n","authors":["Tianyou Li","Haijun Zou","Jiayuan Wu","Zaiwen Wen"],"pdf_url":"https://arxiv.org/pdf/2505.17938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17936v1","updated":"2025-05-23T14:14:17Z","published":"2025-05-23T14:14:17Z","title":"Understanding Gated Neurons in Transformers from Their Input-Output\n  Functionality","summary":"  Interpretability researchers have attempted to understand MLP neurons of\nlanguage models based on both the contexts in which they activate and their\noutput weight vectors. They have paid little attention to a complementary\naspect: the interactions between input and output. For example, when neurons\ndetect a direction in the input, they might add much the same direction to the\nresidual stream (\"enrichment neurons\") or reduce its presence (\"depletion\nneurons\"). We address this aspect by examining the cosine similarity between\ninput and output weights of a neuron. We apply our method to 12 models and find\nthat enrichment neurons dominate in early-middle layers whereas later layers\ntend more towards depletion. To explain this finding, we argue that enrichment\nneurons are largely responsible for enriching concept representations, one of\nthe first steps of factual recall. Our input-output perspective is a complement\nto activation-dependent analyses and to approaches that treat input and output\nseparately.\n","authors":["Sebastian Gerstner","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2505.17936v1.pdf","comment":"31 pages, 22 figures"},{"id":"http://arxiv.org/abs/2505.17932v1","updated":"2025-05-23T14:08:56Z","published":"2025-05-23T14:08:56Z","title":"Selection Mechanisms for Sequence Modeling using Linear State Space\n  Models","summary":"  Recent advancements in language modeling tasks have been driven by\narchitectures such as Transformers and, more recently, by Selective State Space\nModels (SSMs). In this paper, we introduce an alternative selection mechanism\ninspired by control theory methodologies. Specifically, we propose a novel\nresidual generator for selection, drawing an analogy to fault detection\nstrategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes\nLinear Time-Varying (LTV) systems, our approach combines multiple LTI systems,\npreserving their beneficial properties during training while achieving\ncomparable selectivity. To evaluate the effectiveness of the proposed\narchitecture, we test its performance on synthetic tasks. While these tasks are\nnot inherently critical, they serve as benchmarks to test the selectivity\nproperties of different cores architecture. This work highlights the potential\nof integrating theoretical insights with experimental advancements, offering a\ncomplementary perspective to deep learning innovations at the intersection of\ncontrol theory and machine learning.\n","authors":["Umberto Casti","Sandro Zampieri","Fabio Pasqualetti"],"pdf_url":"https://arxiv.org/pdf/2505.17932v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.17931v1","updated":"2025-05-23T14:07:21Z","published":"2025-05-23T14:07:21Z","title":"AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation\n  of Foundation Models","summary":"  Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.\n","authors":["Xingjian Li","Qifeng Wu","Colleen Que","Yiran Ding","Adithya S. Ubaradka","Jianhua Xing","Tianyang Wang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2505.17931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17929v1","updated":"2025-05-23T14:06:42Z","published":"2025-05-23T14:06:42Z","title":"Predicting Length of Stay in Neurological ICU Patients Using Classical\n  Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV","summary":"  Intensive care unit (ICU) is a crucial hospital department that handles\nlife-threatening cases. Nowadays machine learning (ML) is being leveraged in\nhealthcare ubiquitously. In recent years, management of ICU became one of the\nmost significant parts of the hospital functionality (largely but not only due\nto the worldwide COVID-19 pandemic). This study explores multiple ML approaches\nfor predicting LOS in ICU specifically for the patients with neurological\ndiseases based on the MIMIC-IV dataset. The evaluated models include classic ML\nalgorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and\nNeural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS\nprediction is often framed as a classification task, this study categorizes LOS\ninto three groups: less than two days, less than a week, and a week or more. As\nthe first ML-based approach targeting LOS prediction for neurological disorder\npatients, this study does not aim to outperform existing methods but rather to\nassess their effectiveness in this specific context. The findings provide\ninsights into the applicability of ML techniques for improving ICU resource\nmanagement and patient care. According to the results, Random Forest model\nproved to outperform others on static, achieving an accuracy of 0.68, a\nprecision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model\noutperformed LSTM model on time-series data with an accuracy of 0.80, a\nprecision of 0.80, a recall of 0.80 and F1-score 0.80.\n","authors":["Alexander Gabitashvili","Philipp Kellmeyer"],"pdf_url":"https://arxiv.org/pdf/2505.17929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17928v1","updated":"2025-05-23T14:06:26Z","published":"2025-05-23T14:06:26Z","title":"Towards Practical Defect-Focused Automated Code Review","summary":"  The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability.\n","authors":["Junyi Lu","Lili Jiang","Xiaojia Li","Jianbing Fang","Fengjun Zhang","Li Yang","Chun Zuo"],"pdf_url":"https://arxiv.org/pdf/2505.17928v1.pdf","comment":"Accepted to Forty-Second International Conference on Machine Learning\n  (ICML 2025)"},{"id":"http://arxiv.org/abs/2502.05599v2","updated":"2025-05-23T14:02:47Z","published":"2025-02-08T14:58:49Z","title":"Online Bidding Algorithms with Strict Return on Spend (ROS) Constraint","summary":"  Auto-bidding problem under a strict return-on-spend constraint (ROSC) is\nconsidered, where an algorithm has to make decisions about how much to bid for\nan ad slot depending on the revealed value, and the hidden allocation and\npayment function that describes the probability of winning the ad-slot\ndepending on its bid. The objective of an algorithm is to maximize the expected\nutility (product of ad value and probability of winning the ad slot) summed\nacross all time slots subject to the total expected payment being less than the\ntotal expected utility, called the ROSC. A (surprising) impossibility result is\nderived that shows that no online algorithm can achieve a sub-linear regret\neven when the value, allocation and payment function are drawn i.i.d. from an\nunknown distribution. The problem is non-trivial even when the revealed value\nremains constant across time slots, and an algorithm with regret guarantee that\nis optimal up to logarithmic factor is derived.\n","authors":["Rahul Vaze","Abhishek Sinha"],"pdf_url":"https://arxiv.org/pdf/2502.05599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17921v1","updated":"2025-05-23T13:59:02Z","published":"2025-05-23T13:59:02Z","title":"Evaluation of Few-Shot Learning Methods for Kidney Stone Type\n  Recognition in Ureteroscopy","summary":"  Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset.\n","authors":["Carlos Salazar-Ruiz","Francisco Lopez-Tiro","Ivan Reyes-Amezcua","Clement Larose","Gilberto Ochoa-Ruiz","Christian Daul"],"pdf_url":"https://arxiv.org/pdf/2505.17921v1.pdf","comment":"6 pages, 3 figures, 3 tables, conference, cbms25"},{"id":"http://arxiv.org/abs/2505.17919v1","updated":"2025-05-23T13:58:29Z","published":"2025-05-23T13:58:29Z","title":"KITINet: Kinetics Theory Inspired Network Architectures with PDE\n  Simulation Approaches","summary":"  Despite the widely recognized success of residual connections in modern\nneural networks, their design principles remain largely heuristic. This paper\nintroduces KITINet (Kinetics Theory Inspired Network), a novel architecture\nthat reinterprets feature propagation through the lens of non-equilibrium\nparticle dynamics and partial differential equation (PDE) simulation. At its\ncore, we propose a residual module that models feature updates as the\nstochastic evolution of a particle system, numerically simulated via a\ndiscretized solver for the Boltzmann transport equation (BTE). This formulation\nmimics particle collisions and energy exchange, enabling adaptive feature\nrefinement via physics-informed interactions. Additionally, we reveal that this\nmechanism induces network parameter condensation during training, where\nparameters progressively concentrate into a sparse subset of dominant channels.\nExperiments on scientific computation (PDE operator), image classification\n(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent\nimprovements over classic network baselines, with negligible increase of FLOPs.\n","authors":["Mingquan Feng","Yifan Fu","Tongcheng Zhang","Yu Jiang","Yixin Huang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2505.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02298v3","updated":"2025-05-23T13:58:15Z","published":"2025-01-04T14:33:27Z","title":"Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds\n  for Score-Based Generative Models in W2-distance","summary":"  Score-based Generative Models (SGMs) aim to sample from a target distribution\nby learning score functions using samples perturbed by Gaussian noise. Existing\nconvergence bounds for SGMs in the $\\mathcal{W}_2$-distance rely on stringent\nassumptions about the data distribution. In this work, we present a novel\nframework for analyzing $\\mathcal{W}_2$-convergence in SGMs, significantly\nrelaxing traditional assumptions such as log-concavity and score regularity.\nLeveraging the regularization properties of the Ornstein--Uhlenbeck (OU)\nprocess, we show that weak log-concavity of the data distribution evolves into\nlog-concavity over time. This transition is rigorously quantified through a\nPDE-based analysis of the Hamilton--Jacobi--Bellman equation governing the\nlog-density of the forward process. Moreover, we establish that the drift of\nthe time-reversed OU process alternates between contractive and non-contractive\nregimes, reflecting the dynamics of concavity. Our approach circumvents the\nneed for stringent regularity conditions on the score function and its\nestimators, relying instead on milder, more practical assumptions. We\ndemonstrate the wide applicability of this framework through explicit\ncomputations on Gaussian mixture models, illustrating its versatility and\npotential for broader classes of data distributions.\n","authors":["Marta Gentiloni-Silveri","Antonio Ocello"],"pdf_url":"https://arxiv.org/pdf/2501.02298v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17918v1","updated":"2025-05-23T13:57:53Z","published":"2025-05-23T13:57:53Z","title":"LLM Meeting Decision Trees on Tabular Data","summary":"  Tabular data have been playing a vital role in diverse real-world fields,\nincluding healthcare, finance, etc. With the recent success of Large Language\nModels (LLMs), early explorations of extending LLMs to the domain of tabular\ndata have been developed. Most of these LLM-based methods typically first\nserialize tabular data into natural language descriptions, and then tune LLMs\nor directly infer on these serialized data. However, these methods suffer from\ntwo key inherent issues: (i) data perspective: existing data serialization\nmethods lack universal applicability for structured tabular data, and may pose\nprivacy risks through direct textual exposure, and (ii) model perspective: LLM\nfine-tuning methods struggle with tabular data, and in-context learning\nscalability is bottle-necked by input length constraints (suitable for few-shot\nlearning). This work explores a novel direction of integrating LLMs into\ntabular data throughough logical decision tree rules as intermediaries,\nproposes a decision tree enhancer with LLM-derived rule for tabular prediction,\nDeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied\nto full data learning setting without LLM fine-tuning. Specifically, we\nleverage the reasoning ability of LLMs to redesign an improved rule given a set\nof decision tree rules. Furthermore, we provide a calibration method for\noriginal decision trees via new generated rule by LLM, which approximates the\nerror correction vector to steer the original decision tree predictions in the\ndirection of ``errors'' reducing. Finally, extensive experiments on diverse\ntabular benchmarks show that our method achieves state-of-the-art performance.\n","authors":["Hangting Ye","Jinmeng Li","He Zhao","Dandan Guo","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2505.17918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17917v1","updated":"2025-05-23T13:57:23Z","published":"2025-05-23T13:57:23Z","title":"M-learner:A Flexible And Powerful Framework To Study Heterogeneous\n  Treatment Effect In Mediation Model","summary":"  We propose a novel method, termed the M-learner, for estimating heterogeneous\nindirect and total treatment effects and identifying relevant subgroups within\na mediation framework. The procedure comprises four key steps. First, we\ncompute individual-level conditional average indirect/total treatment effect\nSecond, we construct a distance matrix based on pairwise differences. Third, we\napply tSNE to project this matrix into a low-dimensional Euclidean space,\nfollowed by K-means clustering to identify subgroup structures. Finally, we\ncalibrate and refine the clusters using a threshold-based procedure to\ndetermine the optimal configuration. To the best of our knowledge, this is the\nfirst approach specifically designed to capture treatment effect heterogeneity\nin the presence of mediation. Experimental results validate the robustness and\neffectiveness of the proposed framework. Application to the real-world Jobs II\ndataset highlights the broad adaptability and potential applicability of our\nmethod.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.\n","authors":["Xingyu Li","Qing Liu","Tony Jiang","Hong Amy Xia","Brian P. Hobbs","Peng Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17914v1","updated":"2025-05-23T13:56:30Z","published":"2025-05-23T13:56:30Z","title":"Flexible MOF Generation with Torsion-Aware Flow Matching","summary":"  Designing metal-organic frameworks (MOFs) with novel chemistries is a\nlong-standing challenge due to their large combinatorial space and the complex\n3D arrangements of building blocks. While recent deep generative models have\nenabled scalable MOF generation, they assume (1) a fixed set of building blocks\nand (2) known ground-truth local block-wise 3D coordinates. However, this\nlimits their ability to (1) design novel MOFs and (2) generate the structure\nusing novel building blocks. We propose a two-stage de novo MOF generation\nframework that overcomes these limitations by modeling both chemical and\ngeometric degrees of freedom. First, we train a SMILES-based autoregressive\nmodel to generate novel metal and organic building blocks, paired with\ncheminformatics for 3D structure initialization. Second, we introduce a\nflow-matching model that predicts translations, rotations, and torsional angles\nto assemble flexible blocks into valid 3D frameworks. Our experiments\ndemonstrate improved reconstruction accuracy, the generation of valid, novel,\nand unique MOFs, and the ability of our model to create novel building blocks.\n","authors":["Nayoung Kim","Seongsu Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2505.17914v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.17909v1","updated":"2025-05-23T13:53:21Z","published":"2025-05-23T13:53:21Z","title":"NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective\n  Ensembling","summary":"  Model ensembles have long been a cornerstone for improving generalization and\nrobustness in deep learning. However, their effectiveness often comes at the\ncost of substantial computational overhead. To address this issue,\nstate-of-the-art methods aim to replicate ensemble-class performance without\nrequiring multiple independently trained networks. Unfortunately, these\nalgorithms often still demand considerable compute at inference. In response to\nthese limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head\narchitecture with dynamically evolving topology. This unexplored model-agnostic\ntraining paradigm improves ensemble performance while reducing the required\nresources. We analyze the underlying reason for its effectiveness and observe\nthat the various neural trails induced by dynamic sparsity attain a\n$\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays\nefficacy with convolutional and transformer-based architectures on computer\nvision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,\namong many others, demonstrate increased accuracy and stronger robustness in\nzero-shot generalization, while requiring significantly fewer parameters.\n","authors":["Bram Grooten","Farid Hasanov","Chenxiang Zhang","Qiao Xiao","Boqian Wu","Zahra Atashgahi","Ghada Sokar","Shiwei Liu","Lu Yin","Elena Mocanu","Mykola Pechenizkiy","Decebal Constantin Mocanu"],"pdf_url":"https://arxiv.org/pdf/2505.17909v1.pdf","comment":"Our open-source code is available at\n  https://github.com/bramgrooten/neurotrails"},{"id":"http://arxiv.org/abs/2505.17907v1","updated":"2025-05-23T13:53:02Z","published":"2025-05-23T13:53:02Z","title":"Function Forms of Simple ReLU Networks with Random Hidden Weights","summary":"  We investigate the function space dynamics of a two-layer ReLU neural network\nin the infinite-width limit, highlighting the Fisher information matrix (FIM)'s\nrole in steering learning. Extending seminal works on approximate\neigendecomposition of the FIM, we derive the asymptotic behavior of basis\nfunctions ($f_v(x) = X^{\\top} v $) for four groups of approximate eigenvectors,\nshowing their convergence to distinct function forms. These functions,\nprioritized by gradient descent, exhibit FIM-induced inner products that\napproximate orthogonality in the function space, forging a novel connection\nbetween parameter and function spaces. Simulations validate the accuracy of\nthese theoretical approximations, confirming their practical relevance. By\nrefining the function space inner product's role, we advance the theoretical\nframework for ReLU networks, illuminating their optimization and expressivity.\nOverall, this work offers a robust foundation for understanding wide neural\nnetworks and enhances insights into scalable deep learning architectures,\npaving the way for improved design and analysis of neural networks.\n","authors":["Ka Long Keith Ho","Yoshinari Takeishi","Junichi Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2505.17907v1.pdf","comment":"21 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2405.18220v3","updated":"2025-05-23T13:52:56Z","published":"2024-05-28T14:28:28Z","title":"E$^2$M: Double Bounded $$-Divergence Optimization for Tensor-based\n  Discrete Density Estimation","summary":"  Tensor-based discrete density estimation requires flexible modeling and\nproper divergence criteria to enable effective learning; however, traditional\napproaches using $\\alpha$-divergence face analytical challenges due to the\n$\\alpha$-power terms in the objective function, which hinder the derivation of\nclosed-form update rules. We present a generalization of the\nexpectation-maximization (EM) algorithm, called E$^2$M algorithm. It\ncircumvents this issue by first relaxing the optimization into minimization of\na surrogate objective based on the Kullback-Leibler (KL) divergence, which is\ntractable via the standard EM algorithm, and subsequently applying a tensor\nmany-body approximation in the M-step to enable simultaneous closed-form\nupdates of all parameters. Our approach offers flexible modeling for not only a\nvariety of low-rank structures, including the CP, Tucker, and Tensor Train\nformats, but also their mixtures, thus allowing us to leverage the strengths of\ndifferent low-rank structures. We demonstrate the effectiveness of our approach\nin classification and density estimation tasks.\n","authors":["Kazu Ghalamkari","Jesper Lve Hinrich","Morten Mrup"],"pdf_url":"https://arxiv.org/pdf/2405.18220v3.pdf","comment":"34 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.17902v1","updated":"2025-05-23T13:50:02Z","published":"2025-05-23T13:50:02Z","title":"Evolving Machine Learning: A Survey","summary":"  In an era defined by rapid data evolution, traditional machine learning (ML)\nmodels often fall short in adapting to dynamic environments. Evolving Machine\nLearning (EML) has emerged as a critical paradigm, enabling continuous learning\nand adaptation in real-time data streams. This survey presents a comprehensive\nanalysis of EML, focusing on five core challenges: data drift, concept drift,\ncatastrophic forgetting, skewed learning, and network adaptation. We\nsystematically review over 120 studies, categorizing state-of-the-art methods\nacross supervised, unsupervised, and semi-supervised approaches. The survey\nexplores diverse evaluation metrics, benchmark datasets, and real-world\napplications, offering a comparative lens on the effectiveness and limitations\nof current techniques. Additionally, we highlight the growing role of adaptive\nneural architectures, meta-learning, and ensemble strategies in addressing\nevolving data complexities. By synthesizing insights from recent literature,\nthis work not only maps the current landscape of EML but also identifies\ncritical gaps and opportunities for future research. Our findings aim to guide\nresearchers and practitioners in developing robust, ethical, and scalable EML\nsystems for real-world deployment.\n","authors":["Ignacio Cabrera Martin","Subhaditya Mukherjee","Almas Baimagambetov","Joaquin Vanschoren","Nikolaos Polatidis"],"pdf_url":"https://arxiv.org/pdf/2505.17902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07910v2","updated":"2025-05-23T13:49:55Z","published":"2025-05-12T13:19:14Z","title":"Tuning for Trustworthiness -- Balancing Performance and Explanation\n  Consistency in Neural Network Optimization","summary":"  Despite the growing interest in Explainable Artificial Intelligence (XAI),\nexplainability is rarely considered during hyperparameter tuning or neural\narchitecture optimization, where the focus remains primarily on minimizing\npredictive loss. In this work, we introduce the novel concept of XAI\nconsistency, defined as the agreement among different feature attribution\nmethods, and propose new metrics to quantify it. For the first time, we\nintegrate XAI consistency directly into the hyperparameter tuning objective,\ncreating a multi-objective optimization framework that balances predictive\nperformance with explanation robustness. Implemented within the Sequential\nParameter Optimization Toolbox (SPOT), our approach uses both weighted\naggregation and desirability-based strategies to guide model selection. Through\nour proposed framework and supporting tools, we explore the impact of\nincorporating XAI consistency into the optimization process. This enables us to\ncharacterize distinct regions in the architecture configuration space: one\nregion with poor performance and comparatively low interpretability, another\nwith strong predictive performance but weak interpretability due to low\n\\gls{xai} consistency, and a trade-off region that balances both objectives by\noffering high interpretability alongside competitive performance. Beyond\nintroducing this novel approach, our research provides a foundation for future\ninvestigations into whether models from the trade-off zone-balancing\nperformance loss and XAI consistency-exhibit greater robustness by avoiding\noverfitting to training performance, thereby leading to more reliable\npredictions on out-of-distribution data.\n","authors":["Alexander Hinterleitner","Thomas Bartz-Beielstein"],"pdf_url":"https://arxiv.org/pdf/2505.07910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17899v1","updated":"2025-05-23T13:47:35Z","published":"2025-05-23T13:47:35Z","title":"Universal Domain Adaptation Benchmark for Time Series Data\n  Representation","summary":"  Deep learning models have significantly improved the ability to detect\nnovelties in time series (TS) data. This success is attributed to their strong\nrepresentation capabilities. However, due to the inherent variability in TS\ndata, these models often struggle with generalization and robustness. To\naddress this, a common approach is to perform Unsupervised Domain Adaptation,\nparticularly Universal Domain Adaptation (UniDA), to handle domain shifts and\nemerging novel classes. While extensively studied in computer vision, UniDA\nremains underexplored for TS data. This work provides a comprehensive\nimplementation and comparison of state-of-the-art TS backbones in a UniDA\nframework. We propose a reliable protocol to evaluate their robustness and\ngeneralization across different domains. The goal is to provide practitioners\nwith a framework that can be easily extended to incorporate future advancements\nin UniDA and TS architectures. Our results highlight the critical influence of\nbackbone selection in UniDA performance and enable a robustness analysis across\nvarious datasets and architectures.\n","authors":["Romain Mussard","Fannia Pacheco","Maxime Berar","Gilles Gasso","Paul Honeine"],"pdf_url":"https://arxiv.org/pdf/2505.17899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16923v2","updated":"2025-05-23T13:47:03Z","published":"2025-05-22T17:16:41Z","title":"TULiP: Test-time Uncertainty Estimation via Linearization and Weight\n  Perturbation","summary":"  A reliable uncertainty estimation method is the foundation of many modern\nout-of-distribution (OOD) detectors, which are critical for safe deployments of\ndeep learning models in the open world. In this work, we propose TULiP, a\ntheoretically-driven post-hoc uncertainty estimator for OOD detection. Our\napproach considers a hypothetical perturbation applied to the network before\nconvergence. Based on linearized training dynamics, we bound the effect of such\nperturbation, resulting in an uncertainty score computable by perturbing model\nparameters. Ultimately, our approach computes uncertainty from a set of sampled\npredictions. We visualize our bound on synthetic regression and classification\ndatasets. Furthermore, we demonstrate the effectiveness of TULiP using\nlarge-scale OOD detection benchmarks for image classification. Our method\nexhibits state-of-the-art performance, particularly for near-distribution\nsamples.\n","authors":["Yuhui Zhang","Dongshen Wu","Yuichiro Wada","Takafumi Kanamori"],"pdf_url":"https://arxiv.org/pdf/2505.16923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01468v2","updated":"2025-05-23T13:45:49Z","published":"2025-03-03T12:23:07Z","title":"Overcoming Non-stationary Dynamics with Evidential Proximal Policy\n  Optimization","summary":"  Continuous control of non-stationary environments is a major challenge for\ndeep reinforcement learning algorithms. The time-dependency of the state\ntransition dynamics aggravates the notorious stability problems of model-free\ndeep actor-critic architectures. We posit that two properties will play a key\nrole in overcoming non-stationarity in transition dynamics: (i) preserving the\nplasticity of the critic network, (ii) directed exploration for rapid\nadaptation to the changing dynamics. We show that performing on-policy\nreinforcement learning with an evidential critic provides both of these\nproperties. The evidential design ensures a fast and sufficiently accurate\napproximation to the uncertainty around the state-value, which maintains the\nplasticity of the critic network by detecting the distributional shifts caused\nby the change in dynamics. The probabilistic critic also makes the actor\ntraining objective a random variable, enabling the use of directed exploration\napproaches as a by-product. We name the resulting algorithm as $\\textit{\nEvidential Proximal Policy Optimization (EPPO)}$ due to the integral role of\nevidential uncertainty quantification in both policy evaluation and policy\nimprovement stages. Through experiments on non-stationary continuous control\ntasks, where the environment dynamics change at regular intervals, we\ndemonstrate that our algorithm outperforms state-of-the-art on-policy\nreinforcement learning variants in both task-specific and overall return.\n","authors":["Abdullah Akgl","Gulcin Baykal","Manuel Haumann","Melih Kandemir"],"pdf_url":"https://arxiv.org/pdf/2503.01468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17895v1","updated":"2025-05-23T13:43:14Z","published":"2025-05-23T13:43:14Z","title":"DataRater: Meta-Learned Dataset Curation","summary":"  The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.\n","authors":["Dan A. Calian","Gregory Farquhar","Iurii Kemaev","Luisa M. Zintgraf","Matteo Hessel","Jeremy Shar","Junhyuk Oh","Andrs Gyrgy","Tom Schaul","Jeffrey Dean","Hado van Hasselt","David Silver"],"pdf_url":"https://arxiv.org/pdf/2505.17895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08137v2","updated":"2025-05-23T13:34:47Z","published":"2024-08-15T13:13:17Z","title":"Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature\n  Attribution Explainability","summary":"  Deep neural network predictions are notoriously difficult to interpret.\nFeature attribution methods aim to explain these predictions by identifying the\ncontribution of each input feature. Faithfulness, often evaluated using the\narea over the perturbation curve (AOPC), reflects feature attributions'\naccuracy in describing the internal mechanisms of deep neural networks.\nHowever, many studies rely on AOPC to compare faithfulness across different\nmodels, which we show can lead to false conclusions about models' faithfulness.\nSpecifically, we find that AOPC is sensitive to variations in the model,\nresulting in unreliable cross-model comparisons. Moreover, AOPC scores are\ndifficult to interpret in isolation without knowing the model-specific lower\nand upper limits. To address these issues, we propose a normalization approach,\nNormalized AOPC (NAOPC), enabling consistent cross-model evaluations and more\nmeaningful interpretation of individual scores. Our experiments demonstrate\nthat this normalization can radically change AOPC results, questioning the\nconclusions of earlier studies and offering a more robust framework for\nassessing feature attribution faithfulness. Our code is available at\nhttps://github.com/JoakimEdin/naopc.\n","authors":["Joakim Edin","Andreas Geert Motzfeldt","Casper L. Christensen","Tuukka Ruotsalo","Lars Maale","Maria Maistro"],"pdf_url":"https://arxiv.org/pdf/2408.08137v2.pdf","comment":"Accepted to ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.17883v1","updated":"2025-05-23T13:31:54Z","published":"2025-05-23T13:31:54Z","title":"FastCAV: Efficient Computation of Concept Activation Vectors for\n  Explaining Deep Neural Networks","summary":"  Concepts such as objects, patterns, and shapes are how humans understand the\nworld. Building on this intuition, concept-based explainability methods aim to\nstudy representations learned by deep neural networks in relation to\nhuman-understandable concepts. Here, Concept Activation Vectors (CAVs) are an\nimportant tool and can identify whether a model learned a concept or not.\nHowever, the computational cost and time requirements of existing CAV\ncomputation pose a significant challenge, particularly in large-scale,\nhigh-dimensional architectures. To address this limitation, we introduce\nFastCAV, a novel approach that accelerates the extraction of CAVs by up to\n63.6x (on average 46.4x). We provide a theoretical foundation for our approach\nand give concrete assumptions under which it is equivalent to established\nSVM-based methods. Our empirical results demonstrate that CAVs calculated with\nFastCAV maintain similar performance while being more efficient and stable. In\ndownstream applications, i.e., concept-based explanation methods, we show that\nFastCAV can act as a replacement leading to equivalent insights. Hence, our\napproach enables previously infeasible investigations of deep models, which we\ndemonstrate by tracking the evolution of concepts during model training.\n","authors":["Laines Schmalwasser","Niklas Penzel","Joachim Denzler","Julia Niebling"],"pdf_url":"https://arxiv.org/pdf/2505.17883v1.pdf","comment":"Accepted at ICML 2025, 27 pages, 20 figures, 9 tables"},{"id":"http://arxiv.org/abs/2211.13723v4","updated":"2025-05-23T13:31:36Z","published":"2022-11-24T17:19:30Z","title":"Improving Multi-task Learning via Seeking Task-based Flat Regions","summary":"  Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for\ntraining deep neural networks that allows learning more than one objective by a\nsingle backbone. Compared to training tasks separately, MTL significantly\nreduces computational costs, improves data efficiency, and potentially enhances\nmodel performance by leveraging knowledge across tasks. Hence, it has been\nadopted in a variety of applications, ranging from computer vision to natural\nlanguage processing and speech recognition. Among them, there is an emerging\nline of work in MTL that focuses on manipulating the task gradient to derive an\nultimate gradient descent direction to benefit all tasks. Despite achieving\nimpressive results on many benchmarks, directly applying these approaches\nwithout using appropriate regularization techniques might lead to suboptimal\nsolutions on real-world problems. In particular, standard training that\nminimizes the empirical loss on the training data can easily suffer from\noverfitting to low-resource tasks or be spoiled by noisy-labeled ones, which\ncan cause negative transfer between tasks and overall performance drop. To\nalleviate such problems, we propose to leverage a recently introduced training\nmethod, named Sharpness-aware Minimization, which can enhance model\ngeneralization ability on single-task learning. Accordingly, we present a novel\nMTL training methodology, encouraging the model to find task-based flat minima\nfor coherently improving its generalization capability on all tasks. Finally,\nwe conduct comprehensive experiments on a variety of applications to\ndemonstrate the merit of our proposed approach to existing gradient-based MTL\nmethods, as suggested by our developed theory.\n","authors":["Hoang Phan","Lam Tran","Quyen Tran","Ngoc N. Tran","Tuan Truong","Qi Lei","Nhat Ho","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2211.13723v4.pdf","comment":"35 pages, 17 figures, 7 tables"},{"id":"http://arxiv.org/abs/2505.17881v1","updated":"2025-05-23T13:31:13Z","published":"2025-05-23T13:31:13Z","title":"Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring\n  Factors Regularization","summary":"  In recent years, tensor decomposition-based approaches for hyperspectral\nanomaly detection (HAD) have gained significant attention in the field of\nremote sensing. However, existing methods often fail to fully leverage both the\nglobal correlations and local smoothness of the background components in\nhyperspectral images (HSIs), which exist in both the spectral and spatial\ndomains. This limitation results in suboptimal detection performance. To\nmitigate this critical issue, we put forward a novel HAD method named\nHAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)\nfactors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first\ndecomposed into background and anomaly components. The TR decomposition is then\nemployed to capture the spatial-spectral correlations within the background\ncomponent. Additionally, we introduce a unified and efficient nonconvex\nregularizer, induced by tensor singular value decomposition (TSVD), to\nsimultaneously encode the low-rankness and sparsity of the 3-D gradient TR\nfactors into a unique concise form. The above characterization scheme enables\nthe interpretable gradient TR factors to inherit the low-rankness and\nsmoothness of the original background. To further enhance anomaly detection, we\ndesign a generalized nonconvex regularization term to exploit the group\nsparsity of the anomaly component. To solve the resulting doubly nonconvex\nmodel, we develop a highly efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) framework. Experimental\nresults on several benchmark datasets demonstrate that our proposed method\noutperforms existing state-of-the-art (SOTA) approaches in terms of detection\naccuracy.\n","authors":["Wenjin Qin","Hailin Wang","Hao Shu","Feng Zhang","Jianjun Wang","Xiangyong Cao","Xi-Le Zhao","Gemine Vivone"],"pdf_url":"https://arxiv.org/pdf/2505.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17877v1","updated":"2025-05-23T13:27:35Z","published":"2025-05-23T13:27:35Z","title":"Toward Optimal ANC: Establishing Mutual Information Lower Bound","summary":"  Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic\ndisturbances by generating anti-noise signals that destructively interfere with\nthe original noise in real time. Although recent deep learning-based ANC\nalgorithms have set new performance benchmarks, there remains a shortage of\ntheoretical limits to rigorously assess their improvements. To address this, we\nderive a unified lower bound on cancellation performance composed of two\ncomponents. The first component is information-theoretic: it links residual\nerror power to the fraction of disturbance entropy captured by the anti-noise\nsignal, thereby quantifying limits imposed by information-processing capacity.\nThe second component is support-based: it measures the irreducible error\narising in frequency bands that the cancellation path cannot address,\nreflecting fundamental physical constraints. By taking the maximum of these two\nterms, our bound establishes a theoretical ceiling on the Normalized Mean\nSquared Error (NMSE) attainable by any ANC algorithm. We validate its tightness\nempirically on the NOISEX dataset under varying reverberation times,\ndemonstrating robustness across diverse acoustic conditions.\n","authors":["Franois Derrida","Shahar Lutati","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2505.17877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15618v4","updated":"2025-05-23T13:25:56Z","published":"2024-10-21T03:40:29Z","title":"Erasing Undesirable Concepts in Diffusion Models with Adversarial\n  Preservation","summary":"  Diffusion models excel at generating visually striking content from text but\ncan inadvertently produce undesirable or harmful content when trained on\nunfiltered internet data. A practical solution is to selectively removing\ntarget concepts from the model, but this may impact the remaining concepts.\nPrior approaches have tried to balance this by introducing a loss term to\npreserve neutral content or a regularization term to minimize changes in the\nmodel parameters, yet resolving this trade-off remains challenging. In this\nwork, we propose to identify and preserving concepts most affected by parameter\nchanges, termed as \\textit{adversarial concepts}. This approach ensures stable\nerasure with minimal impact on the other concepts. We demonstrate the\neffectiveness of our method using the Stable Diffusion model, showing that it\noutperforms state-of-the-art erasure methods in eliminating unwanted content\nwhile maintaining the integrity of other unrelated elements. Our code is\navailable at https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.\n","authors":["Anh Bui","Long Vuong","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2410.15618v4.pdf","comment":"Erasing Concepts, Generative Unlearning, NeurIPS 2024. arXiv admin\n  note: text overlap with arXiv:2403.12326"},{"id":"http://arxiv.org/abs/2505.17875v1","updated":"2025-05-23T13:25:41Z","published":"2025-05-23T13:25:41Z","title":"Semi-Supervised Multi-Label Feature Selection with Consistent Sparse\n  Graph Learning","summary":"  In practical domains, high-dimensional data are usually associated with\ndiverse semantic labels, whereas traditional feature selection methods are\ndesigned for single-label data. Moreover, existing multi-label methods\nencounter two main challenges in semi-supervised scenarios: (1). Most\nsemi-supervised methods fail to evaluate the label correlations without enough\nlabeled samples, which are the critical information of multi-label feature\nselection, making label-specific features discarded. (2). The similarity graph\nstructure directly derived from the original feature space is suboptimal for\nmulti-label problems in existing graph-based methods, leading to unreliable\nsoft labels and degraded feature selection performance. To overcome them, we\npropose a consistent sparse graph learning method for multi-label\nsemi-supervised feature selection (SGMFS), which can enhance the feature\nselection performance by maintaining space consistency and learning label\ncorrelations in semi-supervised scenarios. Specifically, for Challenge (1),\nSGMFS learns a low-dimensional and independent label subspace from the\nprojected features, which can compatibly cross multiple labels and effectively\nachieve the label correlations. For Challenge (2), instead of constructing a\nfixed similarity graph for semi-supervised learning, SGMFS thoroughly explores\nthe intrinsic structure of the data by performing sparse reconstruction of\nsamples in both the label space and the learned subspace simultaneously. In\nthis way, the similarity graph can be adaptively learned to maintain the\nconsistency between label space and the learned subspace, which can promote\npropagating proper soft labels for unlabeled samples, facilitating the ultimate\nfeature selection. An effective solution with fast convergence is designed to\noptimize the objective function. Extensive experiments validate the superiority\nof SGMFS.\n","authors":["Yan Zhong","Xingyu Wu","Xinping Zhao","Li Zhang","Xinyuan Song","Lei Shi","Bingbing Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.17875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17872v1","updated":"2025-05-23T13:24:39Z","published":"2025-05-23T13:24:39Z","title":"Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time\n  Series Forecasting","summary":"  Multi-task forecasting has become the standard approach for time-series\nforecasting (TSF). However, we show that it suffers from an Expressiveness\nBottleneck, where predictions at different time steps share the same\nrepresentation, leading to unavoidable errors even with optimal\nrepresentations. To address this issue, we propose a two-stage framework:\nfirst, pre-train a foundation model for one-step-ahead prediction; then, adapt\nit using step-specific LoRA modules.This design enables the foundation model to\nhandle any number of forecast steps while avoiding the expressiveness\nbottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which\nemploys adaptively weighted LoRA experts to achieve partial parameter sharing\nacross steps. This approach enhances both efficiency and forecasting\nperformance by exploiting interdependencies between forecast steps. Experiments\nshow that MoLA significantly improves model expressiveness and outperforms\nstate-of-the-art time-series forecasting methods. Code is available at\nhttps://anonymous.4open.science/r/MoLA-BC92.\n","authors":["Licheng Pan","Zhichao Chen","Haoxuan Li","Guangyi Liu","Zhijian Xu","Zhaoran Liu","Hao Wang","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2505.17872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18950v3","updated":"2025-05-23T13:23:21Z","published":"2025-01-31T08:17:23Z","title":"Fantastic Targets for Concept Erasure in Diffusion Models and Where To\n  Find Them","summary":"  Concept erasure has emerged as a promising technique for mitigating the risk\nof harmful content generation in diffusion models by selectively unlearning\nundesirable concepts. The common principle of previous works to remove a\nspecific concept is to map it to a fixed generic concept, such as a neutral\nconcept or just an empty text prompt. In this paper, we demonstrate that this\nfixed-target strategy is suboptimal, as it fails to account for the impact of\nerasing one concept on the others. To address this limitation, we model the\nconcept space as a graph and empirically analyze the effects of erasing one\nconcept on the remaining concepts. Our analysis uncovers intriguing geometric\nproperties of the concept space, where the influence of erasing a concept is\nconfined to a local region. Building on this insight, we propose the Adaptive\nGuided Erasure (AGE) method, which \\emph{dynamically} selects optimal target\nconcepts tailored to each undesirable concept, minimizing unintended side\neffects. Experimental results show that AGE significantly outperforms\nstate-of-the-art erasure methods on preserving unrelated concepts while\nmaintaining effective erasure performance. Our code is published at\n{https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.\n","authors":["Anh Bui","Trang Vu","Long Vuong","Trung Le","Paul Montague","Tamas Abraham","Junae Kim","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2501.18950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13052v2","updated":"2025-05-23T13:21:08Z","published":"2025-05-19T12:41:19Z","title":"Model Selection for Gaussian-gated Gaussian Mixture of Experts Using\n  Dendrograms of Mixing Measures","summary":"  Mixture of Experts (MoE) models constitute a widely utilized class of\nensemble learning approaches in statistics and machine learning, known for\ntheir flexibility and computational efficiency. They have become integral\ncomponents in numerous state-of-the-art deep neural network architectures,\nparticularly for analyzing heterogeneous data across diverse domains. Despite\ntheir practical success, the theoretical understanding of model selection,\nespecially concerning the optimal number of mixture components or experts,\nremains limited and poses significant challenges. These challenges primarily\nstem from the inclusion of covariates in both the Gaussian gating functions and\nexpert networks, which introduces intrinsic interactions governed by partial\ndifferential equations with respect to their parameters. In this paper, we\nrevisit the concept of dendrograms of mixing measures and introduce a novel\nextension to Gaussian-gated Gaussian MoE models that enables consistent\nestimation of the true number of mixture components and achieves the pointwise\noptimal convergence rate for parameter estimation in overfitted scenarios.\nNotably, this approach circumvents the need to train and compare a range of\nmodels with varying numbers of components, thereby alleviating the\ncomputational burden, particularly in high-dimensional or deep neural network\nsettings. Experimental results on synthetic data demonstrate the effectiveness\nof the proposed method in accurately recovering the number of experts. It\noutperforms common criteria such as the Akaike information criterion, the\nBayesian information criterion, and the integrated completed likelihood, while\nachieving optimal convergence rates for parameter estimation and accurately\napproximating the regression function.\n","authors":["Tuan Thai","TrungTin Nguyen","Dat Do","Nhat Ho","Christopher Drovandi"],"pdf_url":"https://arxiv.org/pdf/2505.13052v2.pdf","comment":"Correct typos and update the numerical experiments. Tuan Thai and\n  TrungTin Nguyen are co-first authors"},{"id":"http://arxiv.org/abs/2505.17871v1","updated":"2025-05-23T13:20:47Z","published":"2025-05-23T13:20:47Z","title":"BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting\n  Models","summary":"  The advent of universal time series forecasting models has revolutionized\nzero-shot forecasting across diverse domains, yet the critical role of data\ndiversity in training these models remains underexplored. Existing large-scale\ntime series datasets often suffer from inherent biases and imbalanced\ndistributions, leading to suboptimal model performance and generalization. To\naddress this gap, we introduce BLAST, a novel pre-training corpus designed to\nenhance data diversity through a balanced sampling strategy. First, BLAST\nincorporates 321 billion observations from publicly available datasets and\nemploys a comprehensive suite of statistical metrics to characterize time\nseries patterns. Then, to facilitate pattern-oriented sampling, the data is\nimplicitly clustered using grid-based partitioning. Furthermore, by integrating\ngrid sampling and grid mixup techniques, BLAST ensures a balanced and\nrepresentative coverage of diverse patterns. Experimental results demonstrate\nthat models pre-trained on BLAST achieve state-of-the-art performance with a\nfraction of the computational resources and training tokens required by\nexisting methods. Our findings highlight the pivotal role of data diversity in\nimproving both training efficiency and model performance for the universal\nforecasting task.\n","authors":["Zezhi Shao","Yujie Li","Fei Wang","Chengqing Yu","Yisong Fu","Tangwen Qian","Bin Xu","Boyu Diao","Yongjun Xu","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.17871v1.pdf","comment":"Accepted by SIGKDD 2025 (Research Track)"},{"id":"http://arxiv.org/abs/2411.11171v3","updated":"2025-05-23T13:18:16Z","published":"2024-11-17T20:44:34Z","title":"LLMmlein: Compact and Competitive German-Only Language Models from\n  Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v3.pdf","comment":"camera ready;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2505.17869v1","updated":"2025-05-23T13:16:59Z","published":"2025-05-23T13:16:59Z","title":"Best Group Identification in Multi-Objective Bandits","summary":"  We introduce the Best Group Identification problem in a multi-objective\nmulti-armed bandit setting, where an agent interacts with groups of arms with\nvector-valued rewards. The performance of a group is determined by an\nefficiency vector which represents the group's best attainable rewards across\ndifferent dimensions. The objective is to identify the set of optimal groups in\nthe fixed-confidence setting. We investigate two key formulations: group Pareto\nset identification, where efficiency vectors of optimal groups are Pareto\noptimal and linear best group identification, where each reward dimension has a\nknown weight and the optimal group maximizes the weighted sum of its efficiency\nvector's entries. For both settings, we propose elimination-based algorithms,\nestablish upper bounds on their sample complexity, and derive lower bounds that\napply to any correct algorithm. Through numerical experiments, we demonstrate\nthe strong empirical performance of the proposed algorithms.\n","authors":["Mohammad Shahverdikondori","Mohammad Reza Badri","Negar Kiyavash"],"pdf_url":"https://arxiv.org/pdf/2505.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17868v1","updated":"2025-05-23T13:16:54Z","published":"2025-05-23T13:16:54Z","title":"SpectraLDS: Provable Distillation for Linear Dynamical Systems","summary":"  We present the first provable method for identifying symmetric linear\ndynamical systems (LDS) with accuracy guarantees that are independent of the\nsystems' state dimension or effective memory. Our approach builds upon recent\nwork that represents symmetric LDSs as convolutions learnable via fixed\nspectral transformations. We show how to invert this representation, thereby\nrecovering an LDS model from its spectral transform and yielding an end-to-end\nconvex optimization procedure. This distillation preserves predictive accuracy\nwhile enabling constant-time and constant-space inference per token,\nindependent of sequence length. We evaluate our method, SpectraLDS, as a\ncomponent in sequence prediction architectures and demonstrate that accuracy is\npreserved while inference efficiency is improved on tasks such as language\nmodeling.\n","authors":["Devan Shah","Shlomo Fortgang","Sofiia Druchyna","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2505.17868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17866v1","updated":"2025-05-23T13:16:01Z","published":"2025-05-23T13:16:01Z","title":"DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization","summary":"  Designing effective black-box optimizers is hampered by limited\nproblem-specific knowledge and manual control that spans months for almost\nevery detail. In this paper, we present DesignX, the first automated algorithm\ndesign framework that generates an effective optimizer specific to a given\nblack-box optimization problem within seconds. Rooted in the first principles,\nwe identify two key sub-tasks: 1) algorithm structure generation and 2)\nhyperparameter control. To enable systematic construction, a comprehensive\nmodular algorithmic space is first built, embracing hundreds of algorithm\ncomponents collected from decades of research. We then introduce a dual-agent\nreinforcement learning system that collaborates on structural and parametric\ndesign through a novel cooperative training objective, enabling large-scale\nmeta-training across 10k diverse instances. Remarkably, through days of\nautonomous learning, the DesignX-generated optimizers continuously surpass\nhuman-crafted optimizers by orders of magnitude, either on synthetic testbed or\non realistic optimization scenarios such as Protein-docking, AutoML and UAV\npath planning. Further in-depth analysis reveals DesignX's capability to\ndiscover non-trivial algorithm patterns beyond expert intuition, which,\nconversely, provides valuable design insights for the optimization community.\nWe provide DesignX's inference code at https://github.com/MetaEvo/DesignX.\n","authors":["Hongshu Guo","Zeyuan Ma","Yining Ma","Xinglin Zhang","Wei-Neng Chen","Yue-Jiao Gong"],"pdf_url":"https://arxiv.org/pdf/2505.17866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17863v1","updated":"2025-05-23T13:14:02Z","published":"2025-05-23T13:14:02Z","title":"The emergence of sparse attention: impact of data distribution and\n  benefits of repetition","summary":"  Emergence is a fascinating property of large language models and neural\nnetworks more broadly: as models scale and train for longer, they sometimes\ndevelop new abilities in sudden ways. Despite initial studies, we still lack a\ncomprehensive understanding of how and when these abilities emerge. To address\nthis gap, we study the emergence over training of sparse attention, a critical\nand frequently observed attention pattern in Transformers. By combining\ntheoretical analysis of a toy model with empirical observations on small\nTransformers trained on a linear regression variant, we uncover the mechanics\ndriving sparse attention emergence and reveal that emergence timing follows\npower laws based on task structure, architecture, and optimizer choice. We\nadditionally find that repetition can greatly speed up emergence. Finally, we\nconfirm these results on a well-studied in-context associative recall task. Our\nfindings provide a simple, theoretically grounded framework for understanding\nhow data distributions and model design influence the learning dynamics behind\none form of emergence.\n","authors":["Nicolas Zucchet","Francesco d'Angelo","Andrew K. Lampinen","Stephanie C. Y. Chan"],"pdf_url":"https://arxiv.org/pdf/2505.17863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17860v1","updated":"2025-05-23T13:13:00Z","published":"2025-05-23T13:13:00Z","title":"Multi-Person Interaction Generation from Two-Person Motion Priors","summary":"  Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.\n","authors":["Wenning Xu","Shiyu Fan","Paul Henderson","Edmond S. L. Ho"],"pdf_url":"https://arxiv.org/pdf/2505.17860v1.pdf","comment":"SIGGRAPH 2025 Conference Papers"},{"id":"http://arxiv.org/abs/2505.17859v1","updated":"2025-05-23T13:12:37Z","published":"2025-05-23T13:12:37Z","title":"Scalable Valuation of Human Feedback through Provably Robust Model\n  Alignment","summary":"  Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods.\n","authors":["Masahiro Fujisawa","Masaki Adachi","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2505.17859v1.pdf","comment":"38 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.11312v2","updated":"2025-05-23T13:12:15Z","published":"2025-05-16T14:38:30Z","title":"Where You Place the Norm Matters: From Prejudiced to Neutral\n  Initializations","summary":"  Normalization layers, such as Batch Normalization and Layer Normalization,\nare central components in modern neural networks, widely adopted to improve\ntraining stability and generalization. While their practical effectiveness is\nwell documented, a detailed theoretical understanding of how normalization\naffects model behavior, starting from initialization, remains an important open\nquestion. In this work, we investigate how both the presence and placement of\nnormalization within hidden layers influence the statistical properties of\nnetwork predictions before training begins. In particular, we study how these\nchoices shape the distribution of class predictions at initialization, which\ncan range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a\nsubset of classes. Our analysis shows that normalization placement induces\nsystematic differences in the initial prediction behavior of neural networks,\nwhich in turn shape the dynamics of learning. By linking architectural choices\nto prediction statistics at initialization, our work provides a principled\nunderstanding of how normalization can influence early training behavior and\noffers guidance for more controlled and interpretable network design.\n","authors":["Emanuele Francazi","Francesco Pinto","Aurelien Lucchi","Marco Baity-Jesi"],"pdf_url":"https://arxiv.org/pdf/2505.11312v2.pdf","comment":"Fixed a Typo in Fig.1 and refined the Appendix"},{"id":"http://arxiv.org/abs/2502.01406v2","updated":"2025-05-23T13:10:08Z","published":"2025-02-03T14:38:27Z","title":"GRADIEND: Monosemantic Feature Learning within Neural Networks Applied\n  to Gender Debiasing of Transformer Models","summary":"  AI systems frequently exhibit and amplify social biases, including gender\nbias, leading to harmful consequences in critical areas. This study introduces\na novel encoder-decoder approach that leverages model gradients to learn a\nsingle monosemantic feature neuron encoding gender information. We show that\nour method can be used to debias transformer-based language models, while\nmaintaining other capabilities. We demonstrate the effectiveness of our\napproach across various model architectures and highlight its potential for\nbroader applications.\n","authors":["Jonathan Drechsel","Steffen Herbold"],"pdf_url":"https://arxiv.org/pdf/2502.01406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17856v1","updated":"2025-05-23T13:07:18Z","published":"2025-05-23T13:07:18Z","title":"Stochastic Weight Sharing for Bayesian Neural Networks","summary":"  While offering a principled framework for uncertainty quantification in deep\nlearning, the employment of Bayesian Neural Networks (BNNs) is still\nconstrained by their increased computational requirements and the convergence\ndifficulties when training very deep, state-of-the-art architectures. In this\nwork, we reinterpret weight-sharing quantization techniques from a stochastic\nperspective in the context of training and inference with Bayesian Neural\nNetworks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,\nWasserstein distance estimations, and alpha blending to encode the stochastic\nbehaviour of a BNN in a lower dimensional, soft Gaussian representation.\nThrough extensive empirical investigation, we demonstrate that our approach\nsignificantly reduces the computational overhead inherent in Bayesian learning\nby several orders of magnitude, enabling the efficient Bayesian training of\nlarge-scale models, such as ResNet-101 and Vision Transformer (VIT). On various\ncomputer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our\napproach compresses model parameters by approximately 50x and reduces model\nsize by 75, while achieving accuracy and uncertainty estimations comparable to\nthe state-of-the-art.\n","authors":["Moule Lin","Shuhao Guan","Weipeng Jing","Goetz Botterweck","Andrea Patane"],"pdf_url":"https://arxiv.org/pdf/2505.17856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17854v1","updated":"2025-05-23T13:05:07Z","published":"2025-05-23T13:05:07Z","title":"Out of the Shadows: Exploring a Latent Space for Neural Network\n  Verification","summary":"  Neural networks are ubiquitous. However, they are often sensitive to small\ninput changes. Hence, to prevent unexpected behavior in safety-critical\napplications, their formal verification -- a notoriously hard problem -- is\nnecessary. Many state-of-the-art verification algorithms use reachability\nanalysis or abstract interpretation to enclose the set of possible outputs of a\nneural network. Often, the verification is inconclusive due to the conservatism\nof the enclosure. To address this problem, we design a novel latent space for\nformal verification that enables the transfer of output specifications to the\ninput space for an iterative specification-driven input refinement, i.e., we\niteratively reduce the set of possible inputs to only enclose the unsafe ones.\nThe latent space is constructed from a novel view of projection-based set\nrepresentations, e.g., zonotopes, which are commonly used in reachability\nanalysis of neural networks. A projection-based set representation is a\n\"shadow\" of a higher-dimensional set -- a latent space -- that does not change\nduring a set propagation through a neural network. Hence, the input set and the\noutput enclosure are \"shadows\" of the same latent space that we can use to\ntransfer constraints. We present an efficient verification tool for neural\nnetworks that uses our iterative refinement to significantly reduce the number\nof subproblems in a branch-and-bound procedure. Using zonotopes as a set\nrepresentation, unlike many other state-of-the-art approaches, our approach can\nbe realized by only using matrix operations, which enables a significant\nspeed-up through efficient GPU acceleration. We demonstrate that our tool\nachieves competitive performance, which would place it among the top-ranking\ntools of the last neural network verification competition (VNN-COMP'24).\n","authors":["Lukas Koller","Tobias Ladner","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2505.17854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17852v1","updated":"2025-05-23T13:04:06Z","published":"2025-05-23T13:04:06Z","title":"Scaling Recurrent Neural Networks to a Billion Parameters with\n  Zero-Order Optimization","summary":"  During inference, Recurrent Neural Networks (RNNs) scale constant in both\nFLOPs and GPU memory with increasing context length, as they compress all prior\ntokens into a fixed-size memory. In contrast, transformers scale linearly in\nFLOPs and, at best, linearly in memory during generation, since they must\nattend to all previous tokens explicitly. Despite this inference-time\nadvantage, training large RNNs on long contexts remains impractical because\nstandard optimization methods depend on Backpropagation Through Time (BPTT).\nBPTT requires retention of all intermediate activations during the forward\npass, causing memory usage to scale linearly with both context length and model\nsize. In this paper, we show that Zero-Order Optimization (ZOO) methods such as\nRandom-vector Gradient Estimation (RGE) can successfully replace BPTT to train\nRNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while\nusing orders of magnitude less memory and cost, as the model remains in\ninference mode throughout training. We further demonstrate that\nCentral-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate\nloss, inherently regularizing training and improving generalization. Our method\nmatches or outperforms BPTT across three settings: (1) overfitting, (2)\ntransduction, and (3) language modeling. Across all tasks, with sufficient\nperturbations, our models generalize as well as or better than those trained\nwith BPTT, often in fewer steps. Despite the need for more forward passes per\nstep, we can surpass BPTT wall-clock time per step using recent advancements\nsuch as FlashRNN and distributed inference.\n","authors":["Francois Chaubard","Mykel Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2505.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17847v1","updated":"2025-05-23T13:00:35Z","published":"2025-05-23T13:00:35Z","title":"TransDF: Time-Series Forecasting Needs Transformed Label Alignment","summary":"  Training time-series forecasting models presents unique challenges in\ndesigning effective learning objectives. Existing methods predominantly utilize\nthe temporal mean squared error, which faces two critical challenges: (1) label\nautocorrelation, which leads to bias from the label sequence likelihood; (2)\nexcessive amount of tasks, which increases with the forecast horizon and\ncomplicates optimization. To address these challenges, we propose\nTransform-enhanced Direct Forecast (TransDF), which transforms the label\nsequence into decorrelated components with discriminated significance. Models\nare trained to align the most significant components, thereby effectively\nmitigating label autocorrelation and reducing task amount. Extensive\nexperiments demonstrate that TransDF achieves state-of-the-art performance and\nis compatible with various forecasting models. Code is available at\nhttps://anonymous.4open.science/r/TransDF-88CF.\n","authors":["Hao Wang","Licheng Pan","Zhichao Chen","Xu Chen","Qingyang Dai","Lei Wang","Haoxuan Li","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2505.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11615v2","updated":"2025-05-23T12:56:55Z","published":"2025-03-14T17:35:00Z","title":"From Score Matching to Diffusion: A Fine-Grained Error Analysis in the\n  Gaussian Setting","summary":"  Sampling from an unknown distribution, accessible only through discrete\nsamples, is a fundamental problem at the core of generative AI. The current\nstate-of-the-art methods follow a two-step process: first, estimating the score\nfunction (the gradient of a smoothed log-distribution) and then applying a\ndiffusion-based sampling algorithm -- such as Langevin or Diffusion models. The\nresulting distribution's correctness can be impacted by four major factors: the\ngeneralization and optimization errors in score matching, and the\ndiscretization and minimal noise amplitude in the diffusion. In this paper, we\nmake the sampling error explicit when using a diffusion sampler in the Gaussian\nsetting. We provide a sharp analysis of the Wasserstein sampling error that\narises from these four error sources. This allows us to rigorously track how\nthe anisotropy of the data distribution (encoded by its power spectrum)\ninteracts with key parameters of the end-to-end sampling method, including the\nnumber of initial samples, the stepsizes in both score matching and diffusion,\nand the noise amplitude. Notably, we show that the Wasserstein sampling error\ncan be expressed as a kernel-type norm of the data power spectrum, where the\nspecific kernel depends on the method parameters. This result provides a\nfoundation for further analysis of the tradeoffs involved in optimizing\nsampling accuracy.\n","authors":["Samuel Hurault","Matthieu Terris","Thomas Moreau","Gabriel Peyr"],"pdf_url":"https://arxiv.org/pdf/2503.11615v2.pdf","comment":"59 pages"},{"id":"http://arxiv.org/abs/2505.17838v1","updated":"2025-05-23T12:52:54Z","published":"2025-05-23T12:52:54Z","title":"Continuum Transformers Perform In-Context Learning by Operator Gradient\n  Descent","summary":"  Transformers robustly exhibit the ability to perform in-context learning,\nwhereby their predictive accuracy on a task can increase not by parameter\nupdates but merely with the placement of training samples in their context\nwindows. Recent works have shown that transformers achieve this by implementing\ngradient descent in their forward passes. Such results, however, are restricted\nto standard transformer architectures, which handle finite-dimensional inputs.\nIn the space of PDE surrogate modeling, a generalization of transformers to\nhandle infinite-dimensional function inputs, known as \"continuum transformers,\"\nhas been proposed and similarly observed to exhibit in-context learning.\nDespite impressive empirical performance, such in-context learning has yet to\nbe theoretically characterized. We herein demonstrate that continuum\ntransformers perform in-context operator learning by performing gradient\ndescent in an operator RKHS. We demonstrate this using novel proof strategies\nthat leverage a generalized representer theorem for Hilbert spaces and gradient\nflows over the space of functionals of a Hilbert space. We additionally show\nthe operator learned in context is the Bayes Optimal Predictor in the infinite\ndepth limit of the transformer. We then provide empirical validations of this\noptimality result and demonstrate that the parameters under which such gradient\ndescent is performed are recovered through the continuum transformer training.\n","authors":["Abhiti Mishra","Yash Patel","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2505.17838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17836v1","updated":"2025-05-23T12:51:03Z","published":"2025-05-23T12:51:03Z","title":"Robust Distributed Estimation: Extending Gossip Algorithms to Ranking\n  and Trimmed Means","summary":"  This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes.\n","authors":["Anna Van Elst","Igor Colin","Stephan Clmenon"],"pdf_url":"https://arxiv.org/pdf/2505.17836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17834v1","updated":"2025-05-23T12:48:35Z","published":"2025-05-23T12:48:35Z","title":"Hybrid Mamba-Transformer Decoder for Error-Correcting Codes","summary":"  We introduce a novel deep learning method for decoding error correction codes\nbased on the Mamba architecture, enhanced with Transformer layers. Our approach\nproposes a hybrid decoder that leverages Mamba's efficient sequential modeling\nwhile maintaining the global context capabilities of Transformers. To further\nimprove performance, we design a novel layer-wise masking strategy applied to\neach Mamba layer, allowing selective attention to relevant code features at\ndifferent depths. Additionally, we introduce a progressive layer-wise loss,\nsupervising the network at intermediate stages and promoting robust feature\nextraction throughout the decoding process. Comprehensive experiments across a\nrange of linear codes demonstrate that our method significantly outperforms\nTransformer-only decoders and standard Mamba models.\n","authors":["Shy-el Cohen","Yoni Choukroun","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2505.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17833v1","updated":"2025-05-23T12:47:21Z","published":"2025-05-23T12:47:21Z","title":"Investigating Affect Mining Techniques for Annotation Sample Selection\n  in the Creation of Finnish Affective Speech Corpus","summary":"  Study of affect in speech requires suitable data, as emotional expression and\nperception vary across languages. Until now, no corpus has existed for natural\nexpression of affect in spontaneous Finnish, existing data being acted or from\na very specific communicative setting. This paper presents the first such\ncorpus, created by annotating 12,000 utterances for emotional arousal and\nvalence, sampled from three large-scale Finnish speech corpora. To ensure\ndiverse affective expression, sample selection was conducted with an affect\nmining approach combining acoustic, cross-linguistic speech emotion, and text\nsentiment features. We compare this method to random sampling in terms of\nannotation diversity, and conduct post-hoc analyses to identify sampling\nchoices that would have maximized the diversity. As an outcome, the work\nintroduces a spontaneous Finnish affective speech corpus and informs sampling\nstrategies for affective speech corpus creation in other languages or domains.\n","authors":["Kalle Lahtinen","Einari Vaaras","Liisa Mustanoja","Okko Rsnen"],"pdf_url":"https://arxiv.org/pdf/2505.17833v1.pdf","comment":"Accepted for publication at Interspeech 2025, Rotterdam, The\n  Netherlands"},{"id":"http://arxiv.org/abs/2505.17830v1","updated":"2025-05-23T12:43:55Z","published":"2025-05-23T12:43:55Z","title":"Imagine Beyond! Distributionally Robust Auto-Encoding for State Space\n  Coverage in Online Reinforcement Learning","summary":"  Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.\n","authors":["Nicolas Castanet","Olivier Sigaud","Sylvain Lamprier"],"pdf_url":"https://arxiv.org/pdf/2505.17830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19099v2","updated":"2025-05-23T12:41:11Z","published":"2025-01-31T12:46:04Z","title":"Elucidating Subspace Perturbation in Zeroth-Order Optimization: Theory\n  and Practice at Scale","summary":"  Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods often suffer\nfrom slow convergence due to high-variance stochastic gradient estimators.\nWhile subspace perturbations, such as sparsity and low-rank constraints, have\nbeen explored to mitigate this issue, their effectiveness remains poorly\nunderstood. In this work, we develop a \\emph{unified theoretical framework}\nthat analyzes both the convergence and generalization properties of ZO\noptimization under subspace perturbations. We show that high dimensionality is\nthe primary bottleneck and introduce the notion of \\textit{subspace alignment}\nto explain how the subspace perturbations reduce gradient noise and accelerate\nconvergence. Our analysis further shows that a broad class of subspace\nperturbations exhibits a similar convergence rate, motivating us to prioritize\npractical considerations in real-world algorithm design. Building on these\ninsights, we propose an efficient ZO method using block coordinate descent\n(MeZO-BCD), which perturbs and updates only a subset of parameters at each\nstep. Extensive experiments show that MeZO-BCD significantly accelerates\noptimization, achieving up to $\\mathbf{\\times2.77}$ speedup in wall-clock time\nover MeZO on OPT-13B, while maintaining comparable iteration complexity and\nfine-tuning performance.\n","authors":["Sihwan Park","Jihun Yun","SungYub Kim","Souvik Kundu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2501.19099v2.pdf","comment":"49 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.17826v1","updated":"2025-05-23T12:41:09Z","published":"2025-05-23T12:41:09Z","title":"Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models","summary":"  Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.\n","authors":["Xuchen Pan","Yanxi Chen","Yushuo Chen","Yuchang Sun","Daoyuan Chen","Wenhao Zhang","Yuexiang Xie","Yilun Huang","Yilei Zhang","Dawei Gao","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17826v1.pdf","comment":"This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT"},{"id":"http://arxiv.org/abs/2505.17823v1","updated":"2025-05-23T12:39:23Z","published":"2025-05-23T12:39:23Z","title":"Source Separation of Small Classical Ensembles: Challenges and\n  Opportunities","summary":"  Musical (MSS) source separation of western popular music using non-causal\ndeep learning can be very effective. In contrast, MSS for classical music is an\nunsolved problem. Classical ensembles are harder to separate than popular music\nbecause of issues such as the inherent greater variation in the music; the\nsparsity of recordings with ground truth for supervised training; and greater\nambiguity between instruments. The Cadenza project has been exploring MSS for\nclassical music. This is being done so music can be remixed to improve\nlistening experiences for people with hearing loss. To enable the work, a new\ndatabase of synthesized woodwind ensembles was created to overcome instrumental\nimbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used\nwith each model being trained to extract a string or woodwind instrument.\nConvTasNet was chosen because it enabled both causal and non-causal approaches\nto be tested. Non-causal approaches have dominated MSS work and are useful for\nrecorded music, but for live music or processing on hearing aids, causal signal\nprocessing is needed. The MSS performance was evaluated on the two small\ndatasets (Bach10 and URMP) of real instrument recordings where the ground-truth\nis available. The performances of the causal and non-causal systems were\nsimilar. Comparing the average Signal-to-Distortion (SDR) of the synthesized\nvalidation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation\nset (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized\nand recorded data is a problem. Future work needs to either gather more real\nrecordings that can be used for training, or to improve the realism and\ndiversity of the synthesized recordings to reduce the mismatch...\n","authors":["Gerardo Roa-Dabike","Trevor J. Cox","Jon P. Barker","Michael A. Akeroyd","Scott Bannister","Bruno Fazenda","Jennifer Firth","Simone Graetzer","Alinka Greasley","Rebecca R. Vos","William M. Whitmer"],"pdf_url":"https://arxiv.org/pdf/2505.17823v1.pdf","comment":"5 pages, 4 figures, 2 tables, submitted to WASSPA 2025"},{"id":"http://arxiv.org/abs/2505.17819v1","updated":"2025-05-23T12:35:14Z","published":"2025-05-23T12:35:14Z","title":"Quantifying uncertainty in spectral clusterings: expectations for\n  perturbed and incomplete data","summary":"  Spectral clustering is a popular unsupervised learning technique which is\nable to partition unlabelled data into disjoint clusters of distinct shapes.\nHowever, the data under consideration are often experimental data, implying\nthat the data is subject to measurement errors and measurements may even be\nlost or invalid. These uncertainties in the corrupted input data induce\ncorresponding uncertainties in the resulting clusters, and the clusterings thus\nbecome unreliable.\n  Modelling the uncertainties as random processes, we discuss a mathematical\nframework based on random set theory for the computational Monte Carlo\napproximation of statistically expected clusterings in case of corrupted, i.e.,\nperturbed, incomplete, and possibly even additional, data. We propose several\ncomputationally accessible quantities of interest and analyze their consistency\nin the infinite data point and infinite Monte Carlo sample limit. Numerical\nexperiments are provided to illustrate and compare the proposed quantities.\n","authors":["Jrgen Dlz","Jolanda Weygandt"],"pdf_url":"https://arxiv.org/pdf/2505.17819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17810v1","updated":"2025-05-23T12:28:10Z","published":"2025-05-23T12:28:10Z","title":"VIBE: Vector Index Benchmark for Embeddings","summary":"  Approximate nearest neighbor (ANN) search is a performance-critical component\nof many machine learning pipelines. Rigorous benchmarking is essential for\nevaluating the performance of vector indexes for ANN search. However, the\ndatasets of the existing benchmarks are no longer representative of the current\napplications of ANN search. Hence, there is an urgent need for an up-to-date\nset of benchmarks. To this end, we introduce Vector Index Benchmark for\nEmbeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE\ncontains a pipeline for creating benchmark datasets using dense embedding\nmodels characteristic of modern applications, such as retrieval-augmented\ngeneration (RAG). To replicate real-world workloads, we also include\nout-of-distribution (OOD) datasets where the queries and the corpus are drawn\nfrom different distributions. We use VIBE to conduct a comprehensive evaluation\nof SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution\nand 6 out-of-distribution datasets.\n","authors":["Elias Jsaari","Ville Hyvnen","Matteo Ceccarello","Teemu Roos","Martin Aumller"],"pdf_url":"https://arxiv.org/pdf/2505.17810v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.01027v2","updated":"2025-05-23T12:24:23Z","published":"2025-02-03T03:44:35Z","title":"Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and\n  Guarantees","summary":"  Two-stage Learning-to-Defer (L2D) enables optimal task delegation by\nassigning each input to either a fixed main model or one of several offline\nexperts, supporting reliable decision-making in complex, multi-agent\nenvironments. However, existing L2D frameworks assume clean inputs and are\nvulnerable to adversarial perturbations that can manipulate query\nallocation--causing costly misrouting or expert overload. We present the first\ncomprehensive study of adversarial robustness in two-stage L2D systems. We\nintroduce two novel attack strategie--untargeted and targeted--which\nrespectively disrupt optimal allocations or force queries to specific agents.\nTo defend against such threats, we propose SARD, a convex learning algorithm\nbuilt on a family of surrogate losses that are provably Bayes-consistent and\n$(\\mathcal{R}, \\mathcal{G})$-consistent. These guarantees hold across\nclassification, regression, and multi-task settings. Empirical results\ndemonstrate that SARD significantly improves robustness under adversarial\nattacks while maintaining strong clean performance, marking a critical step\ntoward secure and trustworthy L2D deployment.\n","authors":["Yannis Montreuil","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2502.01027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17804v1","updated":"2025-05-23T12:21:19Z","published":"2025-05-23T12:21:19Z","title":"Hyperparameter Optimization via Interacting with Probabilistic Circuits","summary":"  Despite the growing interest in designing truly interactive hyperparameter\noptimization (HPO) methods, to date, only a few allow to include human\nfeedback. Existing interactive Bayesian optimization (BO) methods incorporate\nhuman beliefs by weighting the acquisition function with a user-defined prior\ndistribution. However, in light of the non-trivial inner optimization of the\nacquisition function prevalent in BO, such weighting schemes do not always\naccurately reflect given user beliefs. We introduce a novel BO approach\nleveraging tractable probabilistic models named probabilistic circuits (PCs) as\na surrogate model. PCs encode a tractable joint distribution over the hybrid\nhyperparameter space and evaluation scores. They enable exact conditional\ninference and sampling. Based on conditional sampling, we construct a novel\nselection policy that enables an acquisition function-free generation of\ncandidate points (thereby eliminating the need for an additional inner-loop\noptimization) and ensures that user beliefs are reflected accurately in the\nselection policy. We provide a theoretical analysis and an extensive empirical\nevaluation, demonstrating that our method achieves state-of-the-art performance\nin standard HPO and outperforms interactive BO baselines in interactive HPO.\n","authors":["Jonas Seng","Fabrizio Ventola","Zhongjie Yu","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2505.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17799v1","updated":"2025-05-23T12:18:34Z","published":"2025-05-23T12:18:34Z","title":"A Coreset Selection of Coreset Selection Literature: Introduction and\n  Recent Advances","summary":"  Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Stanislav Frolov","Federico Raue","Joachim Folz","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2505.17799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17797v1","updated":"2025-05-23T12:16:35Z","published":"2025-05-23T12:16:35Z","title":"Latent Mode Decomposition","summary":"  We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm\nfor extracting oscillatory modes and associated connectivity structures from\nmultivariate signals. VLMD addresses key limitations of existing Multivariate\nMode Decomposition (MMD) techniques -including high computational cost,\nsensitivity to parameter choices, and weak modeling of interchannel\ndependencies. Its improved performance is driven by a novel underlying model,\nLatent Mode Decomposition (LMD), which blends sparse coding and mode\ndecomposition to represent multichannel signals as sparse linear combinations\nof shared latent components composed of AM-FM oscillatory modes. This\nformulation enables VLMD to operate in a lower-dimensional latent space,\nenhancing robustness to noise, scalability, and interpretability. The algorithm\nsolves a constrained variational optimization problem that jointly enforces\nreconstruction fidelity, sparsity, and frequency regularization. Experiments on\nsynthetic and real-world datasets demonstrate that VLMD outperforms\nstate-of-the-art MMD methods in accuracy, efficiency, and interpretability of\nextracted structures.\n","authors":["Manuel Morante","Naveed ur Rehman"],"pdf_url":"https://arxiv.org/pdf/2505.17797v1.pdf","comment":"12 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.17794v1","updated":"2025-05-23T12:11:40Z","published":"2025-05-23T12:11:40Z","title":"RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based\n  Temporal Knowledge Graph Completion","summary":"  Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped\nrelations between entities. TKG completion involves forecasting missing or\nfuture links, requiring models to reason over time-evolving structure. While\nLLMs show promise for this task, existing approaches often overemphasize\nsupervised fine-tuning and struggle particularly when historical evidence is\nlimited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient\nframework designed to improve accuracy and generalization in settings with\nsparse historical context. It combines (1) rule-based multi-hop retrieval for\nstructurally diverse history, (2) contrastive fine-tuning of lightweight\nadapters to encode relational semantics, and (3) test-time semantic filtering\nto iteratively refine generations based on embedding similarity. Experiments on\nfour TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based\napproaches, achieving up to 30.6\\% relative improvement in Hits@10. Moreover,\nour proposed framework produces more semantically coherent predictions, even\nfor the samples with limited historical context.\n","authors":["mer Faruk Akgl","Feiyu Zhu","Yuxin Yang","Rajgopal Kannan","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/2505.17794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05000v3","updated":"2025-05-23T12:07:21Z","published":"2025-01-09T06:29:50Z","title":"Load Forecasting for Households and Energy Communities: Are Deep\n  Learning Models Worth the Effort?","summary":"  Energy communities (ECs) play a key role in enabling local demand shifting\nand enhancing self-sufficiency, as energy systems transition toward\ndecentralized structures with high shares of renewable generation. To optimally\noperate them, accurate short-term load forecasting is essential, particularly\nfor implementing demand-side management strategies. With the recent rise of\ndeep learning methods, data-driven forecasting has gained significant\nattention, however, it remains insufficiently explored in many practical\ncontexts. Therefore, this study evaluates the effectiveness of state-of-the-art\ndeep learning models -- including LSTM, xLSTM, and Transformer architectures --\ncompared to traditional benchmarks such as K-Nearest Neighbors (KNN) and\npersistence forecasting, across varying community size, historical data\navailability, and model complexity. Additionally, we assess the benefits of\ntransfer learning using publicly available synthetic load profiles. On average,\ntransfer learning improves the normalized mean absolute error by 1.97%pt when\nonly two months of training data are available. Interestingly, for less than\nsix months of training data, simple persistence models outperform deep learning\narchitectures in forecast accuracy. The practical value of improved forecasting\nis demonstrated using a mixed-integer linear programming optimization for ECs\nwith a shared battery energy storage system. The most accurate deep learning\nmodel achieves an average reduction in financial energy costs of 8.06%.\nNotably, a simple KNN approach achieves average savings of 8.01%, making it a\ncompetitive and robust alternative. All implementations are publicly available\nto facilitate reproducibility. These findings offer actionable insights for\nECs, and they highlight when the additional complexity of deep learning is\nwarranted by performance gains.\n","authors":["Lukas Moosbrugger","Valentin Seiler","Philipp Wohlgenannt","Sebastian Hegenbart","Sashko Ristov","Elias Eder","Peter Kepplinger"],"pdf_url":"https://arxiv.org/pdf/2501.05000v3.pdf","comment":"This preprint was submitted to the Elsevier Journal Applied Energy on\n  May 23, 2025"},{"id":"http://arxiv.org/abs/2410.15729v4","updated":"2025-05-23T12:03:56Z","published":"2024-10-21T07:44:57Z","title":"A Two-Stage Learning-to-Defer Approach for Multi-Task Learning","summary":"  The Two-Stage Learning-to-Defer (L2D) framework has been extensively studied\nfor classification and, more recently, regression tasks. However, many\nreal-world applications require solving both tasks jointly in a multi-task\nsetting. We introduce a novel Two-Stage L2D framework for multi-task learning\nthat integrates classification and regression through a unified deferral\nmechanism. Our method leverages a two-stage surrogate loss family, which we\nprove to be both Bayes-consistent and $(\\mathcal{G}, \\mathcal{R})$-consistent,\nensuring convergence to the Bayes-optimal rejector. We derive explicit\nconsistency bounds tied to the cross-entropy surrogate and the $L_1$-norm of\nagent-specific costs, and extend minimizability gap analysis to the\nmulti-expert two-stage regime. We also make explicit how shared representation\nlearning--commonly used in multi-task models--affects these consistency\nguarantees. Experiments on object detection and electronic health record\nanalysis demonstrate the effectiveness of our approach and highlight the\nlimitations of existing L2D methods in multi-task scenarios.\n","authors":["Yannis Montreuil","Shu Heng Yeo","Axel Carlier","Lai Xing Ng","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2410.15729v4.pdf","comment":"32 pages, 17 main paper"},{"id":"http://arxiv.org/abs/2505.17789v1","updated":"2025-05-23T12:02:13Z","published":"2025-05-23T12:02:13Z","title":"Optimal Online Change Detection via Random Fourier Features","summary":"  This article studies the problem of online non-parametric change point\ndetection in multivariate data streams. We approach the problem through the\nlens of kernel-based two-sample testing and introduce a sequential testing\nprocedure based on random Fourier features, running with logarithmic time\ncomplexity per observation and with overall logarithmic space complexity. The\nalgorithm has two advantages compared to the state of the art. First, our\napproach is genuinely online, and no access to training data known to be from\nthe pre-change distribution is necessary. Second, the algorithm does not\nrequire the user to specify a window parameter over which local tests are to be\ncalculated. We prove strong theoretical guarantees on the algorithm's\nperformance, including information-theoretic bounds demonstrating that the\ndetection delay is optimal in the minimax sense. Numerical studies on real and\nsynthetic data show that our algorithm is competitive with respect to the state\nof the art.\n","authors":["Florian Kalinke","Shakeel Gavioli-Akilagun"],"pdf_url":"https://arxiv.org/pdf/2505.17789v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.16809v2","updated":"2025-05-23T10:55:45Z","published":"2025-05-22T15:49:25Z","title":"Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor\n  Segmentation with Missing Modalities","summary":"  Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.\n","authors":["Junze Wang","Lei Fan","Weipeng Jing","Donglin Di","Yang Song","Sidong Liu","Cong Cong"],"pdf_url":"https://arxiv.org/pdf/2505.16809v2.pdf","comment":"MICCAI 2025 Early Accept. The code is available at\n  https://github.com/reeive/ReHyDIL"},{"id":"http://arxiv.org/abs/2505.17645v1","updated":"2025-05-23T09:06:09Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human\n  Sensing and Reasoning","summary":"  Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.\n","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2505.17645v1.pdf","comment":"18 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.14151v2","updated":"2025-05-23T08:51:27Z","published":"2025-05-20T10:01:37Z","title":"ReactDiff: Latent Diffusion for Facial Reaction Generation","summary":"  Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.\n","authors":["Jiaming Li","Sheng Wang","Xin Wang","Yitao Zhu","Honglin Xiong","Zixu Zhuang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14151v2.pdf","comment":"Neural Networks"},{"id":"http://arxiv.org/abs/2412.12453v2","updated":"2025-05-23T08:23:23Z","published":"2024-12-17T01:36:32Z","title":"Multimodal Classification and Out-of-distribution Detection for\n  Multimodal Intent Understanding","summary":"  Multimodal intent understanding is a significant research area that requires\neffective leveraging of multiple modalities to analyze human language. Existing\nmethods face two main challenges in this domain. Firstly, they have limitations\nin capturing the nuanced and high-level semantics underlying complex\nin-distribution (ID) multimodal intents. Secondly, they exhibit poor\ngeneralization when confronted with unseen out-of-distribution (OOD) data in\nreal-world scenarios. To address these issues, we propose a novel method for\nboth ID classification and OOD detection (MIntOOD). We first introduce a\nweighted feature fusion network that models multimodal representations. This\nnetwork dynamically learns the importance of each modality, adapting to\nmultimodal contexts. To develop discriminative representations for both tasks,\nwe synthesize pseudo-OOD data from convex combinations of ID data and engage in\nmultimodal representation learning from both coarse-grained and fine-grained\nperspectives. The coarse-grained perspective focuses on distinguishing between\nID and OOD binary classes, while the fine-grained perspective not only enhances\nthe discrimination between different ID classes but also captures\ninstance-level interactions between ID and OOD samples, promoting proximity\namong similar instances and separation from dissimilar ones. We establish\nbaselines for three multimodal intent datasets and build an OOD benchmark.\nExtensive experiments on these datasets demonstrate that our method\nsignificantly improves OOD detection performance with a 3~10% increase in AUROC\nscores while achieving new state-of-the-art results in ID classification. Data\nand codes are available at https://github.com/thuiar/MIntOOD.\n","authors":["Hanlei Zhang","Qianrui Zhou","Hua Xu","Jianhua Su","Roberto Evans","Kai Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12453v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2505.16592v2","updated":"2025-05-23T07:33:48Z","published":"2025-05-22T12:27:12Z","title":"What Media Frames Reveal About Stance: A Dataset and Study about Memes\n  in Climate Change Discourse","summary":"  Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.\n","authors":["Shijia Zhou","Siyao Peng","Simon Luebke","Jrg Haler","Mario Haim","Saif M. Mohammad","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2505.16592v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.17543v1","updated":"2025-05-23T06:47:06Z","published":"2025-05-23T06:47:06Z","title":"MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance\n  Generation","summary":"  Music-driven 3D dance generation has attracted increasing attention in recent\nyears, with promising applications in choreography, virtual reality, and\ncreative content creation. Previous research has generated promising realistic\ndance movement from audio signals. However, traditional methods underutilize\ngenre conditioning, often treating it as auxiliary modifiers rather than core\nsemantic drivers. This oversight compromises music-motion synchronization and\ndisrupts dance genre continuity, particularly during complex rhythmic\ntransitions, thereby leading to visually unsatisfactory effects. To address the\nchallenge, we propose MEGADance, a novel architecture for music-driven 3D dance\ngeneration. By decoupling choreographic consistency into dance generality and\ngenre specificity, MEGADance demonstrates significant dance quality and strong\ngenre controllability. It consists of two stages: (1) High-Fidelity Dance\nQuantization Stage (HFDQ), which encodes dance motions into a latent\nrepresentation by Finite Scalar Quantization (FSQ) and reconstructs them with\nkinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage\n(GADG), which maps music into the latent representation by synergistic\nutilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid\nbackbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate\nthe state-of-the-art performance of MEGADance both qualitatively and\nquantitatively. Code will be released upon acceptance.\n","authors":["Kaixing Yang","Xulong Tang","Ziqiao Peng","Yuxuan Hu","Jun He","Hongyan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.17543v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2505.14222"},{"id":"http://arxiv.org/abs/2505.17534v1","updated":"2025-05-23T06:41:07Z","published":"2025-05-23T06:41:07Z","title":"Co-Reinforcement Learning for Unified Multimodal Understanding and\n  Generation","summary":"  This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs.\n","authors":["Jingjing Jiang","Chongjie Si","Jun Luo","Hanwang Zhang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2505.17534v1.pdf","comment":null}]},"2025-05-22T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.17330v1","updated":"2025-05-22T22:53:58Z","published":"2025-05-22T22:53:58Z","title":"FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich\n  Document Understanding","summary":"  In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag\n","authors":["Amit Agarwal","Srikant Panda","Kulbhushan Pachauri"],"pdf_url":"https://arxiv.org/pdf/2505.17330v1.pdf","comment":"Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114"},{"id":"http://arxiv.org/abs/2505.17326v1","updated":"2025-05-22T22:42:40Z","published":"2025-05-22T22:42:40Z","title":"VoxRAG: A Step Toward Transcription-Free RAG Systems in Spoken Question\n  Answering","summary":"  We introduce VoxRAG, a modular speech-to-speech retrieval-augmented\ngeneration system that bypasses transcription to retrieve semantically relevant\naudio segments directly from spoken queries. VoxRAG employs silence-aware\nsegmentation, speaker diarization, CLAP audio embeddings, and FAISS retrieval\nusing L2-normalized cosine similarity. We construct a 50-query test set\nrecorded as spoken input by a native English speaker. Retrieval quality was\nevaluated using LLM-as-a-judge annotations. For very relevant segments, cosine\nsimilarity achieved a Recall@10 of 0.34. For somewhat relevant segments,\nRecall@10 rose to 0.60 and nDCG@10 to 0.27, highlighting strong topical\nalignment. Answer quality was judged on a 0--2 scale across relevance,\naccuracy, completeness, and precision, with mean scores of 0.84, 0.58, 0.56,\nand 0.46 respectively. While precision and retrieval quality remain key\nlimitations, VoxRAG shows that transcription-free speech-to-speech retrieval is\nfeasible in RAG systems.\n","authors":["Zackary Rackauckas","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2505.17326v1.pdf","comment":"Accepted to ACL 2025 Workshop MAGMaR"},{"id":"http://arxiv.org/abs/2504.05522v3","updated":"2025-05-22T21:24:56Z","published":"2025-04-07T21:44:12Z","title":"User Feedback Alignment for LLM-powered Exploration in Large-scale\n  Recommendation Systems","summary":"  Exploration, the act of broadening user experiences beyond their established\npreferences, is challenging in large-scale recommendation systems due to\nfeedback loops and limited signals on user exploration patterns. Large Language\nModels (LLMs) offer potential solutions by leveraging their world knowledge to\nrecommend novel content outside these loops. A key challenge is aligning LLMs\nwith user preferences while preserving their knowledge and reasoning. To\nenhance planning for new user interests using LLMs, this paper introduces a\nnovel approach that combines hierarchical planning with LLM inference-time\nscaling. This method aims to improve recommendation relevancy without\ncompromising novelty. We decouple novelty and user-alignment, training separate\nLLMs for each objective. We then scale up the novelty-focused LLM's inference\nand select the best-of-n predictions using the user-aligned LLM. Live\nexperiments demonstrate efficacy, showing significant gains in both user\nsatisfaction (measured by watch activity and active user counts) and\nexploration diversity.\n","authors":["Jianling Wang","Yifan Liu","Yinghao Sun","Xuejian Ma","Yueqi Wang","He Ma","Zhengyang Su","Minmin Chen","Mingyan Gao","Onkar Dalal","Ed H. Chi","Lichan Hong","Ningren Han","Haokai Lu"],"pdf_url":"https://arxiv.org/pdf/2504.05522v3.pdf","comment":"ACL'25 (Industry) Oral"},{"id":"http://arxiv.org/abs/2503.05227v2","updated":"2025-05-22T19:14:29Z","published":"2025-03-07T08:25:08Z","title":"MOHPER: Multi-objective Hyperparameter Optimization Framework for\n  E-commerce Retrieval System","summary":"  E-commerce search optimization has evolved to include a wider range of\nmetrics that reflect user engagement and business objectives. Modern search\nframeworks now incorporate advanced quality features, such as sales counts and\ndocument-query relevance, to better align search results with these goals.\nTraditional methods typically focus on click-through rate (CTR) as a measure of\nengagement or relevance, but this can miss true purchase intent, creating a gap\nbetween user interest and actual conversions. Joint training with the\nclick-through conversion rate (CTCVR) has become essential for understanding\nbuying behavior, although its sparsity poses challenges for reliable\noptimization. This study presents MOHPER, a Multi-Objective Hyperparameter\nOptimization framework for E-commerce Retrieval systems. Utilizing Bayesian\noptimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant\nobjectives, focusing on engagement and conversion of the users. In addition, to\nimprove the selection of the best configuration from multi-objective\noptimization, we suggest advanced methods for hyperparameter selection,\nincluding a meta-configuration voting strategy and a cumulative training\napproach that leverages prior optimal configurations, to improve speeds of\ntraining and efficiency. Currently deployed in a live setting, our proposed\nframework substantiates its practical efficacy in achieving a balanced\noptimization that aligns with both user satisfaction and revenue goals.\n","authors":["Jungbae Park","Heonseok Jang"],"pdf_url":"https://arxiv.org/pdf/2503.05227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17207v1","updated":"2025-05-22T18:32:39Z","published":"2025-05-22T18:32:39Z","title":"Content Moderation in TV Search: Balancing Policy Compliance, Relevance,\n  and User Experience","summary":"  Millions of people rely on search functionality to find and explore content\non entertainment platforms. Modern search systems use a combination of\ncandidate generation and ranking approaches, with advanced methods leveraging\ndeep learning and LLM-based techniques to retrieve, generate, and categorize\nsearch results. Despite these advancements, search algorithms can still surface\ninappropriate or irrelevant content due to factors like model unpredictability,\nmetadata errors, or overlooked design flaws. Such issues can misalign with\nproduct goals and user expectations, potentially harming user trust and\nbusiness outcomes. In this work, we introduce an additional monitoring layer\nusing Large Language Models (LLMs) to enhance content moderation. This\nadditional layer flags content if the user did not intend to search for it.\nThis approach serves as a baseline for product quality assurance, with\ncollected feedback used to refine the initial retrieval mechanisms of the\nsearch model, ensuring a safer and more reliable user experience.\n","authors":["Adeep Hande","Kishorekumar Sundararajan","Sardar Hamidian","Ferhan Ture"],"pdf_url":"https://arxiv.org/pdf/2505.17207v1.pdf","comment":"Accepted at SIGIR 2025 Industry Track. 5 pages, 1 figure, 2 tables.\n  DOI: 10.1145/3726302.3731962"},{"id":"http://arxiv.org/abs/2505.17005v1","updated":"2025-05-22T17:58:26Z","published":"2025-05-22T17:58:26Z","title":"R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning","summary":"  Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.\n","authors":["Huatong Song","Jinhao Jiang","Wenqing Tian","Zhipeng Chen","Yuhuan Wu","Jiahao Zhao","Yingqian Min","Wayne Xin Zhao","Lei Fang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.17005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16994v1","updated":"2025-05-22T17:55:43Z","published":"2025-05-22T17:55:43Z","title":"$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning","summary":"  Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.\n","authors":["Runyang You","Yongqi Li","Xinyu Lin","Xin Zhang","Wenjie Wang","Wenjie Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.16994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16967v1","updated":"2025-05-22T17:47:57Z","published":"2025-05-22T17:47:57Z","title":"Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval","summary":"  Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.\n","authors":["Nandan Thakur","Crystina Zhang","Xueguang Ma","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2505.16967v1.pdf","comment":"Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn"},{"id":"http://arxiv.org/abs/2505.16886v1","updated":"2025-05-22T16:41:37Z","published":"2025-05-22T16:41:37Z","title":"Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?","summary":"  With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.\n","authors":["Nour Jedidi","Yung-Sung Chuang","James Glass","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2505.16886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16865v1","updated":"2025-05-22T16:22:54Z","published":"2025-05-22T16:22:54Z","title":"LARES: Latent Reasoning for Sequential Recommendation","summary":"  Sequential recommender systems have become increasingly important in\nreal-world applications that model user behavior sequences to predict their\npreferences. However, existing sequential recommendation methods predominantly\nrely on non-reasoning paradigms, which may limit the model's computational\ncapacity and result in suboptimal recommendation performance. To address these\nlimitations, we present LARES, a novel and scalable LAtent REasoning framework\nfor Sequential recommendation that enhances model's representation capabilities\nthrough increasing the computation density of parameters by depth-recurrent\nlatent reasoning. Our proposed approach employs a recurrent architecture that\nallows flexible expansion of reasoning depth without increasing parameter\ncomplexity, thereby effectively capturing dynamic and intricate user interest\npatterns. A key difference of LARES lies in refining all input tokens at each\nimplicit reasoning step to improve the computation utilization. To fully unlock\nthe model's reasoning potential, we design a two-phase training strategy: (1)\nSelf-supervised pre-training (SPT) with dual alignment objectives; (2)\nReinforcement post-training (RPT). During the first phase, we introduce\ntrajectory-level alignment and step-level alignment objectives, which enable\nthe model to learn recommendation-oriented latent reasoning patterns without\nrequiring supplementary annotated data. The subsequent phase utilizes\nreinforcement learning (RL) to harness the model's exploratory ability, further\nrefining its reasoning capabilities. Comprehensive experiments on real-world\nbenchmarks demonstrate our framework's superior performance. Notably, LARES\nexhibits seamless compatibility with existing advanced models, further\nimproving their recommendation performance.\n","authors":["Enze Liu","Bowen Zheng","Xiaolei Wang","Wayne Xin Zhao","Jinpeng Wang","Sheng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.16865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17166v1","updated":"2025-05-22T16:13:02Z","published":"2025-05-22T16:13:02Z","title":"ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval","summary":"  The ViDoRe Benchmark V1 was approaching saturation with top models exceeding\n90% nDCG@5, limiting its ability to discern improvements. ViDoRe Benchmark V2\nintroduces realistic, challenging retrieval scenarios via blind contextual\nquerying, long and cross-document queries, and a hybrid synthetic and\nhuman-in-the-loop query generation process. It comprises four diverse,\nmultilingual datasets and provides clear evaluation instructions. Initial\nresults demonstrate substantial room for advancement and highlight insights on\nmodel generalization and multilingual capability. This benchmark is designed as\na living resource, inviting community contributions to maintain relevance\nthrough future evaluations.\n","authors":["Quentin Mac","Antnio Loison","Manuel Faysse"],"pdf_url":"https://arxiv.org/pdf/2505.17166v1.pdf","comment":"Published as a HuggingFace Blog"},{"id":"http://arxiv.org/abs/2505.16849v1","updated":"2025-05-22T16:11:35Z","published":"2025-05-22T16:11:35Z","title":"Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented\n  Generation via Knowledge Graph Walks","summary":"  Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research.\n","authors":["Martin Bckling","Heiko Paulheim","Andreea Iana"],"pdf_url":"https://arxiv.org/pdf/2505.16849v1.pdf","comment":"Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.16834v1","updated":"2025-05-22T16:05:02Z","published":"2025-05-22T16:05:02Z","title":"SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis","summary":"  Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.\n","authors":["Shuang Sun","Huatong Song","Yuhao Wang","Ruiyang Ren","Jinhao Jiang","Junjie Zhang","Fei Bai","Jia Deng","Wayne Xin Zhao","Zheng Liu","Lei Fang","Zhongyuan Wang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.16834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16810v1","updated":"2025-05-22T15:49:38Z","published":"2025-05-22T15:49:38Z","title":"DeepRec: Towards a Deep Dive Into the Item Space with Large Language\n  Model Based Recommendation","summary":"  Recently, large language models (LLMs) have been introduced into recommender\nsystems (RSs), either to enhance traditional recommendation models (TRMs) or\nserve as recommendation backbones. However, existing LLM-based RSs often do not\nfully exploit the complementary advantages of LLMs (e.g., world knowledge and\nreasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to\nfully explore the item space. To address this, we propose DeepRec, a novel\nLLM-based RS that enables autonomous multi-turn interactions between LLMs and\nTRMs for deep exploration of the item space. In each interaction turn, LLMs\nreason over user preferences and interact with TRMs to retrieve candidate\nitems. After multi-turn interactions, LLMs rank the retrieved items to generate\nthe final recommendations. We adopt reinforcement learning(RL) based\noptimization and propose novel designs from three aspects: recommendation model\nbased data rollout, recommendation-oriented hierarchical rewards, and a\ntwo-stage RL training strategy. For data rollout, we introduce a\npreference-aware TRM, with which LLMs interact to construct trajectory data.\nFor rewards, we design a hierarchical reward function that involves both\nprocess-level and outcome-level rewards to optimize the interaction process and\nrecommendation performance, respectively. For RL training, we develop a\ntwo-stage training strategy, where the first stage aims to guide LLMs to\ninteract with TRMs and the second stage focuses on performance improvement.\nExperiments on public datasets demonstrate that DeepRec significantly\noutperforms both traditional and LLM-based baselines, offering a new paradigm\nfor deep exploration in recommendation systems.\n","authors":["Bowen Zheng","Xiaolei Wang","Enze Liu","Xi Wang","Lu Hongyu","Yu Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2505.16810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16806v1","updated":"2025-05-22T15:45:29Z","published":"2025-05-22T15:45:29Z","title":"Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement","summary":"  Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.\n","authors":["Kexin Zhang","Junlan Chen","Daifeng Li","Yuxuan Zhang","Yangyang Feng","Bowen Deng","Weixu Chen"],"pdf_url":"https://arxiv.org/pdf/2505.16806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17162v1","updated":"2025-05-22T15:13:33Z","published":"2025-05-22T15:13:33Z","title":"DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on\n  Capturing Real-World Changes","summary":"  We propose DailyQA, an automatically updated dynamic dataset that updates\nquestions weekly and contains answers to questions on any given date. DailyQA\nutilizes daily updates from Wikipedia revision logs to implement a fully\nautomated pipeline of data filtering, query generation synthesis, quality\nchecking, answer extraction, and query classification. The benchmark requires\nlarge language models (LLMs) to process and answer questions involving\nfast-changing factual data and covering multiple domains. We evaluate several\nopen-source and closed-source LLMs using different RAG pipelines with web\nsearch augmentation. We compare the ability of different models to process\ntime-sensitive web information and find that rerank of web retrieval results is\ncritical. Our results indicate that LLMs still face significant challenges in\nhandling frequently updated information, suggesting that DailyQA benchmarking\nprovides valuable insights into the direction of progress for LLMs and RAG\nsystems.\n","authors":["Jiehan Cheng","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2505.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16756v1","updated":"2025-05-22T14:59:30Z","published":"2025-05-22T14:59:30Z","title":"Representation Discrepancy Bridging Method for Remote Sensing Image-Text\n  Retrieval","summary":"  Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in\ngeographic information interpretation, disaster monitoring, and urban planning\nby establishing semantic associations between image and textual descriptions.\nExisting Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language\nPre-training (VLP) models typically adopt symmetric adapter structures for\nexploring cross-modal correlations. However, the strong discriminative nature\nof text modality may dominate the optimization process and inhibits image\nrepresentation learning. The nonnegligible imbalanced cross-modal optimization\nremains a bottleneck to enhancing the model performance. To address this issue,\nthis study proposes a Representation Discrepancy Bridging (RDB) method for the\nRSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is\ndesigned to enable modality-specific optimization and improve feature\nalignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text\nSemantic Adapter (TSA). VEA mines fine-grained image features by Differential\nAttention (DA) mechanism, while TSA identifies key textual semantics through\nHierarchical Attention (HA) mechanism. On the other hand, this study extends\nthe traditional single-task retrieval framework to a dual-task optimization\nframework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves\ncross-modal alignment robustness through an adaptive weighted combination of\ncross-modal, classification, and exponential moving average consistency\nconstraints. Experiments on RSICD and RSITMD datasets show that the proposed\nRDB method achieves a 6%-11% improvement in mR metrics compared to\nstate-of-the-art PEFT methods and a 1.15%-2% improvement over the full\nfine-tuned GeoRSCLIP model.\n","authors":["Hailong Ning","Siying Wang","Tao Lei","Xiaopeng Cao","Huanmin Dou","Bin Zhao","Asoke K. Nandi","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2505.16756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16752v1","updated":"2025-05-22T14:58:53Z","published":"2025-05-22T14:58:53Z","title":"Action is All You Need: Dual-Flow Generative Ranking Network for\n  Recommendation","summary":"  We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm.\n","authors":["Hao Guo","Erpeng Xue","Lei Huang","Shichao Wang","Xiaolei Wang","Lei Wang","Jinpeng Wang","Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.16752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15091v2","updated":"2025-05-22T14:53:00Z","published":"2025-05-21T04:25:18Z","title":"ThinkRec: Thinking-based recommendation via LLM","summary":"  Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.\n","authors":["Qihang Yu","Kairui Fu","Shengyu Zhang","Zheqi Lv","Fan Wu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2505.15091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16708v1","updated":"2025-05-22T14:09:39Z","published":"2025-05-22T14:09:39Z","title":"A Novel Generative Model with Causality Constraint for Mitigating Biases\n  in Recommender Systems","summary":"  Accurately predicting counterfactual user feedback is essential for building\neffective recommender systems. However, latent confounding bias can obscure the\ntrue causal relationship between user feedback and item exposure, ultimately\ndegrading recommendation performance. Existing causal debiasing approaches\noften rely on strong assumptions-such as the availability of instrumental\nvariables (IVs) or strong correlations between latent confounders and proxy\nvariables-that are rarely satisfied in real-world scenarios. To address these\nlimitations, we propose a novel generative framework called Latent Causality\nConstraints for Debiasing representation learning in Recommender Systems\n(LCDR). Specifically, LCDR leverages an identifiable Variational Autoencoder\n(iVAE) as a causal constraint to align the latent representations learned by a\nstandard Variational Autoencoder (VAE) through a unified loss function. This\nalignment allows the model to leverage even weak or noisy proxy variables to\nrecover latent confounders effectively. The resulting representations are then\nused to improve recommendation performance. Extensive experiments on three\nreal-world datasets demonstrate that LCDR consistently outperforms existing\nmethods in both mitigating bias and improving recommendation accuracy.\n","authors":["Jianfeng Deng","Qingfeng Chen","Debo Cheng","Jiuyong Li","Lin Liu","Shichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.16708v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2504.04452v2","updated":"2025-05-22T13:50:30Z","published":"2025-04-06T11:42:49Z","title":"COHESION: Composite Graph Convolutional Network with Dual-Stage Fusion\n  for Multimodal Recommendation","summary":"  Recent works in multimodal recommendations, which leverage diverse modal\ninformation to address data sparsity and enhance recommendation accuracy, have\ngarnered considerable interest. Two key processes in multimodal recommendations\nare modality fusion and representation learning. Previous approaches in\nmodality fusion often employ simplistic attentive or pre-defined strategies at\nearly or late stages, failing to effectively handle irrelevant information\namong modalities. In representation learning, prior research has constructed\nheterogeneous and homogeneous graph structures encapsulating user-item,\nuser-user, and item-item relationships to better capture user interests and\nitem profiles. Modality fusion and representation learning were considered as\ntwo independent processes in previous work. In this paper, we reveal that these\ntwo processes are complementary and can support each other. Specifically,\npowerful representation learning enhances modality fusion, while effective\nfusion improves representation quality. Stemming from these two processes, we\nintroduce a COmposite grapH convolutional nEtwork with dual-stage fuSION for\nthe multimodal recommendation, named COHESION. Specifically, it introduces a\ndual-stage fusion strategy to reduce the impact of irrelevant information,\nrefining all modalities using ID embedding in the early stage and fusing their\nrepresentations at the late stage. It also proposes a composite graph\nconvolutional network that utilizes user-item, user-user, and item-item graphs\nto extract heterogeneous and homogeneous latent relationships within users and\nitems. Besides, it introduces a novel adaptive optimization to ensure balanced\nand reasonable representations across modalities. Extensive experiments on\nthree widely used datasets demonstrate the significant superiority of COHESION\nover various competitive baselines.\n","authors":["Jinfeng Xu","Zheyu Chen","Wei Wang","Xiping Hu","Sang-Wook Kim","Edith C. H. Ngai"],"pdf_url":"https://arxiv.org/pdf/2504.04452v2.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.16665v1","updated":"2025-05-22T13:28:55Z","published":"2025-05-22T13:28:55Z","title":"MDVT: Enhancing Multimodal Recommendation with Model-Agnostic\n  Multimodal-Driven Virtual Triplets","summary":"  The data sparsity problem significantly hinders the performance of\nrecommender systems, as traditional models rely on limited historical\ninteractions to learn user preferences and item properties. While incorporating\nmultimodal information can explicitly represent these preferences and\nproperties, existing works often use it only as side information, failing to\nfully leverage its potential. In this paper, we propose MDVT, a model-agnostic\napproach that constructs multimodal-driven virtual triplets to provide valuable\nsupervision signals, effectively mitigating the data sparsity problem in\nmultimodal recommendation systems. To ensure high-quality virtual triplets, we\nintroduce three tailored warm-up threshold strategies: static, dynamic, and\nhybrid. The static warm-up threshold strategy exhaustively searches for the\noptimal number of warm-up epochs but is time-consuming and computationally\nintensive. The dynamic warm-up threshold strategy adjusts the warm-up period\nbased on loss trends, improving efficiency but potentially missing optimal\nperformance. The hybrid strategy combines both, using the dynamic strategy to\nfind the approximate optimal number of warm-up epochs and then refining it with\nthe static strategy in a narrow hyper-parameter space. Once the warm-up\nthreshold is satisfied, the virtual triplets are used for joint model\noptimization by our enhanced pair-wise loss function without causing\nsignificant gradient skew. Extensive experiments on multiple real-world\ndatasets demonstrate that integrating MDVT into advanced multimodal\nrecommendation models effectively alleviates the data sparsity problem and\nimproves recommendation performance, particularly in sparse data scenarios.\n","authors":["Jinfeng Xu","Zheyu Chen","Jinze Li","Shuo Yang","Hewei Wang","Yijie Li","Mengran Li","Puzhen Wu","Edith C. H. Ngai"],"pdf_url":"https://arxiv.org/pdf/2505.16665v1.pdf","comment":"Accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2411.13865v3","updated":"2025-05-22T13:18:14Z","published":"2024-11-21T06:01:47Z","title":"Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems","summary":"  Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a semantic-enhanced\nhierarchical mechanism that aligns rich textual descriptions processed by large\nlanguage models (LLMs) with collaborative information directly in hyperbolic\nspace, allowing for more nuanced updates that respect the underlying\nhierarchical structure in user-item profiles; (2) an automatic hierarchical\nrepresentation by optimizing Dasgupta's cost, which discovers hierarchical\nstructures without requiring predefined hyperparameters, enabling\nuser-adjustable exploration-exploitation trade-offs. Extensive experiments\ndemonstrate that HERec consistently outperforms both Euclidean and hyperbolic\nbaselines, achieving up to 5.49% improvement in utility metrics and 11.39%\nincrease in diversity metrics, effectively mitigating information cocoons. We\nopen-source our model implementation at https://github.com/Martin-qyma/HERec.\n","authors":["Qiyao Ma","Menglin Yang","Mingxuan Ju","Tong Zhao","Neil Shah","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2411.13865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16631v1","updated":"2025-05-22T13:03:15Z","published":"2025-05-22T13:03:15Z","title":"MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed\n  Language Queries","summary":"  Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries.\n","authors":["Jonghwi Kim","Deokhyung Kang","Seonjeong Hwang","Yunsu Kim","Jungseul Ok","Gary Lee"],"pdf_url":"https://arxiv.org/pdf/2505.16631v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.16532v1","updated":"2025-05-22T11:21:51Z","published":"2025-05-22T11:21:51Z","title":"Causal-Invariant Cross-Domain Out-of-Distribution Recommendation","summary":"  Cross-Domain Recommendation (CDR) aims to leverage knowledge from a\nrelatively data-richer source domain to address the data sparsity problem in a\nrelatively data-sparser target domain. While CDR methods need to address the\ndistribution shifts between different domains, i.e., cross-domain distribution\nshifts (CDDS), they typically assume independent and identical distribution\n(IID) between training and testing data within the target domain. However, this\nIID assumption rarely holds in real-world scenarios due to single-domain\ndistribution shift (SDDS). The above two co-existing distribution shifts lead\nto out-of-distribution (OOD) environments that hinder effective knowledge\ntransfer and generalization, ultimately degrading recommendation performance in\nCDR. To address these co-existing distribution shifts, we propose a novel\nCausal-Invariant Cross-Domain Out-of-distribution Recommendation framework,\ncalled CICDOR. In CICDOR, we first learn dual-level causal structures to infer\ndomain-specific and domain-shared causal-invariant user preferences for\ntackling both CDDS and SDDS under OOD environments in CDR. Then, we propose an\nLLM-guided confounder discovery module that seamlessly integrates LLMs with a\nconventional causal discovery method to extract observed confounders for\neffective deconfounding, thereby enabling accurate causal-invariant preference\ninference. Extensive experiments on two real-world datasets demonstrate the\nsuperior recommendation accuracy of CICDOR over state-of-the-art methods across\nvarious OOD scenarios.\n","authors":["Jiajie Zhu","Yan Wang","Feng Zhu","Pengfei Ding","Hongyang Liu","Zhu Sun"],"pdf_url":"https://arxiv.org/pdf/2505.16532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16506v1","updated":"2025-05-22T10:41:55Z","published":"2025-05-22T10:41:55Z","title":"Utilizing citation index and synthetic quality measure to compare\n  Wikipedia languages across various topics","summary":"  This study presents a comparative analysis of 55 Wikipedia language editions\nemploying a citation index alongside a synthetic quality measure. Specifically,\nwe identified the most significant Wikipedia articles within distinct topical\nareas, selecting the top 10, top 25, and top 100 most cited articles in each\ntopic and language version. This index was built on the basis of wikilinks\nbetween Wikipedia articles in each language version and in order to do that we\nprocessed 6.6 billion page-to-page link records. Next, we used a quality score\nfor each Wikipedia article - a synthetic measure scaled from 0 to 100. This\napproach enabled quality comparison of Wikipedia articles even between language\nversions with different quality grading schemes. Our results highlight\ndisparities among Wikipedia language editions, revealing strengths and gaps in\ncontent coverage and quality across topics.\n","authors":["Wodzimierz Lewoniewski","Krzysztof Wcel","Witold Abramowicz"],"pdf_url":"https://arxiv.org/pdf/2505.16506v1.pdf","comment":"Presented at the Wiki Workshop 2025"},{"id":"http://arxiv.org/abs/2505.16470v1","updated":"2025-05-22T09:52:57Z","published":"2025-05-22T09:52:57Z","title":"Benchmarking Retrieval-Augmented Multimomal Generation for Document\n  Question Answering","summary":"  Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.\n","authors":["Kuicai Dong","Yujing Chang","Shijie Huang","Yasheng Wang","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2505.16470v1.pdf","comment":"preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}"},{"id":"http://arxiv.org/abs/2505.16466v1","updated":"2025-05-22T09:48:17Z","published":"2025-05-22T09:48:17Z","title":"Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for\n  GNN-based Recommendation Methods","summary":"  Recommender systems based on graph neural networks perform well in tasks such\nas rating and ranking. However, in real-world recommendation scenarios, noise\nsuch as user misuse and malicious advertisement gradually accumulates through\nthe message propagation mechanism. Even if existing studies mitigate their\neffects by reducing the noise propagation weights, the severe sparsity of the\nrecommender system still leads to the low-weighted noisy neighbors being\nmistaken as meaningful information, and the prediction result obtained based on\nthe polluted nodes is not entirely trustworthy. Therefore, it is crucial to\nmeasure the confidence of the prediction results in this highly noisy\nframework. Furthermore, our evaluation of the existing representative GNN-based\nrecommendation shows that it suffers from overconfidence. Based on the above\nconsiderations, we propose a new method to quantify and calibrate the\nprediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,\nwe propose a rating calibration method that dynamically adjusts excessive\nratings to mitigate overconfidence based on user personalization. We also\ndesign a confidence loss function to reduce the overconfidence of negative\nsamples and effectively improve recommendation performance. Experiments on\npublic datasets demonstrate the validity of Conf-GNNRec in prediction\nconfidence and recommendation performance.\n","authors":["Meng Yan","Cai Xu","Xujing Wang","Ziyu Guan","Wei Zhao","Yuhang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.16466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11180v4","updated":"2025-05-22T09:32:08Z","published":"2024-04-17T08:50:29Z","title":"Causal Deconfounding via Confounder Disentanglement for Dual-Target\n  Cross-Domain Recommendation","summary":"  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on seven real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n","authors":["Jiajie Zhu","Yan Wang","Feng Zhu","Zhu Sun"],"pdf_url":"https://arxiv.org/pdf/2404.11180v4.pdf","comment":"Accepted by ACM TOIS for publication"},{"id":"http://arxiv.org/abs/2505.16367v1","updated":"2025-05-22T08:22:46Z","published":"2025-05-22T08:22:46Z","title":"Chain-of-Thought Poisoning Attacks against R1-based Retrieval-Augmented\n  Generation Systems","summary":"  Retrieval-augmented generation (RAG) systems can effectively mitigate the\nhallucination problem of large language models (LLMs),but they also possess\ninherent vulnerabilities. Identifying these weaknesses before the large-scale\nreal-world deployment of RAG systems is of great importance, as it lays the\nfoundation for building more secure and robust RAG systems in the future.\nExisting adversarial attack methods typically exploit knowledge base poisoning\nto probe the vulnerabilities of RAG systems, which can effectively deceive\nstandard RAG models. However, with the rapid advancement of deep reasoning\ncapabilities in modern LLMs, previous approaches that merely inject incorrect\nknowledge are inadequate when attacking RAG systems equipped with deep\nreasoning abilities. Inspired by the deep thinking capabilities of LLMs, this\npaper extracts reasoning process templates from R1-based RAG systems, uses\nthese templates to wrap erroneous knowledge into adversarial documents, and\ninjects them into the knowledge base to attack RAG systems. The key idea of our\napproach is that adversarial documents, by simulating the chain-of-thought\npatterns aligned with the model's training signals, may be misinterpreted by\nthe model as authentic historical reasoning processes, thus increasing their\nlikelihood of being referenced. Experiments conducted on the MS MARCO passage\nranking dataset demonstrate the effectiveness of our proposed method.\n","authors":["Hongru Song","Yu-an Liu","Ruqing Zhang","Jiafeng Guo","Yixing Fan"],"pdf_url":"https://arxiv.org/pdf/2505.16367v1.pdf","comment":"7 pages,3 figures"},{"id":"http://arxiv.org/abs/2502.11471v3","updated":"2025-05-22T07:05:10Z","published":"2025-02-17T06:02:59Z","title":"GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion","summary":"  Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.\n","authors":["Kangyang Luo","Yuzhuo Bai","Cheng Gao","Shuzheng Si","Yingli Shen","Zhu Liu","Zhitong Wang","Cunliang Kong","Wenhao Li","Yufei Huang","Ye Tian","Xuantang Xiong","Lei Han","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.11471v3.pdf","comment":"Accepted by ACL2025(Findings)"},{"id":"http://arxiv.org/abs/2505.16298v1","updated":"2025-05-22T06:53:03Z","published":"2025-05-22T06:53:03Z","title":"Flow Matching based Sequential Recommender Model","summary":"  Generative models, particularly diffusion model, have emerged as powerful\ntools for sequential recommendation. However, accurately modeling user\npreferences remains challenging due to the noise perturbations inherent in the\nforward and reverse processes of diffusion-based methods. Towards this end,\nthis study introduces FMRec, a Flow Matching based model that employs a\nstraight flow trajectory and a modified loss tailored for the recommendation\ntask. Additionally, from the diffusion-model perspective, we integrate a\nreconstruction loss to improve robustness against noise perturbations, thereby\nretaining user preferences during the forward process. In the reverse process,\nwe employ a deterministic reverse sampler, specifically an ODE-based updating\nfunction, to eliminate unnecessary randomness, thereby ensuring that the\ngenerated recommendations closely align with user needs. Extensive evaluations\non four benchmark datasets reveal that FMRec achieves an average improvement of\n6.53% over state-of-the-art methods. The replication code is available at\nhttps://github.com/FengLiu-1/FMRec.\n","authors":["Feng Liu","Lixin Zou","Xiangyu Zhao","Min Tang","Liming Dong","Dan Luo","Xiangyang Luo","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2505.16298v1.pdf","comment":"11 pages, 8 figures, IJCAI 2025 Accepted Work"},{"id":"http://arxiv.org/abs/2504.06714v2","updated":"2025-05-22T06:13:56Z","published":"2025-04-09T09:15:37Z","title":"Unifying Search and Recommendation: A Generative Paradigm Inspired by\n  Information Theory","summary":"  Recommender systems and search engines serve as foundational elements of\nonline platforms, with the former delivering information proactively and the\nlatter enabling users to seek information actively. Unifying both tasks in a\nshared model is promising since it can enhance user modeling and item\nunderstanding. Previous approaches mainly follow a discriminative paradigm,\nutilizing shared encoders to process input features and task-specific heads to\nperform each task. However, this paradigm encounters two key challenges:\ngradient conflict and manual design complexity. From the information theory\nperspective, these challenges potentially both stem from the same issue -- low\nmutual information between the input features and task-specific outputs during\nthe optimization process.\n  To tackle these issues, we propose GenSR, a novel generative paradigm for\nunifying search and recommendation (S&R), which leverages task-specific prompts\nto partition the model's parameter space into subspaces, thereby enhancing\nmutual information. To construct effective subspaces for each task, GenSR first\nprepares informative representations for each subspace and then optimizes both\nsubspaces in one unified model. Specifically, GenSR consists of two main\nmodules: (1) Dual Representation Learning, which independently models\ncollaborative and semantic historical information to derive expressive item\nrepresentations; and (2) S&R Task Unifying, which utilizes contrastive learning\ntogether with instruction tuning to generate task-specific outputs effectively.\nExtensive experiments on two public datasets show GenSR outperforms\nstate-of-the-art methods across S&R tasks. Our work introduces a new generative\nparadigm compared with previous discriminative methods and establishes its\nsuperiority from the mutual information perspective.\n","authors":["Jujia Zhao","Wenjie Wang","Chen Xu","Xiuying Chen","Zhaochun Ren","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2504.06714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12574v3","updated":"2025-05-22T05:44:17Z","published":"2025-05-18T23:22:53Z","title":"PoisonArena: Uncovering Competing Poisoning Attacks in\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.\nProject page: https://github.com/yxf203/PoisonArena.\n","authors":["Liuji Chen","Xiaofang Yang","Yuanzhuo Lu","Jinghao Zhang","Xin Sun","Qiang Liu","Shu Wu","Jing Dong","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.12574v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2412.06272v2","updated":"2025-05-22T03:52:00Z","published":"2024-12-09T07:46:14Z","title":"Evaluating LLM-based Approaches to Legal Citation Prediction:\n  Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an\n  Australian Law Case Study","summary":"  Large Language Models (LLMs) have demonstrated strong potential across legal\ntasks, yet the problem of legal citation prediction remains under-explored. At\nits core, this task demands fine-grained contextual understanding and precise\nidentification of relevant legislation or precedent. We introduce the AusLaw\nCitation Benchmark, a real-world dataset comprising 55k Australian legal\ninstances and 18,677 unique citations which to the best of our knowledge is the\nfirst of its scale and scope. We then conduct a systematic benchmarking across\na range of solutions: (i) standard prompting of both general and\nlaw-specialised LLMs, (ii) retrieval-only pipelines with both generic and\ndomain-specific embeddings, (iii) supervised fine-tuning, and (iv) several\nhybrid strategies that combine LLMs with retrieval augmentation through query\nexpansion, voting ensembles, or re-ranking. Results show that neither general\nnor law-specific LLMs suffice as stand-alone solutions, with performance near\nzero. Instruction tuning (of even a generic open-source LLM) on task-specific\ndataset is among the best performing solutions. We highlight that database\ngranularity along with the type of embeddings play a critical role in\nretrieval-based approaches, with hybrid methods which utilise a trained\nre-ranker delivering the best results. Despite this, a performance gap of\nnearly 50% remains, underscoring the value of this challenging benchmark as a\nrigorous test-bed for future research in legal-domain.\n","authors":["Jiuzhou Han","Paul Burgess","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2412.06272v2.pdf","comment":"For code, data, and models see https://auslawbench.github.io"},{"id":"http://arxiv.org/abs/2505.14069v2","updated":"2025-05-22T02:39:20Z","published":"2025-05-20T08:21:00Z","title":"Process vs. Outcome Reward: Which is Better for Agentic RAG\n  Reinforcement Learning","summary":"  Retrieval-augmented generation (RAG) enhances the text generation\ncapabilities of large language models (LLMs) by integrating external knowledge\nand up-to-date information. However, traditional RAG systems are limited by\nstatic workflows and lack the adaptability required for multistep reasoning and\ncomplex task management. To address these limitations, agentic RAG systems\n(e.g., DeepResearch) have been proposed, enabling dynamic retrieval strategies,\niterative context refinement, and adaptive workflows for handling complex\nsearch queries beyond the capabilities of conventional RAG. Recent advances,\nsuch as Search-R1, have demonstrated promising gains using outcome-based\nreinforcement learning, where the correctness of the final answer serves as the\nreward signal. Nevertheless, such outcome-supervised agentic RAG methods face\nchallenges including low exploration efficiency, gradient conflict, and sparse\nreward signals. To overcome these challenges, we propose to utilize\nfine-grained, process-level rewards to improve training stability, reduce\ncomputational costs, and enhance efficiency. Specifically, we introduce a novel\nmethod ReasonRAG that automatically constructs RAG-ProGuide, a high-quality\ndataset providing process-level rewards for (i) query generation, (ii) evidence\nextraction, and (iii) answer generation, thereby enhancing model inherent\ncapabilities via process-supervised reinforcement learning. With the\nprocess-level policy optimization, the proposed framework empowers LLMs to\nautonomously invoke search, generate queries, extract relevant evidence, and\nproduce final answers. Compared to existing approaches such as Search-R1 and\ntraditional RAG systems, ReasonRAG, leveraging RAG-ProGuide, achieves superior\nperformance on five benchmark datasets using only 5k training instances,\nsignificantly fewer than the 90k training instances required by Search-R1.\n","authors":["Wenlin Zhang","Xiangyang Li","Kuicai Dong","Yichao Wang","Pengyue Jia","Xiaopeng Li","Yingyi Zhang","Derong Xu","Zhaocheng Du","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.14069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02454v2","updated":"2025-05-22T02:24:27Z","published":"2024-11-03T20:36:44Z","title":"Graph-based Confidence Calibration for Large Language Models","summary":"  Reliable confidence estimation is essential for enhancing the trustworthiness\nof large language models (LLMs), especially in high-stakes scenarios. Despite\nits importance, accurately estimating confidence in LLM responses remains a\nsignificant challenge. In this work, we propose using an auxiliary learning\nmodel to assess response correctness based on the self-consistency of multiple\noutputs generated by the LLM. Our method builds a consistency graph to\nrepresent the agreement among multiple responses and uses a graph neural\nnetwork (GNN) to estimate the likelihood that each response is correct.\nExperiments demonstrate that this method has strong calibration performance on\nvarious benchmark datasets and generalizes well to out-of-domain cases.\n","authors":["Yukun Li","Sijia Wang","Lifu Huang","Li-Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2411.02454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16133v1","updated":"2025-05-22T02:22:11Z","published":"2025-05-22T02:22:11Z","title":"HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine\n  Retrieval and Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) encounters efficiency challenges when\nscaling to massive knowledge bases while preserving contextual relevance. We\npropose Hash-RAG, a framework that integrates deep hashing techniques with\nsystematic optimizations to address these limitations. Our queries directly\nlearn binary hash codes from knowledgebase code, eliminating intermediate\nfeature extraction steps, and significantly reducing storage and computational\noverhead. Building upon this hash-based efficient retrieval framework, we\nestablish the foundation for fine-grained chunking. Consequently, we design a\nPrompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved\nhash-indexed propositions and their original document segments through prompt\nengineering to enhance the LLM's contextual awareness. Experimental evaluations\non NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a\n90% reduction in retrieval time compared to conventional methods while\nmaintaining considerate recall performance. Additionally, The proposed system\noutperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.\n","authors":["Jinyu Guo","Xunlei Chen","Qiyang Xia","Zhaokun Wang","Jie Ou","Libo Qin","Shunyu Yao","Wenhong Tian"],"pdf_url":"https://arxiv.org/pdf/2505.16133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16121v1","updated":"2025-05-22T01:54:58Z","published":"2025-05-22T01:54:58Z","title":"Emotion-based Recommender System","summary":"  Recommender system is one of the most critical technologies for large\ninternet companies such as Amazon and TikTok. Although millions of users use\nrecommender systems globally everyday, and indeed, much data analysis work has\nbeen done to improve the technical accuracy of the system, to our limited\nknowledge, there has been little attention paid to analysis of users' emotion\nin recommender systems. In this paper, we create a new theory and metrics that\ncould capture users' emotion when they are interacting with recommender\nsystems. We also provide effective and efficient visualization techniques for\nvisualization of users' emotion and its change in the customers' lifetime\ncycle. In the end, we design a framework for emotion-based recommendation\nalgorithms, illustrated in a straightforward example with experimental results\nto demonstrate the effectiveness of our new theory.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12459v2","updated":"2025-05-22T00:20:35Z","published":"2024-12-17T01:43:44Z","title":"LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework","summary":"  Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.\n","authors":["Chia-Hsuan Chang","Jui-Tse Tsai","Yi-Hang Tsai","San-Yih Hwang"],"pdf_url":"https://arxiv.org/pdf/2412.12459v2.pdf","comment":"Accepted to PAKDD 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.17022v1","updated":"2025-05-22T17:59:58Z","published":"2025-05-22T17:59:58Z","title":"GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning","summary":"  Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.\n","authors":["Chengqi Duan","Rongyao Fang","Yuqing Wang","Kun Wang","Linjiang Huang","Xingyu Zeng","Hongsheng Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2505.17022v1.pdf","comment":"Github page refer to: https://github.com/gogoduan/GoT-R1"},{"id":"http://arxiv.org/abs/2505.16980v1","updated":"2025-05-22T17:52:34Z","published":"2025-05-22T17:52:34Z","title":"Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose\n  Interaction","summary":"  Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.\n","authors":["Dong Li","Wenqi Zhong","Wei Yu","Yingwei Pan","Dingwen Zhang","Ting Yao","Junwei Han","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2505.16980v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.16977v1","updated":"2025-05-22T17:52:13Z","published":"2025-05-22T17:52:13Z","title":"Incorporating Visual Correspondence into Diffusion Model for Virtual\n  Try-On","summary":"  Diffusion models have shown preliminary success in virtual try-on (VTON)\ntask. The typical dual-branch architecture comprises two UNets for implicit\ngarment deformation and synthesized image generation respectively, and has\nemerged as the recipe for VTON task. Nevertheless, the problem remains\nchallenging to preserve the shape and every detail of the given garment due to\nthe intrinsic stochasticity of diffusion model. To alleviate this issue, we\nnovelly propose to explicitly capitalize on visual correspondence as the prior\nto tame diffusion process instead of simply feeding the whole garment into UNet\nas the appearance reference. Specifically, we interpret the fine-grained\nappearance and texture details as a set of structured semantic points, and\nmatch the semantic points rooted in garment to the ones over target person\nthrough local flow warping. Such 2D points are then augmented into 3D-aware\ncues with depth/normal map of target person. The correspondence mimics the way\nof putting clothing on human body and the 3D-aware cues act as semantic point\nmatching to supervise diffusion model training. A point-focused diffusion loss\nis further devised to fully take the advantage of semantic point matching.\nExtensive experiments demonstrate strong garment detail preservation of our\napproach, evidenced by state-of-the-art VTON performances on both VITON-HD and\nDressCode datasets. Code is publicly available at:\nhttps://github.com/HiDream-ai/SPM-Diff.\n","authors":["Siqi Wan","Jingwen Chen","Yingwei Pan","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2505.16977v1.pdf","comment":"ICLR 2025. Code is publicly available at:\n  https://github.com/HiDream-ai/SPM-Diff"},{"id":"http://arxiv.org/abs/2505.16976v1","updated":"2025-05-22T17:51:50Z","published":"2025-05-22T17:51:50Z","title":"Creatively Upscaling Images with Global-Regional Priors","summary":"  Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2505.16976v1.pdf","comment":"International Journal of Computer Vision (IJCV) 2025"},{"id":"http://arxiv.org/abs/2505.16756v1","updated":"2025-05-22T14:59:30Z","published":"2025-05-22T14:59:30Z","title":"Representation Discrepancy Bridging Method for Remote Sensing Image-Text\n  Retrieval","summary":"  Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in\ngeographic information interpretation, disaster monitoring, and urban planning\nby establishing semantic associations between image and textual descriptions.\nExisting Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language\nPre-training (VLP) models typically adopt symmetric adapter structures for\nexploring cross-modal correlations. However, the strong discriminative nature\nof text modality may dominate the optimization process and inhibits image\nrepresentation learning. The nonnegligible imbalanced cross-modal optimization\nremains a bottleneck to enhancing the model performance. To address this issue,\nthis study proposes a Representation Discrepancy Bridging (RDB) method for the\nRSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is\ndesigned to enable modality-specific optimization and improve feature\nalignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text\nSemantic Adapter (TSA). VEA mines fine-grained image features by Differential\nAttention (DA) mechanism, while TSA identifies key textual semantics through\nHierarchical Attention (HA) mechanism. On the other hand, this study extends\nthe traditional single-task retrieval framework to a dual-task optimization\nframework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves\ncross-modal alignment robustness through an adaptive weighted combination of\ncross-modal, classification, and exponential moving average consistency\nconstraints. Experiments on RSICD and RSITMD datasets show that the proposed\nRDB method achieves a 6%-11% improvement in mR metrics compared to\nstate-of-the-art PEFT methods and a 1.15%-2% improvement over the full\nfine-tuned GeoRSCLIP model.\n","authors":["Hailong Ning","Siying Wang","Tao Lei","Xiaopeng Cao","Huanmin Dou","Bin Zhao","Asoke K. Nandi","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2505.16756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16663v1","updated":"2025-05-22T13:27:54Z","published":"2025-05-22T13:27:54Z","title":"CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation","summary":"  Embodied navigation demands comprehensive scene understanding and precise\nspatial reasoning. While image-text models excel at interpreting pixel-level\ncolor and lighting cues, 3D-text models capture volumetric structure and\nspatial relationships. However, unified fusion approaches that jointly fuse 2D\nimages, 3D point clouds, and textual instructions face challenges in limited\navailability of triple-modality data and difficulty resolving conflicting\nbeliefs among modalities. In this work, we introduce CoNav, a collaborative\ncross-modal reasoning framework where a pretrained 3D-text model explicitly\nguides an image-text navigation agent by providing structured spatial-semantic\nknowledge to resolve ambiguities during navigation. Specifically, we introduce\nCross-Modal Belief Alignment, which operationalizes this cross-modal guidance\nby simply sharing textual hypotheses from the 3D-text model to the navigation\nagent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the\nnavigation agent learns to integrate visual cues with spatial-semantic\nknowledge derived from the 3D-text model, enabling effective reasoning in\nembodied navigation. CoNav achieves significant improvements on four standard\nembodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial\nreasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success\nRate, CoNav often generates shorter paths compared to other methods (as\nmeasured by SPL), showcasing the potential and challenges of fusing data from\ndifferent modalities in embodied navigation. Project Page:\nhttps://oceanhao.github.io/CoNav/\n","authors":["Haihong Hao","Mingfei Han","Changlin Li","Zhihui Li","Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2505.16663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02575v2","updated":"2025-05-22T09:51:35Z","published":"2024-12-03T17:02:40Z","title":"Copy-Move Forgery Detection and Question Answering for Remote Sensing\n  Image","summary":"  Driven by practical demands in land resource monitoring and national defense\nsecurity, this paper introduces the Remote Sensing Copy-Move Question Answering\n(RSCMQA) task. Unlike traditional Remote Sensing Visual Question Answering\n(RSVQA), RSCMQA focuses on interpreting complex tampering scenarios and\ninferring relationships between objects. We present a suite of global RSCMQA\ndatasets, comprising images from 29 different regions across 14 countries.\nSpecifically, we propose five distinct datasets, including the basic dataset\nRS-CMQA, the category-balanced dataset RS-CMQA-B, the high-authenticity dataset\nReal-RSCM, the extended dataset RS-TQA, and the extended category-balanced\ndataset RS-TQA-B. These datasets fill a critical gap in the field while\nensuring comprehensiveness, balance, and challenge. Furthermore, we introduce a\nregion-discrimination-guided multimodal copy-move forgery perception framework\n(CMFPF), which enhances the accuracy of answering questions about tampered\nimages by leveraging prompt about the differences and connections between the\nsource and tampered domains. Extensive experiments demonstrate that our method\nprovides a stronger benchmark for RSCMQA compared to general VQA and RSVQA\nmodels. Our datasets and code are publicly available at\nhttps://github.com/shenyedepisa/RSCMQA.\n","authors":["Ze Zhang","Enyuan Zhao","Di Niu","Jie Nie","Xinyue Liang","Lei Huang"],"pdf_url":"https://arxiv.org/pdf/2412.02575v2.pdf","comment":"11 figs, 7 tables"},{"id":"http://arxiv.org/abs/2505.16434v1","updated":"2025-05-22T09:18:51Z","published":"2025-05-22T09:18:51Z","title":"Joint Flow And Feature Refinement Using Attention For Video Restoration","summary":"  Recent advancements in video restoration have focused on recovering\nhigh-quality video frames from low-quality inputs. Compared with static images,\nthe performance of video restoration significantly depends on efficient\nexploitation of temporal correlations among successive video frames. The\nnumerous techniques make use of temporal information via flow-based strategies\nor recurrent architectures. However, these methods often encounter difficulties\nin preserving temporal consistency as they utilize degraded input video frames.\nTo resolve this issue, we propose a novel video restoration framework named\nJoint Flow and Feature Refinement using Attention (JFFRA). The proposed JFFRA\nis based on key philosophy of iteratively enhancing data through the\nsynergistic collaboration of flow (alignment) and restoration. By leveraging\npreviously enhanced features to refine flow and vice versa, JFFRA enables\nefficient feature enhancement using temporal information. This interplay\nbetween flow and restoration is executed at multiple scales, reducing the\ndependence on precise flow estimation. Moreover, we incorporate an\nocclusion-aware temporal loss function to enhance the network's capability in\neliminating flickering artifacts. Comprehensive experiments validate the\nversatility of JFFRA across various restoration tasks such as denoising,\ndeblurring, and super-resolution. Our method demonstrates a remarkable\nperformance improvement of up to 1.62 dB compared to state-of-the-art\napproaches.\n","authors":["Ranjith Merugu","Mohammad Sameer Suhail","Akshay P Sarashetti","Venkata Bharath Reddy Reddem","Pankaj Kumar Bajpai","Amit Satish Unde"],"pdf_url":"https://arxiv.org/pdf/2505.16434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16279v1","updated":"2025-05-22T06:23:05Z","published":"2025-05-22T06:23:05Z","title":"MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie\n  Dubbing","summary":"  Current movie dubbing technology can produce the desired speech using a\nreference voice and input video, maintaining perfect synchronization with the\nvisuals while effectively conveying the intended emotions. However, crucial\naspects of movie dubbing, including adaptation to various dubbing styles,\neffective handling of dialogue, narration, and monologues, as well as\nconsideration of subtle details such as speaker age and gender, remain\ninsufficiently explored. To tackle these challenges, we introduce a multi-modal\ngenerative framework. First, it utilizes a multi-modal large vision-language\nmodel (VLM) to analyze visual inputs, enabling the recognition of dubbing types\nand fine-grained attributes. Second, it produces high-quality dubbing using\nlarge speech generation models, guided by multi-modal inputs. Additionally, a\nmovie dubbing dataset with annotations for dubbing types and subtle details is\nconstructed to enhance movie understanding and improve dubbing quality for the\nproposed multi-modal framework. Experimental results across multiple benchmark\ndatasets show superior performance compared to state-of-the-art (SOTA) methods.\nIn details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to\n1.09%, 8.80%, 19.08%, and 18.74%, respectively.\n","authors":["Junjie Zheng","Zihao Chen","Chaofan Ding","Yunming Liang","Yihan Fan","Huan Yang","Lei Xie","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2505.16279v1.pdf","comment":"5 pages, 4 figures, accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.16256v1","updated":"2025-05-22T05:46:14Z","published":"2025-05-22T05:46:14Z","title":"DualComp: End-to-End Learning of a Unified Dual-Modality Lossless\n  Compressor","summary":"  Most learning-based lossless compressors are designed for a single modality,\nrequiring separate models for multi-modal data and lacking flexibility.\nHowever, different modalities vary significantly in format and statistical\nproperties, making it ineffective to use compressors that lack\nmodality-specific adaptations. While multi-modal large language models (MLLMs)\noffer a potential solution for modality-unified compression, their excessive\ncomplexity hinders practical deployment. To address these challenges, we focus\non the two most common modalities, image and text, and propose DualComp, the\nfirst unified and lightweight learning-based dual-modality lossless compressor.\nBuilt on a lightweight backbone, DualComp incorporates three key structural\nenhancements to handle modality heterogeneity: modality-unified tokenization,\nmodality-switching contextual learning, and modality-routing\nmixture-of-experts. A reparameterization training strategy is also used to\nboost compression performance. DualComp integrates both modality-specific and\nshared parameters for efficient parameter utilization, enabling near real-time\ninference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp\nachieves compression performance on par with the SOTA LLM-based methods for\nboth text and image datasets. Its simplified single-modality variant surpasses\nthe previous best image compressor on the Kodak dataset by about 9% using just\n1.2% of the model size.\n","authors":["Yan Zhao","Zhengxue Cheng","Junxuan Zhang","Qunshan Gu","Qi Wang","Li Song"],"pdf_url":"https://arxiv.org/pdf/2505.16256v1.pdf","comment":"18 pages, 11 figures, 7 tables"}]},"2025-05-21T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.16065v1","updated":"2025-05-21T22:33:40Z","published":"2025-05-21T22:33:40Z","title":"Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated\n  Synthetic Data Augmentation","summary":"  Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.\n","authors":["Ruijie Xi","He Ba","Hao Yuan","Rishu Agrawal","Arul Prakash"],"pdf_url":"https://arxiv.org/pdf/2505.16065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09861v2","updated":"2025-05-21T22:19:59Z","published":"2025-05-14T23:54:57Z","title":"LiDDA: Data Driven Attribution at LinkedIn","summary":"  Data Driven Attribution, which assigns conversion credits to marketing\ninteractions based on causal patterns learned from data, is the foundation of\nmodern marketing intelligence and vital to any marketing businesses and\nadvertising platform. In this paper, we introduce a unified transformer-based\nattribution approach that can handle member-level data, aggregate-level data,\nand integration of external macro factors. We detail the large scale\nimplementation of the approach at LinkedIn, showcasing significant impact. We\nalso share learning and insights that are broadly applicable to the marketing\nand ad tech fields.\n","authors":["John Bencina","Erkut Aykutlug","Yue Chen","Zerui Zhang","Stephanie Sorenson","Shao Tang","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16101v2","updated":"2025-05-21T21:12:31Z","published":"2025-02-22T05:50:15Z","title":"Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the\n  Robustness of RAG Against Misleading Retrievals","summary":"  Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to handle misleading retrievals and often fail to maintain their own\nreasoning when exposed to conflicting or selectively-framed evidence, making\nthem vulnerable to real-world misinformation. In such real-world retrieval\nscenarios, misleading and conflicting information is rampant, particularly in\nthe political domain, where evidence is often selectively framed, incomplete,\nor polarized. However, existing RAG benchmarks largely assume a clean retrieval\nsetting, where models succeed by accurately retrieving and generating answers\nfrom gold-standard documents. This assumption fails to align with real-world\nconditions, leading to an overestimation of RAG system performance. To bridge\nthis gap, we introduce RAGuard, a fact-checking dataset designed to evaluate\nthe robustness of RAG systems against misleading retrievals. Unlike prior\nbenchmarks that rely on synthetic noise, our dataset constructs its retrieval\ncorpus from Reddit discussions, capturing naturally occurring misinformation.\nIt categorizes retrieved evidence into three types: supporting, misleading, and\nirrelevant, providing a realistic and challenging testbed for assessing how\nwell RAG systems navigate different retrieval information. Our benchmark\nexperiments reveal that when exposed to misleading retrievals, all tested\nLLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no\nretrieval at all), highlighting their susceptibility to noisy environments. To\nthe best of our knowledge, RAGuard is the first benchmark to systematically\nassess RAG robustness against misleading evidence. We expect this benchmark\nwill drive future research toward improving RAG systems beyond idealized\ndatasets, making them more reliable for real-world applications.\n","authors":["Linda Zeng","Rithwik Gupta","Divij Motwani","Diji Yang","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.16101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17125v1","updated":"2025-05-21T21:03:37Z","published":"2025-05-21T21:03:37Z","title":"NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record\n  Extraction","summary":"  Effective evaluation of web data record extraction methods is crucial, yet\nhampered by static, domain-specific benchmarks and opaque scoring practices.\nThis makes fair comparison between traditional algorithmic techniques, which\nrely on structural heuristics, and Large Language Model (LLM)-based approaches,\noffering zero-shot extraction across diverse layouts, particularly challenging.\nTo overcome these limitations, we introduce a concrete evaluation framework.\nOur framework systematically generates evaluation datasets from arbitrary MHTML\nsnapshots, annotates XPath-based supervision labels, and employs\nstructure-aware metrics for consistent scoring, specifically preventing text\nhallucination and allowing only for the assessment of positional hallucination.\nIt also incorporates preprocessing strategies to optimize input for LLMs while\npreserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON.\nAdditionally, we created a publicly available synthetic dataset by transforming\nDOM structures and modifying content. We benchmark deterministic heuristic\nalgorithms and off-the-shelf LLMs across these multiple input formats. Our\nbenchmarking shows that Flat JSON input enables LLMs to achieve superior\nextraction accuracy (F1 score of 0.9567) and minimal hallucination compared to\nother input formats like Slimmed HTML and Hierarchical JSON. We establish a\nstandardized foundation for rigorous benchmarking, paving the way for the next\nprincipled advancements in web data record extraction.\n","authors":["Soyeon Kim","Namhee Kim","Yeonwoo Jeong"],"pdf_url":"https://arxiv.org/pdf/2505.17125v1.pdf","comment":"Web Data Record Extraction, Zero-Shot Extraction, Large Language\n  Models (LLMs) Evaluation Framework, Comparative Analysis"},{"id":"http://arxiv.org/abs/2504.20090v2","updated":"2025-05-21T18:26:09Z","published":"2025-04-25T20:33:57Z","title":"Spark: A System for Scientifically Creative Idea Generation","summary":"  Recently, large language models (LLMs) have shown promising abilities to\ngenerate novel research ideas in science, a direction which coincides with many\nfoundational principles in computational creativity (CC). In light of these\ndevelopments, we present an idea generation system named Spark that couples\nretrieval-augmented idea generation using LLMs with a reviewer model named\nJudge trained on 600K scientific reviews from OpenReview. Our work is both a\nsystem demonstration and intended to inspire other CC researchers to explore\ngrounding the generation and evaluation of scientific ideas within foundational\nCC principles. To this end, we release the annotated dataset used to train\nJudge, inviting other researchers to explore the use of LLMs for idea\ngeneration and creative evaluations.\n","authors":["Aishik Sanyal","Samuel Schapiro","Sumuk Shashidhar","Royce Moon","Lav R. Varshney","Dilek Hakkani-Tur"],"pdf_url":"https://arxiv.org/pdf/2504.20090v2.pdf","comment":"Accepted at ICCC 2025"},{"id":"http://arxiv.org/abs/2505.15807v1","updated":"2025-05-21T17:59:01Z","published":"2025-05-21T17:59:01Z","title":"The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation","summary":"  Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.\n","authors":["Patrick Kahardipraja","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2505.15807v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2505.15776v1","updated":"2025-05-21T17:27:42Z","published":"2025-05-21T17:27:42Z","title":"ConvSearch-R1: Enhancing Query Reformulation for Conversational Search\n  with Reasoning via Reinforcement Learning","summary":"  Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.\n","authors":["Changtai Zhu","Siyin Wang","Ruijun Feng","Kai Song","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.15776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11651v2","updated":"2025-05-21T17:26:12Z","published":"2025-05-16T19:22:19Z","title":"MIRACL-VISION: A Large, multilingual, visual document retrieval\n  benchmark","summary":"  Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval.\n","authors":["Radek Osmulski","Gabriel de Souza P. Moreira","Ronay Ak","Mengyao Xu","Benedikt Schifferer","Even Oldridge"],"pdf_url":"https://arxiv.org/pdf/2505.11651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09847v2","updated":"2025-05-21T16:12:30Z","published":"2025-05-14T23:12:20Z","title":"Causal Predictive Optimization and Generation for Business AI","summary":"  The sales process involves sales functions converting leads or opportunities\nto customers and selling more products to existing customers. The optimization\nof the sales process thus is key to success of any B2B business. In this work,\nwe introduce a principled approach to sales optimization and business AI,\nnamely the Causal Predictive Optimization and Generation, which includes three\nlayers: 1) prediction layer with causal ML 2) optimization layer with\nconstraint optimization and contextual bandit 3) serving layer with Generative\nAI and feedback-loop for system enhancement. We detail the implementation and\ndeployment of the system in LinkedIn, showcasing significant wins over legacy\nsystems and sharing learning and insight broadly applicable to this field.\n","authors":["Liyang Zhao","Olurotimi Seton","Himadeep Reddy Reddivari","Suvendu Jena","Shadow Zhao","Rachit Kumar","Changshuai Wei"],"pdf_url":"https://arxiv.org/pdf/2505.09847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15874v1","updated":"2025-05-21T15:40:53Z","published":"2025-05-21T15:40:53Z","title":"Text-to-Pipeline: Bridging Natural Language and Data Preparation\n  Pipelines","summary":"  Data preparation (DP) transforms raw data into a form suitable for downstream\napplications, typically by composing operations into executable pipelines.\nBuilding such pipelines is time-consuming and requires sophisticated\nprogramming skills. If we can build the pipelines with natural language (NL),\nthe technical barrier of DP will be significantly reduced. However,\nconstructing DP pipelines from NL instructions remains underexplored. To fill\nthe gap, we introduce Text-to-Pipeline, a new task that translates NL data\npreparation instructions into DP pipelines. Furthermore, we develop a benchmark\nnamed PARROT to support systematic evaluation. To simulate realistic DP\nscenarios, we mined transformation patterns from production pipelines and\ninstantiated them on 23,009 real-world tables collected from six public\nsources. The resulting benchmark comprises ~18,000 pipelines covering 16 core\nDP operators. We evaluated cutting-edge large language models on PARROTand\nobserved that they only solved 72.86% of the cases, revealing notable\nlimitations in instruction understanding and multi-step reasoning. To address\nthis, we propose Pipeline-Agent, a stronger baseline that iteratively predicts\nand executes operations with intermediate table feedback, achieving the best\nperformance of 76.17%. Despite this improvement, there remains substantial room\nfor progress on Text-to-Pipeline. Our data, codes, and evaluation tools are\navailable at https://anonymous.4open.science/r/Text-to-Pipeline.\n","authors":["Yuhang Ge","Yachuan Liu","Yuren Mao","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2505.15874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10900v2","updated":"2025-05-21T15:33:20Z","published":"2025-05-16T06:07:19Z","title":"Explain What You Mean: Intent Augmented Knowledge Graph Recommender\n  Built With An LLM","summary":"  Interaction sparsity is a long-standing challenge in recommendation systems.\nSparsity manifests in environments with disproportional cardinality of\ngroupings of entities, such as users and products in an online marketplace. It\nis also found for newly introduced entities, described as the cold-start\nproblem. Recent efforts to mitigate this issue either enrich the connectivity\ndata by incorporating social networks or external knowledge graphs, or\nfine-tune LLMs into interaction augmenters or next-item recommenders. However,\nthese techniques tend to be resource demanding, requiring high computational\npower. They also have several limitations, including data availability, low\nquality, or synthetic noise issues. In this work, we propose LLM-based Intent\nKnowledge Graph Recommender (IKGR), a novel framework that leverages\nretrieval-augmented generation and an encoding approach to construct and\ndensify a knowledge graph. IKGR leverages latent user-item affinities from an\ninteraction knowledge graph and further densifies it through mutual intent\nconnectivity. This addresses sparsity issues and allows the model to make\nintent-grounded recommendations with an interpretable embedding translation\nlayer. Through extensive experiments on real-world datasets, we demonstrate\nthat IKGR overcomes knowledge gaps and achieves substantial gains over\nstate-of-the-art baselines on both publicly available and our internal\nrecommendation datasets.\n","authors":["Wenqing Zheng","Noah Fatsi","Daniel Barcklow","Dmitri Kalaev","Steven Yao","Owen Reinert","C. Bayan Bruss","Daniele Rosa"],"pdf_url":"https://arxiv.org/pdf/2505.10900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15636v1","updated":"2025-05-21T15:18:53Z","published":"2025-05-21T15:18:53Z","title":"Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest\n  Neighbor Search","summary":"  Nearest neighbor search is central in machine learning, information\nretrieval, and databases. For high-dimensional datasets, graph-based methods\nsuch as HNSW, DiskANN, and NSG have become popular thanks to their empirical\naccuracy and efficiency. These methods construct a directed graph over the\ndataset and perform beam search on the graph to find nodes close to a given\nquery. While significant work has focused on practical refinements and\ntheoretical understanding of graph-based methods, many questions remain. We\npropose a new distance-based termination condition for beam search to replace\nthe commonly used condition based on beam width. We prove that, as long as the\nsearch graph is navigable, our resulting Adaptive Beam Search method is\nguaranteed to approximately solve the nearest-neighbor problem, establishing a\nconnection between navigability and the performance of graph-based search. We\nalso provide extensive experiments on our new termination condition for both\nnavigable graphs and approximately navigable graphs used in practice, such as\nHNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard\nbeam search over a range of recall values, data sets, graph constructions, and\ntarget number of nearest neighbors. It thus provides a simple and practical way\nto improve the performance of popular methods.\n","authors":["Yousef Al-Jazzazi","Haya Diwan","Jinrui Gou","Cameron Musco","Christopher Musco","Torsten Suel"],"pdf_url":"https://arxiv.org/pdf/2505.15636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17822v2","updated":"2025-05-21T15:05:27Z","published":"2025-01-29T18:14:51Z","title":"Aggregation Schemes for Single-Vector WSI Representation Learning in\n  Digital Pathology","summary":"  A crucial step to efficiently integrate Whole Slide Images (WSIs) in\ncomputational pathology is assigning a single high-quality feature vector,\ni.e., one embedding, to each WSI. With the existence of many pre-trained deep\nneural networks and the emergence of foundation models, extracting embeddings\nfor sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,\ngiven their high resolution and gigapixel nature, inputting them into existing\nGPUs as a single image is not feasible. As a result, WSIs are usually split\ninto many patches. Feeding each patch to a pre-trained model, each WSI can then\nbe represented by a set of patches, hence, a set of embeddings. Hence, in such\na setup, WSI representation learning reduces to set representation learning\nwhere for each WSI we have access to a set of patch embeddings. To obtain a\nsingle embedding from a set of patch embeddings for each WSI, multiple\nset-based learning schemes have been proposed in the literature. In this paper,\nwe evaluate the WSI search performance of multiple recently developed\naggregation techniques (mainly set representation learning techniques)\nincluding simple average or max pooling operations, Deep Sets, Memory networks,\nFocal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse\nand binary Fisher Vector on four different primary sites including bladder,\nbreast, kidney, and Colon from TCGA. Further, we benchmark the search\nperformance of these methods against the median of minimum distances of patch\nembeddings, a non-aggregating approach used for WSI retrieval.\n","authors":["Sobhan Hemati","Ghazal Alabtah","Saghir Alfasly","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2501.17822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15585v1","updated":"2025-05-21T14:40:27Z","published":"2025-05-21T14:40:27Z","title":"MIRB: Mathematical Information Retrieval Benchmark","summary":"  Mathematical Information Retrieval (MIR) is the task of retrieving\ninformation from mathematical documents and plays a key role in various\napplications, including theorem search in mathematical libraries, answer\nretrieval on math forums, and premise selection in automated theorem proving.\nHowever, a unified benchmark for evaluating these diverse retrieval tasks has\nbeen lacking. In this paper, we introduce MIRB (Mathematical Information\nRetrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\nincludes four tasks: semantic statement retrieval, question-answer retrieval,\npremise retrieval, and formula retrieval, spanning a total of 12 datasets. We\nevaluate 13 retrieval models on this benchmark and analyze the challenges\ninherent to MIR. We hope that MIRB provides a comprehensive framework for\nevaluating MIR systems and helps advance the development of more effective\nretrieval models tailored to the mathematical domain.\n","authors":["Haocheng Ju","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2505.15585v1.pdf","comment":"Our code and data are available at https://github.com/j991222/mirb\n  and https://huggingface.co/collections/hcju/mirb-6827001711765454f58c5a76"},{"id":"http://arxiv.org/abs/2505.15561v1","updated":"2025-05-21T14:18:01Z","published":"2025-05-21T14:18:01Z","title":"Do RAG Systems Suffer From Positional Bias?","summary":"  Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.\n","authors":["Florin Cuconasu","Simone Filice","Guy Horowitz","Yoelle Maarek","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2505.15561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15507v1","updated":"2025-05-21T13:27:14Z","published":"2025-05-21T13:27:14Z","title":"Directional Non-Commutative Monoidal Structures for Compositional\n  Embeddings in Machine Learning","summary":"  We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.\n","authors":["Mahesh Godavarti"],"pdf_url":"https://arxiv.org/pdf/2505.15507v1.pdf","comment":"11 pages submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2502.20937v2","updated":"2025-05-21T11:54:57Z","published":"2025-02-28T10:46:56Z","title":"Variations in Relevance Judgments and the Shelf Life of Test Collections","summary":"  The fundamental property of Cranfield-style evaluations, that system rankings\nare stable even when assessors disagree on individual relevance decisions, was\nvalidated on traditional test collections. However, the paradigm shift towards\nneural retrieval models affected the characteristics of modern test\ncollections, e.g., documents are short, judged with four grades of relevance,\nand information needs have no descriptions or narratives. Under these changes,\nit is unclear whether assessor disagreement remains negligible for system\ncomparisons. We investigate this aspect under the additional condition that the\nfew modern test collections are heavily re-used. Given more possible query\ninterpretations due to less formalized information needs, an ``expiration\ndate'' for test collections might be needed if top-effectiveness requires\noverfitting to a single interpretation of relevance. We run a reproducibility\nstudy and re-annotate the relevance judgments of the 2019~TREC Deep Learning\ntrack. We can reproduce prior work in the neural retrieval setting, showing\nthat assessor disagreement does not affect system rankings. However, we observe\nthat some models substantially degrade with our new relevance judgments, and\nsome have already reached the effectiveness of humans as rankers, providing\nevidence that test collections can expire.\n","authors":["Andrew Parry","Maik Frbe","Harrisen Scells","Ferdinand Schlatt","Guglielmo Faggioli","Saber Zerhoudi","Sean MacAvaney","Eugene Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20937v2.pdf","comment":"11 pages, 6 tables, 5 figures, Accepted to SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.15394v1","updated":"2025-05-21T11:35:11Z","published":"2025-05-21T11:35:11Z","title":"Reranking with Compressed Document Representation","summary":"  Reranking, the process of refining the output of a first-stage retriever, is\noften considered computationally expensive, especially with Large Language\nModels. Borrowing from recent advances in document compression for RAG, we\nreduce the input size by compressing documents into fixed-size embedding\nrepresentations. We then teach a reranker to use compressed inputs by\ndistillation. Although based on a billion-size model, our trained reranker\nusing this compressed input can challenge smaller rerankers in terms of both\neffectiveness and efficiency, especially for long documents. Given that text\ncompressors are still in their early development stages, we view this approach\nas promising.\n","authors":["Herv Djean","Stphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2505.15394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08728v2","updated":"2025-05-21T09:45:04Z","published":"2025-05-13T16:39:00Z","title":"Securing RAG: A Risk Assessment and Mitigation Framework","summary":"  Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems.\n","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"pdf_url":"https://arxiv.org/pdf/2505.08728v2.pdf","comment":"8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally.\n  This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.11638v2","updated":"2025-05-21T09:22:08Z","published":"2024-07-16T11:58:54Z","title":"A Comprehensive Evaluation of Large Language Models on Temporal Event\n  Forecasting","summary":"  Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation (RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast, fine-tuning\nLLMs with raw texts can significantly improve performance. Additionally, LLMs\nenhanced with retrieval modules can effectively capture temporal relational\npatterns hidden in historical events. However, issues such as popularity bias\nand the long-tail problem persist in LLMs, particularly in the\nretrieval-augmented generation (RAG) method. These findings not only deepen our\nunderstanding of LLM-based event forecasting methods but also highlight several\npromising research directions. We consider that this comprehensive evaluation,\nalong with the identified research opportunities, will significantly contribute\nto future research on temporal event forecasting through LLMs.\n","authors":["He Chang","Chenchen Ye","Zhulin Tao","Jie Wu","Zhengmao Yang","Yunshan Ma","Xianglin Huang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.11638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15210v1","updated":"2025-05-21T07:38:45Z","published":"2025-05-21T07:38:45Z","title":"Deliberation on Priors: Trustworthy Reasoning of Large Language Models\n  on Knowledge Graphs","summary":"  Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.\n","authors":["Jie Ma","Ning Qu","Zhitao Gao","Rui Xing","Jun Liu","Hongbin Pei","Jiang Xie","Linyun Song","Pinghui Wang","Jing Tao","Zhou Su"],"pdf_url":"https://arxiv.org/pdf/2505.15210v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2504.07112v2","updated":"2025-05-21T05:57:26Z","published":"2025-03-20T08:38:57Z","title":"Are AI Agents interacting with Online Ads?","summary":"  As AI-driven agents become increasingly integrated into the digital\necosystem, they reshape how online advertising is perceived and processed.\nParticularly in the travel and hotel booking sector, these autonomous systems\ninfluence the effectiveness of traditional advertising formats. While visual\ncues and emotional appeals sway human users, AI agents prioritize structured\ndata such as price, availability, and specifications. This study examines how\ndifferent AI agents interact with online advertising, whether they incorporate\nads into their decision-making processes, and which ad formats prove most\neffective. We analyze interaction patterns, click behavior, and decision-making\nstrategies through experiments with multimodal language models such as OpenAI\nGPT-4o, Anthropic Claude, and Google Gemini 2.0 Flash. Our findings reveal that\nAI agents neither ignore nor systematically avoid advertisements but instead\nfavor certain features-particularly keywords and structured data. These\ninsights have significant implications for the future design of advertising\nstrategies in AI-dominated digital environments.\n","authors":["Andreas Stckl","Joel Nitu"],"pdf_url":"https://arxiv.org/pdf/2504.07112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15128v1","updated":"2025-05-21T05:31:49Z","published":"2025-05-21T05:31:49Z","title":"Robust Relevance Feedback for Interactive Known-Item Video Search","summary":"  Known-item search (KIS) involves only a single search target, making\nrelevance feedback-typically a powerful technique for efficiently identifying\nmultiple positive examples to infer user intent-inapplicable. PicHunter\naddresses this issue by asking users to select the top-k most similar examples\nto the unique search target from a displayed set. Under ideal conditions, when\nthe user's perception aligns closely with the machine's perception of\nsimilarity, consistent and precise judgments can elevate the target to the top\nposition within a few iterations. However, in practical scenarios, expecting\nusers to provide consistent judgments is often unrealistic, especially when the\nunderlying embedding features used for similarity measurements lack\ninterpretability. To enhance robustness, we first introduce a pairwise relative\njudgment feedback that improves the stability of top-k selections by mitigating\nthe impact of misaligned feedback. Then, we decompose user perception into\nmultiple sub-perceptions, each represented as an independent embedding space.\nThis approach assumes that users may not consistently align with a single\nrepresentation but are more likely to align with one or several among multiple\nrepresentations. We develop a predictive user model that estimates the\ncombination of sub-perceptions based on each user feedback instance. The\npredictive user model is then trained to filter out the misaligned\nsub-perceptions. Experimental evaluations on the large-scale open-domain\ndataset V3C indicate that the proposed model can optimize over 60% search\ntargets to the top rank when their initial ranks at the search depth between 10\nand 50. Even for targets initially ranked between 1,000 and 5,000, the model\nachieves a success rate exceeding 40% in optimizing ranks to the top,\ndemonstrating the enhanced robustness of relevance feedback in KIS despite\ninconsistent feedback.\n","authors":["Zhixin Ma","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2505.15128v1.pdf","comment":"Accepted to ICMR 2025"},{"id":"http://arxiv.org/abs/2411.08334v3","updated":"2025-05-21T05:30:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Moreover, our ablation studies and analyses explicitly verify the\neffectiveness of our framework in mitigating the text-dominant issue. Our code\nis publicly available: https://github.com/yeongjoonJu/MIRe\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v3.pdf","comment":"Accepted to ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2505.15117v1","updated":"2025-05-21T05:09:43Z","published":"2025-05-21T05:09:43Z","title":"An Empirical Study on Reinforcement Learning for Reasoning-Search\n  Interleaved LLM Agents","summary":"  Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.\n","authors":["Bowen Jin","Jinsung Yoon","Priyanka Kargupta","Sercan O. Arik","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2505.15117v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.15107v1","updated":"2025-05-21T05:01:31Z","published":"2025-05-21T05:01:31Z","title":"StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy\n  Optimization","summary":"  Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our\nimplementation is publicly available at\nhttps://github.com/zxh20001117/StepSearch.\n","authors":["Ziliang Wang","Xuhui Zheng","Kang An","Cijun Ouyang","Jialu Cai","Yuhang Wang","Yichao Wu"],"pdf_url":"https://arxiv.org/pdf/2505.15107v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.15859v1","updated":"2025-05-21T04:32:35Z","published":"2025-05-21T04:32:35Z","title":"AutoData: A Multi-Agent System for Open Web Data Collection","summary":"  The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.\n","authors":["Tianyi Ma","Yiyue Qian","Zheyuan Zhang","Zehong Wang","Xiaoye Qian","Feifan Bai","Yifan Ding","Xuwei Luo","Shinan Zhang","Keerthiram Murugesan","Chuxu Zhang","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2505.15859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15072v1","updated":"2025-05-21T03:39:42Z","published":"2025-05-21T03:39:42Z","title":"MoTime: A Dataset Suite for Multimodal Time Series Forecasting","summary":"  While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research.\n","authors":["Xin Zhou","Weiqing Wang","Francisco J. Baldn","Wray Buntine","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2505.15072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15070v1","updated":"2025-05-21T03:35:51Z","published":"2025-05-21T03:35:51Z","title":"An Alternative to FLOPS Regularization to Effectively Productionize\n  SPLADE-Doc","summary":"  Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,\nwhich need to be sparse to leverage inverted index structures during retrieval.\nSPLADE, the most popular LSR model, uses FLOPS regularization to encourage\nvector sparsity during training. However, FLOPS regularization does not ensure\nsparsity among terms - only within a given query or document. Terms with very\nhigh Document Frequencies (DFs) substantially increase latency in production\nretrieval engines, such as Apache Solr, due to their lengthy posting lists. To\naddress the issue of high DFs, we present a new variant of FLOPS\nregularization: DF-FLOPS. This new regularization technique penalizes the usage\nof high-DF terms, thereby shortening posting lists and reducing retrieval\nlatency. Unlike other inference-time sparsification methods, such as stopword\nremoval, DF-FLOPS regularization allows for the selective inclusion of\nhigh-frequency terms in cases where the terms are truly salient. We find that\nDF-FLOPS successfully reduces the prevalence of high-DF terms and lowers\nretrieval latency (around 10x faster) in a production-grade engine while\nmaintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and\ncross-domain (improved performance in 12 out of 13 tasks on which we tested).\nWith retrieval latencies on par with BM25, this work provides an important step\ntowards making LSR practical for deployment in production-grade search engines.\n","authors":["Aldo Porco","Dhruv Mehra","Igor Malioutov","Karthik Radhakrishnan","Moniba Keymanesh","Daniel Preoiuc-Pietro","Sean MacAvaney","Pengxiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2505.15070v1.pdf","comment":"Accepted as a short paper at SIGIR 2025"},{"id":"http://arxiv.org/abs/2505.10043v2","updated":"2025-05-21T03:08:15Z","published":"2025-05-15T07:41:14Z","title":"Boosting Text-to-Chart Retrieval through Training with Synthesized\n  Semantic Insights","summary":"  Charts are crucial for data analysis and decision-making.Text-to-chart\nretrieval systems have become increasingly important for Business Intelligence\n(BI), where users need to find relevant charts that match their analytical\nneeds. These needs can be categorized into precise queries that are\nwell-specified and fuzzy queries that are more exploratory -- both require\nunderstanding the semantics and context of the charts. However, existing\ntext-to-chart retrieval solutions often fail to capture the semantic content\nand contextual information of charts, primarily due to the lack of\ncomprehensive metadata (or semantic insights). To address this limitation, we\npropose a training data development pipeline that automatically synthesizes\nhierarchical semantic insights for charts, covering visual patterns\n(visual-oriented), statistical properties (statistics-oriented), and practical\napplications (task-oriented), which produces 207,498 semantic insights for\n69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to\nlearn better representations of charts for text-to-chart retrieval. Our method\nleverages rich semantic insights during the training phase to develop a model\nthat understands both visual and semantic aspects of charts.To evaluate\ntext-to-chart retrieval performance, we curate the first benchmark, CRBench,\nfor this task with 21,862 charts and 326 text queries from real-world BI\napplications, with ground-truth labels verified by the crowd\nworkers.Experiments show that ChartFinder significantly outperforms existing\nmethods in text-to-chart retrieval tasks across various settings. For precise\nqueries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than\nstate-of-the-art models. In fuzzy query tasks, our method also demonstrates\nconsistent improvements, with an average increase of 5% across nearly all\nmetrics.\n","authors":["Yifan Wu","Lutao Yan","Yizhang Zhu","Yinan Mei","Jiannan Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2505.10043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15042v1","updated":"2025-05-21T02:51:30Z","published":"2025-05-21T02:51:30Z","title":"GitHub Repository Complexity Leads to Diminished Web Archive\n  Availability","summary":"  Software is often developed using versioned controlled software, such as Git,\nand hosted on centralized Web hosts, such as GitHub and GitLab. These Web\nhosted software repositories are made available to users in the form of\ntraditional HTML Web pages for each source file and directory, as well as a\npresentational home page and various descriptive pages. We examined more than\n12,000 Web hosted Git repository project home pages, primarily from GitHub, to\nmeasure how well their presentational components are preserved in the Internet\nArchive, as well as the source trees of the collected GitHub repositories to\nassess the extent to which their source code has been preserved. We found that\nmore than 31% of the archived repository home pages examined exhibited some\nform of minor page damage and 1.6% exhibited major page damage. We also found\nthat of the source trees analyzed, less than 5% of their source files were\narchived, on average, with the majority of repositories not having source files\nsaved in the Internet Archive at all. The highest concentration of archived\nsource files available were those linked directly from repositories' home pages\nat a rate of 14.89% across all available repositories and sharply dropping off\nat deeper levels of a repository's directory tree.\n","authors":["David Calano","Michele C. Weigle","Michael L. Nelson"],"pdf_url":"https://arxiv.org/pdf/2505.15042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15031v1","updated":"2025-05-21T02:26:47Z","published":"2025-05-21T02:26:47Z","title":"Are the confidence scores of reviewers consistent with the review\n  content? Evidence from top conference proceedings in AI","summary":"  Peer review is vital in academia for evaluating research quality. Top AI\nconferences use reviewer confidence scores to ensure review reliability, but\nexisting studies lack fine-grained analysis of text-score consistency,\npotentially missing key details. This work assesses consistency at word,\nsentence, and aspect levels using deep learning and NLP conference review data.\nWe employ deep learning to detect hedge sentences and aspects, then analyze\nreport length, hedge word/sentence frequency, aspect mentions, and sentiment to\nevaluate text-score alignment. Correlation, significance, and regression tests\nexamine confidence scores' impact on paper outcomes. Results show high\ntext-score consistency across all levels, with regression revealing higher\nconfidence scores correlate with paper rejection, validating expert assessments\nand peer review fairness.\n","authors":["Wenqing Wu","Haixu Xi","Chengzhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.15031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14984v1","updated":"2025-05-21T00:09:34Z","published":"2025-05-21T00:09:34Z","title":"CRAFT: Training-Free Cascaded Retrieval for Tabular QA","summary":"  Table Question Answering (TQA) involves retrieving relevant tables from a\nlarge corpus to answer natural language queries. Traditional dense retrieval\nmodels, such as DTR and ColBERT, not only incur high computational costs for\nlarge-scale retrieval tasks but also require retraining or fine-tuning on new\ndatasets, limiting their adaptability to evolving domains and knowledge. In\nthis work, we propose $\\textbf{CRAFT}$, a cascaded retrieval approach that\nfirst uses a sparse retrieval model to filter a subset of candidate tables\nbefore applying more computationally expensive dense models and neural\nre-rankers. Our approach achieves better retrieval performance than\nstate-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further\nenhance table representations by generating table descriptions and titles using\nGemini Flash 1.5. End-to-end TQA results using various Large Language Models\n(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate\n$\\textbf{CRAFT}$ effectiveness.\n","authors":["Adarsh Singh","Kushal Raj Bhandari","Jianxi Gao","Soham Dan","Vivek Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.14984v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.16057v1","updated":"2025-05-21T22:16:59Z","published":"2025-05-21T22:16:59Z","title":"Signals of Provenance: Practices & Challenges of Navigating Indicators\n  in AI-Generated Media for Sighted and Blind Individuals","summary":"  AI-Generated (AIG) content has become increasingly widespread by recent\nadvances in generative models and the easy-to-use tools that have significantly\nlowered the technical barriers for producing highly realistic audio, images,\nand videos through simple natural language prompts. In response, platforms are\nadopting provable provenance with platforms recommending AIG to be\nself-disclosed and signaled to users. However, these indicators may be often\nmissed, especially when they rely solely on visual cues and make them\nineffective to users with different sensory abilities. To address the gap, we\nconducted semi-structured interviews (N=28) with 15 sighted and 13 BLV\nparticipants to examine their interaction with AIG content through\nself-disclosed AI indicators. Our findings reveal diverse mental models and\npractices, highlighting different strengths and weaknesses of content-based\n(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While\nsighted participants leveraged visual and audio cues, BLV participants\nprimarily relied on audio and existing assistive tools, limiting their ability\nto identify AIG. Across both groups, they frequently overlooked menu-aided\nindicators deployed by platforms and rather interacted with content-based\nindicators such as title and comments. We uncovered usability challenges\nstemming from inconsistent indicator placement, unclear metadata, and cognitive\noverload. These issues were especially critical for BLV individuals due to the\ninsufficient accessibility of interface elements. We provide practical\nrecommendations and design implications for future AIG indicators across\nseveral dimensions.\n","authors":["Ayae Ide","Tory Park","Jaron Mink","Tanusree Sharma"],"pdf_url":"https://arxiv.org/pdf/2505.16057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16025v1","updated":"2025-05-21T21:13:19Z","published":"2025-05-21T21:13:19Z","title":"CP-LLM: Context and Pixel Aware Large Language Model for Video Quality\n  Assessment","summary":"  Video quality assessment (VQA) is a challenging research topic with broad\napplications. Effective VQA necessitates sensitivity to pixel-level distortions\nand a comprehensive understanding of video context to accurately determine the\nperceptual impact of distortions. Traditional hand-crafted and learning-based\nVQA models mainly focus on pixel-level distortions and lack contextual\nunderstanding, while recent LLM-based models struggle with sensitivity to small\ndistortions or handle quality scoring and description as separate tasks. To\naddress these shortcomings, we introduce CP-LLM: a Context and Pixel aware\nLarge Language Model. CP-LLM is a novel multimodal LLM architecture featuring\ndual vision encoders designed to independently analyze perceptual quality at\nboth high-level (video context) and low-level (pixel distortion) granularity,\nalong with a language decoder subsequently reasons about the interplay between\nthese aspects. This design enables CP-LLM to simultaneously produce robust\nquality scores and interpretable quality descriptions, with enhanced\nsensitivity to pixel distortions (e.g. compression artifacts). The model is\ntrained via a multi-task pipeline optimizing for score prediction, description\ngeneration, and pairwise comparisons. Experiment results demonstrate that\nCP-LLM achieves state-of-the-art cross-dataset performance on established VQA\nbenchmarks and superior robustness to pixel distortions, confirming its\nefficacy for comprehensive and practical video quality assessment in real-world\nscenarios.\n","authors":["Wen Wen","Yaohong Wu","Yue Sheng","Neil Birkbeck","Balu Adsumilli","Yilin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.16025v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.03948v3","updated":"2025-05-21T19:52:11Z","published":"2024-11-06T14:29:49Z","title":"Long-Form Text-to-Music Generation with Adaptive Prompts: A Case Study\n  in Tabletop Role-Playing Games Soundtracks","summary":"  This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.\n","authors":["Felipe Marra","Lucas N. Ferreira"],"pdf_url":"https://arxiv.org/pdf/2411.03948v3.pdf","comment":"Proceedings of the 1st Latin American Music Information Retrieval\n  Workshop (LAMIR), pg 80"},{"id":"http://arxiv.org/abs/2502.12562v2","updated":"2025-05-21T15:31:15Z","published":"2025-02-18T05:57:35Z","title":"SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings","summary":"  Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.\n","authors":["Weikai Lu","Hao Peng","Huiping Zhuang","Cen Chen","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.12562v2.pdf","comment":"Accepted in ACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2505.15629v1","updated":"2025-05-21T15:15:04Z","published":"2025-05-21T15:15:04Z","title":"Relationship Analysis of Image-Text Pair in SNS Posts","summary":"  Social networking services (SNS) contain vast amounts of image-text posts,\nnecessitating effective analysis of their relationships for improved\ninformation retrieval. This study addresses the classification of image-text\npairs in SNS, overcoming prior limitations in distinguishing relationships\nbeyond similarity. We propose a graph-based method to classify image-text pairs\ninto similar and complementary relationships. Our approach first embeds images\nand text using CLIP, followed by clustering. Next, we construct an Image-Text\nRelationship Clustering Line Graph (ITRC-Line Graph), where clusters serve as\nnodes. Finally, edges and nodes are swapped in a pseudo-graph representation. A\nGraph Convolutional Network (GCN) then learns node and edge representations,\nwhich are fused with the original embeddings for final classification.\nExperimental results on a publicly available dataset demonstrate the\neffectiveness of our method.\n","authors":["Takuto Nabeoka","Yijun Duan","Qiang Ma"],"pdf_url":"https://arxiv.org/pdf/2505.15629v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.17114v1","updated":"2025-05-21T14:33:36Z","published":"2025-05-21T14:33:36Z","title":"RAVEN: Query-Guided Representation Alignment for Question Answering over\n  Audio, Video, Embedded Sensors, and Natural Language","summary":"  Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.\n","authors":["Subrata Biswas","Mohammad Nur Hossain Khan","Bashima Islam"],"pdf_url":"https://arxiv.org/pdf/2505.17114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14336v2","updated":"2025-05-21T14:22:18Z","published":"2025-05-20T13:20:55Z","title":"Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach","summary":"  Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.\n","authors":["Umberto Cappellazzo","Minsu Kim","Stavros Petridis","Daniele Falavigna","Alessio Brutti"],"pdf_url":"https://arxiv.org/pdf/2505.14336v2.pdf","comment":"Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.01237v2","updated":"2025-05-21T13:54:07Z","published":"2025-05-02T12:59:58Z","title":"CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via\n  Fine-Grained Alignment","summary":"  Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.\n","authors":["Edson Araujo","Andrew Rouditchenko","Yuan Gong","Saurabhchand Bhati","Samuel Thomas","Brian Kingsbury","Leonid Karlinsky","Rogerio Feris","James R. Glass","Hilde Kuehne"],"pdf_url":"https://arxiv.org/pdf/2505.01237v2.pdf","comment":"To be published at CVPR 2025, code available at\n  https://github.com/edsonroteia/cav-mae-sync"},{"id":"http://arxiv.org/abs/2505.15489v1","updated":"2025-05-21T13:14:32Z","published":"2025-05-21T13:14:32Z","title":"Seeing Through Deception: Uncovering Misleading Creator Intent in\n  Multimodal News with Vision-Language Models","summary":"  The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation.\n","authors":["Jiaying Wu","Fanxiao Li","Min-Yen Kan","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2505.15489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17104v1","updated":"2025-05-21T09:06:05Z","published":"2025-05-21T09:06:05Z","title":"P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark","summary":"  Academic posters are vital for scholarly communication, yet their manual\ncreation is time-consuming. However, automated academic poster generation faces\nsignificant challenges in preserving intricate scientific details and achieving\neffective visual-textual integration. Existing approaches often struggle with\nsemantic richness and structural nuances, and lack standardized benchmarks for\nevaluating generated academic posters comprehensively. To address these\nlimitations, we introduce P2P, the first flexible, LLM-based multi-agent\nframework that generates high-quality, HTML-rendered academic posters directly\nfrom research papers, demonstrating strong potential for practical\napplications. P2P employs three specialized agents-for visual element\nprocessing, content generation, and final poster assembly-each integrated with\ndedicated checker modules to enable iterative refinement and ensure output\nquality. To foster advancements and rigorous evaluation in this domain, we\nconstruct and release P2PInstruct, the first large-scale instruction dataset\ncomprising over 30,000 high-quality examples tailored for the academic\npaper-to-poster generation task. Furthermore, we establish P2PEval, a\ncomprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation\nmethodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and\ndetailed, human-annotated checklists. Our contributions aim to streamline\nresearch dissemination and provide the community with robust tools for\ndeveloping and evaluating next-generation poster generation systems.\n","authors":["Tao Sun","Enhao Pan","Zhengkai Yang","Kaixin Sui","Jiajun Shi","Xianfu Cheng","Tongliang Li","Wenhao Huang","Ge Zhang","Jian Yang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2505.17104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15154v1","updated":"2025-05-21T06:20:17Z","published":"2025-05-21T06:20:17Z","title":"Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive\n  Routing for Efficient LLM/MLLM Reasoning","summary":"  Recent advancements in reasoning have significantly enhanced the capabilities\nof Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nacross diverse tasks. However, excessive reliance on chain-of-thought (CoT)\nreasoning can impair model performance and brings unnecessarily lengthened\noutputs, reducing efficiency. Our work reveals that prolonged reasoning does\nnot universally improve accuracy and even degrade performance on simpler tasks.\nTo address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel\nframework that dynamically switches between short answers and long-form\nreasoning based on the model perplexity. CAR first generates a short answer and\nevaluates its perplexity, triggering reasoning only when the model exhibits low\nconfidence (i.e., high perplexity). Experiments across diverse multimodal\nVQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both\nshort-answer and long-form reasoning approaches, striking an optimal balance\nbetween accuracy and efficiency.\n","authors":["Jinghui Lu","Haiyang Yu","Siliang Xu","Shiwei Ran","Guozhi Tang","Siqi Wang","Bin Shan","Teng Fu","Hao Feng","Jingqun Tang","Han Wang","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2505.15154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14222v2","updated":"2025-05-21T05:45:46Z","published":"2025-05-20T11:30:28Z","title":"MatchDance: Collaborative Mamba-Transformer Architecture Matching for\n  High-Quality 3D Dance Synthesis","summary":"  Music-to-dance generation represents a challenging yet pivotal task at the\nintersection of choreography, virtual reality, and creative content generation.\nDespite its significance, existing methods face substantial limitation in\nachieving choreographic consistency. To address the challenge, we propose\nMatchDance, a novel framework for music-to-dance generation that constructs a\nlatent representation to enhance choreographic consistency. MatchDance employs\na two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),\nwhich encodes dance motions into a latent representation by Finite Scalar\nQuantization (FSQ) with kinematic-dynamic constraints and reconstructs them\nwith high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),\nwhich uses a Mamba-Transformer hybrid architecture to map music into the latent\nrepresentation, followed by the KDQS decoder to generate 3D dance motions.\nAdditionally, a music-dance retrieval framework and comprehensive metrics are\nintroduced for evaluation. Extensive experiments on the FineDance dataset\ndemonstrate state-of-the-art performance. Code will be released upon\nacceptance.\n","authors":["Kaixing Yang","Xulong Tang","Yuxuan Hu","Jiahao Yang","Hongyan Liu","Qinnan Zhang","Jun He","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2505.14222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15128v1","updated":"2025-05-21T05:31:49Z","published":"2025-05-21T05:31:49Z","title":"Robust Relevance Feedback for Interactive Known-Item Video Search","summary":"  Known-item search (KIS) involves only a single search target, making\nrelevance feedback-typically a powerful technique for efficiently identifying\nmultiple positive examples to infer user intent-inapplicable. PicHunter\naddresses this issue by asking users to select the top-k most similar examples\nto the unique search target from a displayed set. Under ideal conditions, when\nthe user's perception aligns closely with the machine's perception of\nsimilarity, consistent and precise judgments can elevate the target to the top\nposition within a few iterations. However, in practical scenarios, expecting\nusers to provide consistent judgments is often unrealistic, especially when the\nunderlying embedding features used for similarity measurements lack\ninterpretability. To enhance robustness, we first introduce a pairwise relative\njudgment feedback that improves the stability of top-k selections by mitigating\nthe impact of misaligned feedback. Then, we decompose user perception into\nmultiple sub-perceptions, each represented as an independent embedding space.\nThis approach assumes that users may not consistently align with a single\nrepresentation but are more likely to align with one or several among multiple\nrepresentations. We develop a predictive user model that estimates the\ncombination of sub-perceptions based on each user feedback instance. The\npredictive user model is then trained to filter out the misaligned\nsub-perceptions. Experimental evaluations on the large-scale open-domain\ndataset V3C indicate that the proposed model can optimize over 60% search\ntargets to the top rank when their initial ranks at the search depth between 10\nand 50. Even for targets initially ranked between 1,000 and 5,000, the model\nachieves a success rate exceeding 40% in optimizing ranks to the top,\ndemonstrating the enhanced robustness of relevance feedback in KIS despite\ninconsistent feedback.\n","authors":["Zhixin Ma","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2505.15128v1.pdf","comment":"Accepted to ICMR 2025"},{"id":"http://arxiv.org/abs/2411.08334v3","updated":"2025-05-21T05:30:01Z","published":"2024-11-13T04:32:58Z","title":"MIRe: Enhancing Multimodal Queries Representation via Fusion-Free\n  Modality Interaction for Multimodal Retrieval","summary":"  Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Moreover, our ablation studies and analyses explicitly verify the\neffectiveness of our framework in mitigating the text-dominant issue. Our code\nis publicly available: https://github.com/yeongjoonJu/MIRe\n","authors":["Yeong-Joon Ju","Ho-Joong Kim","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2411.08334v3.pdf","comment":"Accepted to ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2505.13062v2","updated":"2025-05-21T05:14:05Z","published":"2025-05-19T12:52:51Z","title":"Hearing from Silence: Reasoning Audio Descriptions from Silent Videos\n  via Vision-Language Model","summary":"  Humans can intuitively infer sounds from silent videos, but whether\nmultimodal large language models can perform modal-mismatch reasoning without\naccessing target modalities remains relatively unexplored. Current\ntext-assisted-video-to-audio (VT2A) methods excel in video foley tasks but\nstruggle to acquire audio descriptions during inference. We introduce the task\nof Reasoning Audio Descriptions from Silent Videos (SVAD) to address this\nchallenge and investigate vision-language models' (VLMs) capabilities on this\ntask. To further enhance the VLMs' reasoning capacity for the SVAD task, we\nconstruct a CoT-AudioCaps dataset and propose a Chain-of-Thought-based\nsupervised fine-tuning strategy. Experiments on SVAD and subsequent VT2A tasks\ndemonstrate our method's effectiveness in two key aspects: significantly\nimproving VLMs' modal-mismatch reasoning for SVAD and effectively addressing\nthe challenge of acquiring audio descriptions during VT2A inference.\n","authors":["Yong Ren","Chenxing Li","Le Xu","Hao Gu","Duzhen Zhang","Yujie Chen","Manjie Xu","Ruibo Fu","Shan Yang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.13062v2.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2410.18322v3","updated":"2025-05-21T03:10:18Z","published":"2024-10-23T23:10:09Z","title":"Unified Microphone Conversion: Many-to-Many Device Mapping via\n  Feature-wise Linear Modulation","summary":"  We present Unified Microphone Conversion, a unified generative framework\ndesigned to bolster sound event classification (SEC) systems against device\nvariability. While our prior CycleGAN-based methods effectively simulate device\ncharacteristics, they require separate models for each device pair, limiting\nscalability. Our approach overcomes this constraint by conditioning the\ngenerator on frequency response data, enabling many-to-many device mappings\nthrough unpaired training. We integrate frequency-response information via\nFeature-wise Linear Modulation, further enhancing scalability. Additionally,\nincorporating synthetic frequency response differences improves the\napplicability of our framework for real-world application. Experimental results\nshow that our method outperforms the state-of-the-art by 2.6% and reduces\nvariability by 0.8% in macro-average F1 score.\n","authors":["Myeonghoon Ryu","Hongseok Oh","Suji Lee","Han Park"],"pdf_url":"https://arxiv.org/pdf/2410.18322v3.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.12332v2","updated":"2025-05-21T02:08:03Z","published":"2025-05-18T09:58:48Z","title":"VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized\n  Diffusion-based Voice Cloning","summary":"  Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning.\n","authors":["Qianyue Hu","Junyan Wu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.12332v2.pdf","comment":null}]},"2025-05-20T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.05353v2","updated":"2025-05-20T22:46:34Z","published":"2024-07-25T22:58:59Z","title":"IntentRec: Predicting User Session Intent with Hierarchical Multi-Task\n  Learning","summary":"  Recommender systems have played a critical role in diverse digital services\nsuch as e-commerce, streaming media, social networks, etc. If we know what a\nuser's intent is in a given session (e.g. do they want to watch short videos or\na movie or play games; are they shopping for a camping trip), it becomes easier\nto provide high-quality recommendations. In this paper, we introduce IntentRec,\na novel recommendation framework based on hierarchical multi-task neural\nnetwork architecture that tries to estimate a user's latent intent using their\nshort- and long-term implicit signals as proxies and uses the intent prediction\nto predict the next item user is likely to engage with. By directly leveraging\nthe intent prediction, we can offer accurate and personalized recommendations\nto users. Our comprehensive experiments on Netflix user engagement data show\nthat IntentRec outperforms the state-of-the-art next-item and next-intent\npredictors. We also share several findings and downstream applications of\nIntentRec.\n","authors":["Sejoon Oh","Moumita Bhattacharya","Yesu Feng","Sudarshan Lamkhede"],"pdf_url":"https://arxiv.org/pdf/2408.05353v2.pdf","comment":"Accepted @ BayLearn 2024"},{"id":"http://arxiv.org/abs/2505.14959v1","updated":"2025-05-20T22:38:50Z","published":"2025-05-20T22:38:50Z","title":"Privacy Preserving Conversion Modeling in Data Clean Room","summary":"  In the realm of online advertising, accurately predicting the conversion rate\n(CVR) is crucial for enhancing advertising efficiency and user satisfaction.\nThis paper addresses the challenge of CVR prediction while adhering to user\nprivacy preferences and advertiser requirements. Traditional methods face\nobstacles such as the reluctance of advertisers to share sensitive conversion\ndata and the limitations of model training in secure environments like data\nclean rooms. We propose a novel model training framework that enables\ncollaborative model training without sharing sample-level gradients with the\nadvertising platform. Our approach introduces several innovative components:\n(1) utilizing batch-level aggregated gradients instead of sample-level\ngradients to minimize privacy risks; (2) applying adapter-based\nparameter-efficient fine-tuning and gradient compression to reduce\ncommunication costs; and (3) employing de-biasing techniques to train the model\nunder label differential privacy, thereby maintaining accuracy despite\nprivacy-enhanced label perturbations. Our experimental results, conducted on\nindustrial datasets, demonstrate that our method achieves competitive ROCAUC\nperformance while significantly decreasing communication overhead and complying\nwith both advertiser privacy requirements and user privacy choices. This\nframework establishes a new standard for privacy-preserving, high-performance\nCVR prediction in the digital advertising landscape.\n","authors":["Kungang Li","Xiangyi Chen","Ling Leng","Jiajing Xu","Jiankai Sun","Behnam Rezaei"],"pdf_url":"https://arxiv.org/pdf/2505.14959v1.pdf","comment":"Published in Proceedings of the 18th ACM Conference on Recommender\n  Systems. 2024 (RecSys '24)"},{"id":"http://arxiv.org/abs/2503.03062v2","updated":"2025-05-20T22:36:09Z","published":"2025-03-04T23:52:49Z","title":"Scaling Laws for Many-Shot In-Context Learning with Self-Generated\n  Annotations","summary":"  The high cost of obtaining high-quality annotated data for in-context\nlearning (ICL) has motivated the development of methods that use self-generated\nannotations in place of ground-truth labels. While these approaches have shown\npromising results in few-shot settings, they generally do not scale to\nmany-shot scenarios. In this work, we study ICL with self-generated examples\nusing a framework analogous to traditional semi-supervised learning, consisting\nof annotation generation, demonstration selection, and in-context inference.\nWithin this framework, we propose a simple baseline that outperforms\nground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we\nobserve a scaling law with this baseline, where optimal performance is achieved\nwith more than 1,000 demonstrations. To fully exploit the many-shot\ncapabilities of semi-supervised ICL, we introduce IterPSD, an iterative\nannotation approach that integrates iterative refinement and curriculum\npseudo-labeling techniques from semi-supervised learning, yielding up to 6.8%\nadditional gains on classification tasks.\n","authors":["Zhengyao Gu","Henry Peng Zou","Yankai Chen","Aiwei Liu","Weizhi Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2503.03062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14901v1","updated":"2025-05-20T20:52:31Z","published":"2025-05-20T20:52:31Z","title":"Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation","summary":"  Bundle recommendation aims to recommend a set of items to each user. However,\nthe sparser interactions between users and bundles raise a big challenge,\nespecially in cold-start scenarios. Traditional collaborative filtering methods\ndo not work well for this kind of problem because these models rely on\ninteractions to update the latent embedding, which is hard to work in a\ncold-start setting. We propose a new approach (DisCo), which relies on a\npersonalized Diffusion backbone, enhanced by disentangled aspects for the\nuser's interest, to generate a bundle in distribution space for each user to\ntackle the cold-start challenge. During the training phase, DisCo adjusts an\nadditional objective loss term to avoid bias, a prevalent issue while using the\ngenerative model for top-$K$ recommendation purposes. Our empirical experiments\nshow that DisCo outperforms five comparative baselines by a large margin on\nthree real-world datasets. Thereby, this study devises a promising framework\nand essential viewpoints in cold-start recommendation. Our materials for\nreproducibility are available at: https://github.com/bt-nghia/DisCo.\n","authors":["Tuan-Nghia Bui","Huy-Son Nguyen","Cam-Van Thi Nguyen","Hoang-Quynh Le","Duc-Trong Le"],"pdf_url":"https://arxiv.org/pdf/2505.14901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06569v2","updated":"2025-05-20T20:24:44Z","published":"2025-05-10T08:50:44Z","title":"MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context\n  RAG","summary":"  Long-context large language models (LC LLMs) combined with\nretrieval-augmented generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained\nwindows, and fragmented information from suboptimal context construction. We\nintroduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG\nframework that compresses and partitions documents into coarse-to-fine\ngranularities, then adaptively merges relevant contexts through real-time\nchunk- and document-level expansions. By initiating with finest-level retrieval\nand progressively incorporating broader, higher-level context, MacRAG\nconstructs effective query-specific long contexts, optimizing both precision\nand coverage. Evaluations on challenging LongBench expansions of HotpotQA,\n2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG\npipelines in single- and multi-step generation using Llama-3.1-8B,\nGemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient,\nscalable solution for real-world long-context, multi-hop reasoning. Our code is\navailable at https://github.com/Leezekun/MacRAG.\n","authors":["Woosang Lim","Zekun Li","Gyuwan Kim","Sungyoung Ji","HyeonJung Kim","Kyuri Choi","Jin Hyuk Lim","Kyungpyo Park","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.06569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15856v1","updated":"2025-05-20T20:11:00Z","published":"2025-05-20T20:11:00Z","title":"DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster\n  Management","summary":"  Effective disaster management requires timely access to accurate and\ncontextually relevant information. Existing Information Retrieval (IR)\nbenchmarks, however, focus primarily on general or specialized domains, such as\nmedicine or finance, neglecting the unique linguistic complexity and diverse\ninformation needs encountered in disaster management scenarios. To bridge this\ngap, we introduce DisastIR, the first comprehensive IR evaluation benchmark\nspecifically tailored for disaster management. DisastIR comprises 9,600 diverse\nuser queries and more than 1.3 million labeled query-passage pairs, covering 48\ndistinct retrieval tasks derived from six search intents and eight general\ndisaster categories that include 301 specific event types. Our evaluations of\n30 state-of-the-art retrieval models demonstrate significant performance\nvariances across tasks, with no single model excelling universally.\nFurthermore, comparative analyses reveal significant performance gaps between\ngeneral-domain and disaster management-specific tasks, highlighting the\nnecessity of disaster management-specific benchmarks for guiding IR model\nselection to support effective decision-making in disaster management\nscenarios. All source codes and DisastIR are available at\nhttps://github.com/KaiYin97/Disaster_IR.\n","authors":["Kai Yin","Xiangjue Dong","Chengkai Liu","Lipai Huang","Yiming Xiao","Zhewei Liu","Ali Mostafavi","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2505.15856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14868v1","updated":"2025-05-20T19:59:41Z","published":"2025-05-20T19:59:41Z","title":"VisTopics: A Visual Semantic Unsupervised Approach to Topic Modeling of\n  Video and Image Data","summary":"  Understanding visual narratives is crucial for examining the evolving\ndynamics of media representation. This study introduces VisTopics, a\ncomputational framework designed to analyze large-scale visual datasets through\nan end-to-end pipeline encompassing frame extraction, deduplication, and\nsemantic clustering. Applying VisTopics to a dataset of 452 NBC News videos\nresulted in reducing 11,070 frames to 6,928 deduplicated frames, which were\nthen semantically analyzed to uncover 35 topics ranging from political events\nto environmental crises. By integrating Latent Dirichlet Allocation with\ncaption-based semantic analysis, VisTopics demonstrates its potential to\nunravel patterns in visual framing across diverse contexts. This approach\nenables longitudinal studies and cross-platform comparisons, shedding light on\nthe intersection of media, technology, and public discourse. The study\nvalidates the method's reliability through human coding accuracy metrics and\nemphasizes its scalability for communication research. By bridging the gap\nbetween visual representation and semantic meaning, VisTopics provides a\ntransformative tool for advancing the methodological toolkit in computational\nmedia studies. Future research may leverage VisTopics for comparative analyses\nacross media outlets or geographic regions, offering insights into the shifting\nlandscapes of media narratives and their societal implications.\n","authors":["Ayse D Lokmanoglu","Dror Walter"],"pdf_url":"https://arxiv.org/pdf/2505.14868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14681v1","updated":"2025-05-20T17:59:16Z","published":"2025-05-20T17:59:16Z","title":"Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training","summary":"  Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.\n","authors":["Mengru Wang","Xingyu Chen","Yue Wang","Zhiwei He","Jiahao Xu","Tian Liang","Qiuzhi Liu","Yunzhi Yao","Wenxuan Wang","Ruotian Ma","Haitao Mi","Ningyu Zhang","Zhaopeng Tu","Xiaolong Li","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.14681v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.14680v1","updated":"2025-05-20T17:59:13Z","published":"2025-05-20T17:59:13Z","title":"NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search","summary":"  Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.\n","authors":["Sunhao Dai","Wenjie Wang","Liang Pang","Jun Xu","See-Kiong Ng","Ji-Rong Wen","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2505.14680v1.pdf","comment":"SIGIR 2025 Perspective Paper"},{"id":"http://arxiv.org/abs/2311.16515v4","updated":"2025-05-20T16:29:22Z","published":"2023-11-25T14:24:49Z","title":"Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval","summary":"  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n","authors":["Delong Liu","Haiwen Li","Zhaohui Hou","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14558v1","updated":"2025-05-20T16:15:30Z","published":"2025-05-20T16:15:30Z","title":"R2MED: A Benchmark for Reasoning-Driven Medical Retrieval","summary":"  Current medical retrieval benchmarks primarily emphasize lexical or shallow\nsemantic similarity, overlooking the reasoning-intensive demands that are\ncentral to clinical decision-making. In practice, physicians often retrieve\nauthoritative medical evidence to support diagnostic hypotheses. Such evidence\ntypically aligns with an inferred diagnosis rather than the surface form of a\npatient's symptoms, leading to low lexical or semantic overlap between queries\nand relevant documents. To address this gap, we introduce R2MED, the first\nbenchmark explicitly designed for reasoning-driven medical retrieval. It\ncomprises 876 queries spanning three tasks: Q&A reference retrieval, clinical\nevidence retrieval, and clinical case retrieval. These tasks are drawn from\nfive representative medical scenarios and twelve body systems, capturing the\ncomplexity and diversity of real-world medical information needs. We evaluate\n15 widely-used retrieval systems on R2MED and find that even the best model\nachieves only 31.4 nDCG@10, demonstrating the benchmark's difficulty. Classical\nre-ranking and generation-augmented retrieval methods offer only modest\nimprovements. Although large reasoning models improve performance via\nintermediate inference generation, the best results still peak at 41.4 nDCG@10.\nThese findings underscore a substantial gap between current retrieval\ntechniques and the reasoning demands of real clinical tasks. We release R2MED\nas a challenging benchmark to foster the development of next-generation medical\nretrieval systems with enhanced reasoning capabilities. Data and code are\navailable at https://github.com/R2MED/R2MED\n","authors":["Lei Li","Xiao Zhou","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.14558v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2501.08828v2","updated":"2025-05-20T14:49:55Z","published":"2025-01-15T14:30:13Z","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","summary":"  Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.\n","authors":["Kuicai Dong","Yujing Chang","Xin Deik Goh","Dexun Li","Ruiming Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08828v2.pdf","comment":"https://huggingface.co/MMDocIR"},{"id":"http://arxiv.org/abs/2505.14432v1","updated":"2025-05-20T14:39:34Z","published":"2025-05-20T14:39:34Z","title":"Rank-K: Test-Time Reasoning for Listwise Reranking","summary":"  Retrieve-and-rerank is a popular retrieval pipeline because of its ability to\nmake slow but effective rerankers efficient enough at query time by reducing\nthe number of comparisons. Recent works in neural rerankers take advantage of\nlarge language models for their capability in reasoning between queries and\npassages and have achieved state-of-the-art retrieval effectiveness. However,\nsuch rerankers are resource-intensive, even after heavy optimization. In this\nwork, we introduce Rank-K, a listwise passage reranking model that leverages\nthe reasoning capability of the reasoning language model at query time that\nprovides test time scalability to serve hard queries. We show that Rank-K\nimproves retrieval effectiveness by 23\\% over the RankZephyr, the\nstate-of-the-art listwise reranker, when reranking a BM25 initial ranked list\nand 19\\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is\ninherently a multilingual model, we found that it ranks passages based on\nqueries in different languages as effectively as it does in monolingual\nretrieval.\n","authors":["Eugene Yang","Andrew Yates","Kathryn Ricci","Orion Weller","Vivek Chari","Benjamin Van Durme","Dawn Lawrie"],"pdf_url":"https://arxiv.org/pdf/2505.14432v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.14310v1","updated":"2025-05-20T12:59:16Z","published":"2025-05-20T12:59:16Z","title":"Taming Recommendation Bias with Causal Intervention on Evolving Personal\n  Popularity","summary":"  Popularity bias occurs when popular items are recommended far more frequently\nthan they should be, negatively impacting both user experience and\nrecommendation accuracy. Existing debiasing methods mitigate popularity bias\noften uniformly across all users and only partially consider the time evolution\nof users or items. However, users have different levels of preference for item\npopularity, and this preference is evolving over time. To address these issues,\nwe propose a novel method called CausalEPP (Causal Intervention on Evolving\nPersonal Popularity) for taming recommendation bias, which accounts for the\nevolving personal popularity of users. Specifically, we first introduce a\nmetric called {Evolving Personal Popularity} to quantify each user's preference\nfor popular items. Then, we design a causal graph that integrates evolving\npersonal popularity into the conformity effect, and apply deconfounded training\nto mitigate the popularity bias of the causal graph. During inference, we\nconsider the evolution consistency between users and items to achieve a better\nrecommendation. Empirical studies demonstrate that CausalEPP outperforms\nbaseline methods in reducing popularity bias while improving recommendation\naccuracy.\n","authors":["Shiyin Tan","Dongyuan Li","Renhe Jiang","Zhen Wang","Xingtong Yu","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2505.14310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05863v2","updated":"2025-05-20T12:37:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14242v1","updated":"2025-05-20T11:52:17Z","published":"2025-05-20T11:52:17Z","title":"Technical Report on classification of literature related to children\n  speech disorder","summary":"  This technical report presents a natural language processing (NLP)-based\napproach for systematically classifying scientific literature on childhood\nspeech disorders. We retrieved and filtered 4,804 relevant articles published\nafter 2015 from the PubMed database using domain-specific keywords. After\ncleaning and pre-processing the abstracts, we applied two topic modeling\ntechniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify\nlatent thematic structures in the corpus. Our models uncovered 14 clinically\nmeaningful clusters, such as infantile hyperactivity and abnormal epileptic\nbehavior. To improve relevance and precision, we incorporated a custom stop\nword list tailored to speech pathology. Evaluation results showed that the LDA\nmodel achieved a coherence score of 0.42 and a perplexity of -7.5, indicating\nstrong topic coherence and predictive performance. The BERTopic model exhibited\na low proportion of outlier topics (less than 20%), demonstrating its capacity\nto classify heterogeneous literature effectively. These results provide a\nfoundation for automating literature reviews in speech-language pathology.\n","authors":["Ziang Wang","Amir Aryani"],"pdf_url":"https://arxiv.org/pdf/2505.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14241v1","updated":"2025-05-20T11:47:56Z","published":"2025-05-20T11:47:56Z","title":"The Limits of Graph Samplers for Training Inductive Recommender Systems:\n  Extended results","summary":"  Inductive Recommender Systems are capable of recommending for new users and\nwith new items thus avoiding the need to retrain after new data reaches the\nsystem. However, these methods are still trained on all the data available,\nrequiring multiple days to train a single model, without counting\nhyperparameter tuning. In this work we focus on graph-based recommender\nsystems, i.e., systems that model the data as a heterogeneous network. In other\napplications, graph sampling allows to study a subgraph and generalize the\nfindings to the original graph. Thus, we investigate the applicability of\nsampling techniques for this task. We test on three real world datasets, with\nthree state-of-the-art inductive methods, and using six different sampling\nmethods. We find that its possible to maintain performance using only 50% of\nthe training data with up to 86% percent decrease in training time; however,\nusing less training data leads to far worse performance. Further, we find that\nwhen it comes to data for recommendations, graph sampling should also account\nfor the temporal dimension. Therefore, we find that if higher data reduction is\nneeded, new graph based sampling techniques should be studied and new inductive\nmethods should be designed.\n","authors":["Theis E. Jendal","Matteo Lissandrini","Peter Dolog","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2505.14241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02589v3","updated":"2025-05-20T10:58:29Z","published":"2025-03-04T13:12:39Z","title":"MCiteBench: A Multimodal Benchmark for Generating Text with Citations","summary":"  Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, leaving the challenges of multimodal scenarios\nlargely unexplored. In this paper, we introduce MCiteBench, the first benchmark\ndesigned to assess the ability of MLLMs to generate text with citations in\nmultimodal contexts. Our benchmark comprises data derived from academic papers\nand review-rebuttal interactions, featuring diverse information sources and\nmultimodal content. Experimental results reveal that MLLMs struggle to ground\ntheir outputs reliably when handling multimodal input. Further analysis\nuncovers a systematic modality bias and reveals how models internally rely on\ndifferent sources when generating citations, offering insights into model\nbehavior and guiding future directions for multimodal citation tasks.\n","authors":["Caiyu Hu","Yikai Zhang","Tinghui Zhu","Yiwei Ye","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.02589v3.pdf","comment":"https://caiyuhu.github.io/MCiteBench/"},{"id":"http://arxiv.org/abs/2505.14180v1","updated":"2025-05-20T10:36:25Z","published":"2025-05-20T10:36:25Z","title":"Bridge the Gap between Past and Future: Siamese Model Optimization for\n  Context-Aware Document Ranking","summary":"  In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods.\n","authors":["Songhao Wu","Quan Tu","Mingjie Zhong","Hong Liu","Jia Xu","Jinjie Gu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14179v1","updated":"2025-05-20T10:34:45Z","published":"2025-05-20T10:34:45Z","title":"Enhancing Abstractive Summarization of Scientific Papers Using Structure\n  Information","summary":"  Abstractive summarization of scientific papers has always been a research\nfocus, yet existing methods face two main challenges. First, most summarization\nmodels rely on Encoder-Decoder architectures that treat papers as sequences of\nwords, thus fail to fully capture the structured information inherent in\nscientific papers. Second, existing research often use keyword mapping or\nfeature engineering to identify the structural information, but these methods\nstruggle with the structural flexibility of scientific papers and lack\nrobustness across different disciplines. To address these challenges, we\npropose a two-stage abstractive summarization framework that leverages\nautomatic recognition of structural functions within scientific papers. In the\nfirst stage, we standardize chapter titles from numerous scientific papers and\nconstruct a large-scale dataset for structural function recognition. A\nclassifier is then trained to automatically identify the key structural\ncomponents (e.g., Background, Methods, Results, Discussion), which provides a\nfoundation for generating more balanced summaries. In the second stage, we\nemploy Longformer to capture rich contextual relationships across sections and\ngenerating context-aware summaries. Experiments conducted on two\ndomain-specific scientific paper summarization datasets demonstrate that our\nmethod outperforms advanced baselines, and generates more comprehensive\nsummaries. The code and dataset can be accessed at\nhttps://github.com/tongbao96/code-for-SFR-AS.\n","authors":["Tong Bao","Heng Zhang","Chengzhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14156v1","updated":"2025-05-20T10:05:06Z","published":"2025-05-20T10:05:06Z","title":"Unify Graph Learning with Text: Unleashing LLM Potentials for Session\n  Search","summary":"  Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.\n","authors":["Songhao Wu","Quan Tu","Hong Liu","Jia Xu","Zhongyi Liu","Guannan Zhang","Ran Wang","Xiuying Chen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2505.14156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14149v1","updated":"2025-05-20T09:57:34Z","published":"2025-05-20T09:57:34Z","title":"Enhancing Keyphrase Extraction from Academic Articles Using Section\n  Structure Information","summary":"  The exponential increase in academic papers has significantly increased the\ntime required for researchers to access relevant literature. Keyphrase\nExtraction (KPE) offers a solution to this situation by enabling researchers to\nefficiently retrieve relevant literature. The current study on KPE from\nacademic articles aims to improve the performance of extraction models through\ninnovative approaches using Title and Abstract as input corpora. However, the\nsemantic richness of keywords is significantly constrained by the length of the\nabstract. While full-text-based KPE can address this issue, it simultaneously\nintroduces noise, which significantly diminishes KPE performance. To address\nthis issue, this paper utilized the structural features and section texts\nobtained from the section structure information of academic articles to extract\nkeyphrase from academic papers. The approach consists of two main parts: (1)\nexploring the effect of seven structural features on KPE models, and (2)\nintegrating the extraction results from all section texts used as input corpora\nfor KPE models via a keyphrase integration algorithm to obtain the keyphrase\nintegration result. Furthermore, this paper also examined the effect of the\nclassification quality of section structure on the KPE performance. The results\nshow that incorporating structural features improves KPE performance, though\ndifferent features have varying effects on model efficacy. The keyphrase\nintegration approach yields the best performance, and the classification\nquality of section structure can affect KPE performance. These findings\nindicate that using the section structure information of academic articles\ncontributes to effective KPE from academic articles. The code and dataset\nsupporting this study are available at https://github.com/yan-xinyi/SSB_KPE.\n","authors":["Chengzhi Zhang","Xinyi Yan","Lei Zhao","Yingyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.14149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14099v1","updated":"2025-05-20T09:01:52Z","published":"2025-05-20T09:01:52Z","title":"Beyond Chains: Bridging Large Language Models and Knowledge Bases in\n  Complex Question Answering","summary":"  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions using structured knowledge from KBs. While LLM-only approaches offer\ngeneralization, they suffer from outdated knowledge, hallucinations, and lack\nof transparency. Chain-based KG-RAG methods address these issues by\nincorporating external KBs, but are limited to simple chain-structured\nquestions due to the absence of planning and logical structuring. Inspired by\nsemantic parsing methods, we propose PDRR: a four-stage framework consisting of\nPredict, Decompose, Retrieve, and Reason. Our method first predicts the\nquestion type and decomposes the question into structured triples. Then\nretrieves relevant information from KBs and guides the LLM as an agent to\nreason over and complete the decomposed triples. Experimental results\ndemonstrate that PDRR consistently outperforms existing methods across various\nLLM backbones and achieves superior performance on both chain-structured and\nnon-chain complex questions.\n","authors":["Yihua Zhu","Qianying Liu","Akiko Aizawa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2505.14099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14057v1","updated":"2025-05-20T08:02:41Z","published":"2025-05-20T08:02:41Z","title":"Field Matters: A lightweight LLM-enhanced Method for CTR Prediction","summary":"  Click-through rate (CTR) prediction is a fundamental task in modern\nrecommender systems. In recent years, the integration of large language models\n(LLMs) has been shown to effectively enhance the performance of traditional CTR\nmethods. However, existing LLM-enhanced methods often require extensive\nprocessing of detailed textual descriptions for large-scale instances or\nuser/item entities, leading to substantial computational overhead. To address\nthis challenge, this work introduces LLaCTR, a novel and lightweight\nLLM-enhanced CTR method that employs a field-level enhancement paradigm.\nSpecifically, LLaCTR first utilizes LLMs to distill crucial and lightweight\nsemantic knowledge from small-scale feature fields through self-supervised\nfield-feature fine-tuning. Subsequently, it leverages this field-level semantic\nknowledge to enhance both feature representation and feature interactions. In\nour experiments, we integrate LLaCTR with six representative CTR models across\nfour datasets, demonstrating its superior performance in terms of both\neffectiveness and efficiency compared to existing LLM-enhanced methods. Our\ncode is available at https://anonymous.4open.science/r/LLaCTR-EC46.\n","authors":["Yu Cui","Feng Liu","Jiawei Chen","Xingyu Lou","Changwang Zhang","Jun Wang","Yuegang Sun","Xiaohu Yang","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2505.14057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12499v2","updated":"2025-05-20T07:25:42Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14020v1","updated":"2025-05-20T07:22:03Z","published":"2025-05-20T07:22:03Z","title":"Disentangled Multi-span Evolutionary Network against Temporal Knowledge\n  Graph Reasoning","summary":"  Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs\n(KGs), incorporate the temporal feature to express the transience of knowledge\nby describing when facts occur. TKG extrapolation aims to infer possible future\nfacts based on known history, which has garnered significant attention in\nrecent years. Some existing methods treat TKG as a sequence of independent\nsubgraphs to model temporal evolution patterns, demonstrating impressive\nreasoning performance. However, they still have limitations: 1) In modeling\nsubgraph semantic evolution, they usually neglect the internal structural\ninteractions between subgraphs, which are actually crucial for encoding TKGs.\n2) They overlook the potential smooth features that do not lead to semantic\nchanges, which should be distinguished from the semantic evolution process.\nTherefore, we propose a novel Disentangled Multi-span Evolutionary Network\n(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution\nstrategy that captures local neighbor features while perceiving historical\nneighbor semantic information, thus enabling internal interactions between\nsubgraphs during the evolution process. To maximize the capture of semantic\nchange patterns, we design a disentangle component that adaptively separates\nnodes' active and stable features, used to dynamically control the influence of\nhistorical semantics on future evolution. Extensive experiments conducted on\nfour real-world TKG datasets show that DiMNet demonstrates substantial\nperformance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%\nin MRR.\n","authors":["Hao Dong","Ziyue Qiao","Zhiyuan Ning","Qi Hao","Yi Du","Pengyang Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.14020v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2505.10940v2","updated":"2025-05-20T06:58:19Z","published":"2025-05-16T07:26:41Z","title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced\n  Logical Recommendation","summary":"  Recommender systems filter contents/items valuable to users by inferring\npreferences from user features and historical behaviors. Mainstream approaches\nfollow the learning-to-rank paradigm, which focus on discovering and modeling\nitem topics (e.g., categories), and capturing user preferences on these topics\nbased on historical interactions. However, this paradigm often neglects the\nmodeling of user characteristics and their social roles, which are logical\nconfounders influencing the correlated interest and user preference transition.\nTo bridge this gap, we introduce the user role identification task and the\nbehavioral logic modeling task that aim to explicitly model user roles and\nlearn the logical relations between item topics and user social roles. We show\nthat it is possible to explicitly solve these tasks through an efficient\nintegration framework of Large Language Model (LLM) and recommendation systems,\nfor which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal)\nLLM's world knowledge and logic inference ability to extract realistic\ntag-based virtual logic graphs that reveal dynamic and expressive knowledge of\nusers, refining our understanding of user behaviors. On the other hand, TagCF\npresents empirically effective integration modules that take advantage of the\nextracted tag-logic information, augmenting the recommendation performance. We\nconduct both online experiments and offline experiments with industrial and\npublic datasets as verification of TagCF's effectiveness, and we empirically\nshow that the user role modeling strategy is potentially a better choice than\nthe modeling of item topics. Additionally, we provide evidence that the\nextracted logic graphs are empirically a general and transferable knowledge\nthat can benefit a wide range of recommendation tasks.\n","authors":["Qing Yu","Xiaobei Wang","Shuchang Liu","Yandong Bai","Xiaoyu Yang","Xueliang Wang","Chang Meng","Shanshan Wu","Hailan Yang","Huihui Xiao","Xiang Li","Fan Yang","Xiaoqiang Feng","Lantao Hu","Han Li","Kun Gai","Lixin Zou"],"pdf_url":"https://arxiv.org/pdf/2505.10940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13994v1","updated":"2025-05-20T06:44:34Z","published":"2025-05-20T06:44:34Z","title":"Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven\n  Graph Partitioning","summary":"  Retrieval-Augmented Generation (RAG) systems empower large language models\n(LLMs) with external knowledge, yet struggle with efficiency-accuracy\ntrade-offs when scaling to large knowledge graphs. Existing approaches often\nrely on monolithic graph retrieval, incurring unnecessary latency for simple\nqueries and fragmented reasoning for complex multi-hop questions. To address\nthese challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework\nthat addresses these limitations with question-driven semantic graph\npartitioning and collaborative subgraph retrieval. The innovative framework\nfirst create Semantic Partitioning of Linked Information, then use the\nType-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware\ngraph segmentation manages to divide knowledge graphs into semantically\ncoherent subgraphs, ensuring subgraphs align with different query types, while\nlightweight LLM agents are assigned to partitioned subgraphs, and only relevant\npartitions are activated during retrieval, thus reduce search space while\nenhancing efficiency. Finally, a hierarchical merging module resolves\ninconsistencies across subgraph-derived answers through logical verifications.\nExtensive experimental validation demonstrates considerable improvements\ncompared to existing approaches.\n","authors":["Ruiyi Yang","Hao Xue","Imran Razzak","Hakim Hacid","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2505.13994v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.13974v1","updated":"2025-05-20T06:13:53Z","published":"2025-05-20T06:13:53Z","title":"DIFF: Dual Side-Information Filtering and Fusion for Sequential\n  Recommendation","summary":"  Side-information Integrated Sequential Recommendation (SISR) benefits from\nauxiliary item information to infer hidden user preferences, which is\nparticularly effective for sparse interactions and cold-start scenarios.\nHowever, existing studies face two main challenges. (i) They fail to remove\nnoisy signals in item sequence and (ii) they underutilize the potential of\nside-information integration. To tackle these issues, we propose a novel SISR\nmodel, Dual Side-Information Filtering and Fusion (DIFF), which employs\nfrequency-based noise filtering and dual multi-sequence fusion. Specifically,\nwe convert the item sequence to the frequency domain to filter out noisy\nshort-term fluctuations in user interests. We then combine early and\nintermediate fusion to capture diverse relationships across item IDs and\nattributes. Thanks to our innovative filtering and fusion strategy, DIFF is\nmore robust in learning subtle and complex item correlations in the sequence.\nDIFF outperforms state-of-the-art SISR models, achieving improvements of up to\n14.1% and 12.5% in Recall@20 and NDCG@20 across four benchmark datasets.\n","authors":["Hye-young Kim","Minjin Choi","Sunkyung Lee","Ilwoong Baek","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2505.13974v1.pdf","comment":"Accepted by SIGIR 2025. 10 pages"},{"id":"http://arxiv.org/abs/2411.13892v2","updated":"2025-05-20T05:38:53Z","published":"2024-11-21T07:12:47Z","title":"How Does Topology Bias Distort Message Passing? A Dirichlet Energy\n  Perspective","summary":"  Graph-based recommender systems have achieved remarkable effectiveness by\nmodeling high-order interactions between users and items. However, such\napproaches are significantly undermined by popularity bias, which distorts the\ninteraction graph's structure, referred to as topology bias. This leads to\noverrepresentation of popular items, thereby reinforcing biases and fairness\nissues through the user-system feedback loop. Despite attempts to study this\neffect, most prior work focuses on the embedding or gradient level bias,\noverlooking how topology bias fundamentally distorts the message passing\nprocess itself. We bridge this gap by providing an empirical and theoretical\nanalysis from a Dirichlet energy perspective, revealing that graph message\npassing inherently amplifies topology bias and consistently benefits highly\nconnected nodes. To address these limitations, we propose Test-time Simplicial\nPropagation (TSP), which extends message passing to higher-order simplicial\ncomplexes. By incorporating richer structures beyond pairwise connections, TSP\nmitigates harmful topology bias and substantially improves the representation\nand recommendation of long-tail items during inference. Extensive experiments\nacross five real-world datasets demonstrate the superiority of our approach in\nmitigating topology bias and enhancing recommendation quality.\n","authors":["Yanbiao Ji","Yue Ding","Dan Luo","Chang Liu","Yuxiang Lu","Xin Xin","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.13892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13950v1","updated":"2025-05-20T05:29:01Z","published":"2025-05-20T05:29:01Z","title":"Benchmarking the Myopic Trap: Positional Bias in Information Retrieval","summary":"  This study investigates a specific form of positional bias, termed the Myopic\nTrap, where retrieval models disproportionately attend to the early parts of\ndocuments while overlooking relevant information that appears later. To\nsystematically quantify this phenomenon, we propose a semantics-preserving\nevaluation framework that repurposes the existing NLP datasets into\nposition-aware retrieval benchmarks. By evaluating the SOTA models of full\nretrieval pipeline, including BM25, embedding models, ColBERT-style\nlate-interaction models, and reranker models, we offer a broader empirical\nperspective on positional bias than prior work. Experimental results show that\nembedding models and ColBERT-style models exhibit significant performance\ndegradation when query-related content is shifted toward later positions,\nindicating a pronounced head bias. Notably, under the same training\nconfiguration, ColBERT-style approach show greater potential for mitigating\npositional bias compared to the traditional single-vector approach. In\ncontrast, BM25 and reranker models remain largely unaffected by such\nperturbations, underscoring their robustness to positional bias. Code and data\nare publicly available at: www.github.com/NovaSearch-Team/RAG-Retrieval.\n","authors":["Ziyang Zeng","Dun Zhang","Jiacheng Li","Panxiang Zou","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2505.13950v1.pdf","comment":"10 pages, 3 figures, 4 tables. Under review"},{"id":"http://arxiv.org/abs/2504.01281v3","updated":"2025-05-20T04:52:21Z","published":"2025-04-02T01:16:10Z","title":"Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding","summary":"  We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.\n","authors":["Sakhinana Sagar Srinivas","Akash Das","Shivam Gupta","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2504.01281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13928v1","updated":"2025-05-20T04:49:09Z","published":"2025-05-20T04:49:09Z","title":"LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts","summary":"  Long videos contain a vast amount of information, making video-text retrieval\nan essential and challenging task in multimodal learning. However, existing\nbenchmarks suffer from limited video duration, low-quality captions, and coarse\nannotation granularity, which hinder the evaluation of advanced video-text\nretrieval methods. To address these limitations, we introduce LoVR, a benchmark\nspecifically designed for long video-text retrieval. LoVR contains 467 long\nvideos and over 40,804 fine-grained clips with high-quality captions. To\novercome the issue of poor machine-generated annotations, we propose an\nefficient caption generation framework that integrates VLM automatic\ngeneration, caption quality scoring, and dynamic refinement. This pipeline\nimproves annotation accuracy while maintaining scalability. Furthermore, we\nintroduce a semantic fusion method to generate coherent full-video captions\nwithout losing important contextual information. Our benchmark introduces\nlonger videos, more detailed captions, and a larger-scale dataset, presenting\nnew challenges for video understanding and retrieval. Extensive experiments on\nvarious advanced embedding models demonstrate that LoVR is a challenging\nbenchmark, revealing the limitations of current approaches and providing\nvaluable insights for future research. We release the code and dataset link at\nhttps://github.com/TechNomad-ds/LoVR-benchmark\n","authors":["Qifeng Cai","Hao Liang","Hejun Dong","Meiyi Qiang","Ruichuan An","Zhaoyang Han","Zhengzhou Zhu","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.13928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13895v1","updated":"2025-05-20T03:59:26Z","published":"2025-05-20T03:59:26Z","title":"VulCPE: Context-Aware Cybersecurity Vulnerability Retrieval and\n  Management","summary":"  The dynamic landscape of cybersecurity demands precise and scalable solutions\nfor vulnerability management in heterogeneous systems, where\nconfiguration-specific vulnerabilities are often misidentified due to\ninconsistent data in databases like the National Vulnerability Database (NVD).\nInaccurate Common Platform Enumeration (CPE) data in NVD further leads to false\npositives and incomplete vulnerability retrieval. Informed by our systematic\nanalysis of CPE and CVEdeails data, revealing more than 50% vendor name\ninconsistencies, we propose VulCPE, a framework that standardizes data and\nmodels configuration dependencies using a unified CPE schema (uCPE), entity\nrecognition, relation extraction, and graph-based modeling. VulCPE achieves\nsuperior retrieval precision (0.766) and coverage (0.926) over existing tools.\nVulCPE ensures precise, context-aware vulnerability management, enhancing cyber\nresilience.\n","authors":["Yuning Jiang","Feiyang Shang","Freedy Tan Wei You","Huilin Wang","Chia Ren Cong","Qiaoran Meng","Nay Oo","Hoon Wei Lim","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2505.13895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13881v1","updated":"2025-05-20T03:36:54Z","published":"2025-05-20T03:36:54Z","title":"TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias\n  Intrinsically from Regression Models in Recommender Systems","summary":"  Regression models are crucial in recommender systems. However,\nretransformation bias problem has been conspicuously neglected within the\ncommunity. While many works in other fields have devised effective bias\ncorrection methods, all of them are post-hoc cures externally to the model,\nfacing practical challenges when applied to real-world recommender systems.\nHence, we propose a preemptive paradigm to eradicate the bias intrinsically\nfrom the models via minor model refinement. Specifically, a novel TranSUN\nmethod is proposed with a joint bias learning manner to offer theoretically\nguaranteed unbiasedness under empirical superior convergence. It is further\ngeneralized into a novel generic regression model family, termed Generalized\nTranSUN (GTS), which not only offers more theoretical insights but also serves\nas a generic framework for flexibly developing various bias-free models.\nComprehensive experimental results demonstrate the superiority of our methods\nacross data from various domains, which have been successfully deployed in two\nreal-world industrial recommendation scenarios, i.e. product and short video\nrecommendation scenarios in Guess What You Like business domain in the homepage\nof Taobao App (a leading e-commerce platform), to serve the major online\ntraffic. Codes will be released after this paper is published.\n","authors":["Jiahao Yu","Haozhuang Liu","Yeqiu Yang","Lu Chen","Wu Jian","Yuning Jiang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.13881v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.01776v5","updated":"2025-05-20T02:32:33Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v5.pdf","comment":"Accepted by ICML2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.14868v1","updated":"2025-05-20T19:59:41Z","published":"2025-05-20T19:59:41Z","title":"VisTopics: A Visual Semantic Unsupervised Approach to Topic Modeling of\n  Video and Image Data","summary":"  Understanding visual narratives is crucial for examining the evolving\ndynamics of media representation. This study introduces VisTopics, a\ncomputational framework designed to analyze large-scale visual datasets through\nan end-to-end pipeline encompassing frame extraction, deduplication, and\nsemantic clustering. Applying VisTopics to a dataset of 452 NBC News videos\nresulted in reducing 11,070 frames to 6,928 deduplicated frames, which were\nthen semantically analyzed to uncover 35 topics ranging from political events\nto environmental crises. By integrating Latent Dirichlet Allocation with\ncaption-based semantic analysis, VisTopics demonstrates its potential to\nunravel patterns in visual framing across diverse contexts. This approach\nenables longitudinal studies and cross-platform comparisons, shedding light on\nthe intersection of media, technology, and public discourse. The study\nvalidates the method's reliability through human coding accuracy metrics and\nemphasizes its scalability for communication research. By bridging the gap\nbetween visual representation and semantic meaning, VisTopics provides a\ntransformative tool for advancing the methodological toolkit in computational\nmedia studies. Future research may leverage VisTopics for comparative analyses\nacross media outlets or geographic regions, offering insights into the shifting\nlandscapes of media narratives and their societal implications.\n","authors":["Ayse D Lokmanoglu","Dror Walter"],"pdf_url":"https://arxiv.org/pdf/2505.14868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14562v1","updated":"2025-05-20T16:21:27Z","published":"2025-05-20T16:21:27Z","title":"Representation Learning for Semantic Alignment of Language, Audio, and\n  Visual Modalities","summary":"  This paper proposes a single-stage training approach that semantically aligns\nthree modalities - audio, visual, and text using a contrastive learning\nframework. Contrastive training has gained prominence for multimodal alignment,\nutilizing large-scale unlabeled data to learn shared representations. Existing\ndeep learning approach for trimodal alignment involves two-stages, that\nseparately align visual-text and audio-text modalities. This approach suffers\nfrom mismatched data distributions, resulting in suboptimal alignment.\nLeveraging the AVCaps dataset, which provides audio, visual and audio-visual\ncaptions for video clips, our method jointly optimizes the representation of\nall the modalities using contrastive training. Our results demonstrate that the\nsingle-stage approach outperforms the two-stage method, achieving a two-fold\nimprovement in audio based visual retrieval, highlighting the advantages of\nunified multimodal representation learning.\n","authors":["Parthasaarathy Sudarsanam","Irene Martn-Morat","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2505.14562v1.pdf","comment":"Accepted to European Signal Processing Conference (EUSIPCO 2025)"},{"id":"http://arxiv.org/abs/2505.14329v1","updated":"2025-05-20T13:15:58Z","published":"2025-05-20T13:15:58Z","title":"TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust\n  Multimodal Sentiment Analysis","summary":"  Multimodal Sentiment Analysis (MSA) with missing modalities has attracted\nincreasing attention recently. While current Transformer-based methods leverage\ndense text information to maintain model robustness, their quadratic complexity\nhinders efficient long-range modeling and multimodal fusion. To this end, we\npropose a novel and efficient Text-enhanced Fusion Mamba (TF-Mamba) framework\nfor robust MSA with missing modalities. Specifically, a Text-aware Modality\nEnhancement (TME) module aligns and enriches non-text modalities, while\nreconstructing the missing text semantics. Moreover, we develop Text-based\nContext Mamba (TC-Mamba) to capture intra-modal contextual dependencies under\ntext collaboration. Finally, Text-guided Query Mamba (TQ-Mamba) queries\ntext-guided multimodal information and learns joint representations for\nsentiment prediction. Extensive experiments on three MSA datasets demonstrate\nthe effectiveness and efficiency of the proposed method under missing modality\nscenarios. Our code is available at https://github.com/codemous/TF-Mamba.\n","authors":["Xiang Li","Xianfu Cheng","Dezhuang Miao","Xiaoming Zhang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2505.14329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14319v1","updated":"2025-05-20T13:06:19Z","published":"2025-05-20T13:06:19Z","title":"RETRO: REthinking Tactile Representation Learning with Material PriOrs","summary":"  Tactile perception is profoundly influenced by the surface properties of\nobjects in contact. However, despite their crucial role in shaping tactile\nexperiences, these material characteristics have been largely neglected in\nexisting tactile representation learning methods. Most approaches primarily\nfocus on aligning tactile data with visual or textual information, overlooking\nthe richness of tactile feedback that comes from understanding the materials'\ninherent properties. In this work, we address this gap by revisiting the\ntactile representation learning framework and incorporating material-aware\npriors into the learning process. These priors, which represent pre-learned\ncharacteristics specific to different materials, allow tactile models to better\ncapture and generalize the nuances of surface texture. Our method enables more\naccurate, contextually rich tactile feedback across diverse materials and\ntextures, improving performance in real-world applications such as robotics,\nhaptic feedback systems, and material editing.\n","authors":["Weihao Xia","Chenliang Zhou","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2505.14319v1.pdf","comment":"Code: https://github.com/weihaox/RETRO"},{"id":"http://arxiv.org/abs/2502.05863v2","updated":"2025-05-20T12:37:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14272v1","updated":"2025-05-20T12:25:33Z","published":"2025-05-20T12:25:33Z","title":"Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor\n  Retrieval with Limited Labeled Data","summary":"  Considering the importance of detecting hateful language, labeled hate speech\ndata is expensive and time-consuming to collect, particularly for low-resource\nlanguages. Prior work has demonstrated the effectiveness of cross-lingual\ntransfer learning and data augmentation in improving performance on tasks with\nlimited labeled data. To develop an efficient and scalable cross-lingual\ntransfer learning approach, we leverage nearest-neighbor retrieval to augment\nminimal labeled data in the target language, thereby enhancing detection\nperformance. Specifically, we assume access to a small set of labeled training\ninstances in the target language and use these to retrieve the most relevant\nlabeled examples from a large multilingual hate speech detection pool. We\nevaluate our approach on eight languages and demonstrate that it consistently\noutperforms models trained solely on the target language data. Furthermore, in\nmost cases, our method surpasses the current state-of-the-art. Notably, our\napproach is highly data-efficient, retrieving as small as 200 instances in some\ncases while maintaining superior performance. Moreover, it is scalable, as the\nretrieval pool can be easily expanded, and the method can be readily adapted to\nnew languages and tasks. We also apply maximum marginal relevance to mitigate\nredundancy and filter out highly similar retrieved instances, resulting in\nimprovements in some languages.\n","authors":["Faeze Ghorbanpour","Daryna Dementieva","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2505.14272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06149v2","updated":"2025-05-20T12:14:09Z","published":"2025-05-09T16:00:01Z","title":"Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study","summary":"  Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance.\n","authors":["Faeze Ghorbanpour","Daryna Dementieva","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2505.06149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12623v2","updated":"2025-05-20T08:59:05Z","published":"2025-02-18T08:09:42Z","title":"DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning","summary":"  Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring DeepResonance for multi-way instruction tuning. Our model achieves\nstate-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets.\n","authors":["Zhuoyuan Mao","Mengjie Zhao","Qiyu Wu","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2502.12623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14035v1","updated":"2025-05-20T07:31:17Z","published":"2025-05-20T07:31:17Z","title":"ShieldVLM: Safeguarding the Multimodal Implicit Toxicity via\n  Deliberative Reasoning with LVLMs","summary":"  Toxicity detection in multimodal text-image content faces growing challenges,\nespecially with multimodal implicit toxicity, where each modality appears\nbenign on its own but conveys hazard when combined. Multimodal implicit\ntoxicity appears not only as formal statements in social platforms but also\nprompts that can lead to toxic dialogs from Large Vision-Language Models\n(LVLMs). Despite the success in unimodal text or image moderation, toxicity\ndetection for multimodal content, particularly the multimodal implicit\ntoxicity, remains underexplored. To fill this gap, we comprehensively build a\ntaxonomy for multimodal implicit toxicity (MMIT) and introduce an MMIT-dataset,\ncomprising 2,100 multimodal statements and prompts across 7 risk categories (31\nsub-categories) and 5 typical cross-modal correlation modes. To advance the\ndetection of multimodal implicit toxicity, we build ShieldVLM, a model which\nidentifies implicit toxicity in multimodal statements, prompts and dialogs via\ndeliberative cross-modal reasoning. Experiments show that ShieldVLM outperforms\nexisting strong baselines in detecting both implicit and explicit toxicity. The\nmodel and dataset will be publicly available to support future researches.\nWarning: This paper contains potentially sensitive contents.\n","authors":["Shiyao Cui","Qinglin Zhang","Xuan Ouyang","Renmiao Chen","Zhexin Zhang","Yida Lu","Hongning Wang","Han Qiu","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2505.14035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12499v2","updated":"2025-05-20T07:25:42Z","published":"2025-05-18T17:18:06Z","title":"Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video\n  Retrieval","summary":"  Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.\n","authors":["Jian Xiao","Zijie Song","Jialong Hu","Hao Cheng","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.12499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06442v2","updated":"2025-05-20T06:14:42Z","published":"2025-03-09T04:47:19Z","title":"OT-DETECTOR: Delving into Optimal Transport for Zero-shot\n  Out-of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications. While\nzero-shot OOD detection, which requires no training on in-distribution (ID)\ndata, has become feasible with the emergence of vision-language models like\nCLIP, existing methods primarily focus on semantic matching and fail to fully\ncapture distributional discrepancies. To address these limitations, we propose\nOT-DETECTOR, a novel framework that employs Optimal Transport (OT) to quantify\nboth semantic and distributional discrepancies between test samples and ID\nlabels. Specifically, we introduce cross-modal transport mass and transport\ncost as semantic-wise and distribution-wise OOD scores, respectively, enabling\nmore robust detection of OOD samples. Additionally, we present a semantic-aware\ncontent refinement (SaCR) module, which utilizes semantic cues from ID labels\nto amplify the distributional discrepancy between ID and hard OOD samples.\nExtensive experiments on several benchmarks demonstrate that OT-DETECTOR\nachieves state-of-the-art performance across various OOD detection tasks,\nparticularly in challenging hard-OOD scenarios.\n","authors":["Yu Liu","Hao Tang","Haiqi Zhang","Jing Qin","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2503.06442v2.pdf","comment":"Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.13948v1","updated":"2025-05-20T05:27:57Z","published":"2025-05-20T05:27:57Z","title":"Memory-Centric Embodied Question Answer","summary":"  Embodied Question Answering (EQA) requires agents to autonomously explore and\nunderstand the environment to answer context-dependent questions. Existing\nframeworks typically center around the planner, which guides the stopping\nmodule, memory module, and answering module for reasoning. In this paper, we\npropose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric\nEQA models where the memory module cannot fully interact with other modules,\nMemoryEQA flexible feeds memory information into all modules, thereby enhancing\nefficiency and accuracy in handling complex tasks, such as those involving\nmultiple targets across different regions. Specifically, we establish a\nmulti-modal hierarchical memory mechanism, which is divided into global memory\nthat stores language-enhanced scene maps, and local memory that retains\nhistorical observations and state information. When performing EQA tasks, the\nmulti-modal large language model is leveraged to convert memory information\ninto the required input formats for injection into different modules. To\nevaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset\nbased on HM3D, comprising 1,587 question-answer pairs involving multiple\ntargets across various regions, which requires agents to maintain memory of\nexploration-acquired target information. Experimental results on HM-EQA,\nMT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a\n19.8% performance gain on MT-HM3D compared to baseline model further\nunderscores memory capability's pivotal role in resolving complex tasks.\n","authors":["Mingliang Zhai","Zhi Gao","Yuwei Wu","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2505.13948v1.pdf","comment":"14pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.11275v3","updated":"2025-05-20T02:58:01Z","published":"2025-05-16T14:10:41Z","title":"TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding\n  Capabilities of MLLMs","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the ability of artificial intelligence systems to\nunderstand and generate multimodal content. However, these models often exhibit\nlimited effectiveness when applied to non-Western cultural contexts, which\nraises concerns about their wider applicability. To address this limitation, we\npropose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a\nbilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark\nspecifically designed for assessing the understanding of traditional Chinese\nculture by MLLMs. TCC-Bench comprises culturally rich and visually diverse\ndata, incorporating images from museum artifacts, everyday life scenes, comics,\nand other culturally significant contexts. We adopt a semi-automated pipeline\nthat utilizes GPT-4o in text-only mode to generate candidate questions,\nfollowed by human curation to ensure data quality and avoid potential data\nleakage. The benchmark also avoids language bias by preventing direct\ndisclosure of cultural concepts within question texts. Experimental evaluations\nacross a wide range of MLLMs demonstrate that current models still face\nsignificant challenges when reasoning about culturally grounded visual content.\nThe results highlight the need for further research in developing culturally\ninclusive and context-aware multimodal systems. The code and data can be found\nat: https://tcc-bench.github.io/.\n","authors":["Pengju Xu","Yan Wang","Shuyuan Zhang","Xuan Zhou","Xin Li","Yue Yuan","Fengzhao Li","Shunyuan Zhou","Xingyu Wang","Yi Zhang","Haiying Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.11275v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.08175v3","updated":"2025-05-20T02:54:49Z","published":"2025-05-13T02:25:47Z","title":"Fast Text-to-Audio Generation with Adversarial Post-Training","summary":"  Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge.\n","authors":["Zachary Novack","Zach Evans","Zack Zukowski","Josiah Taylor","CJ Carr","Julian Parker","Adnan Al-Sinan","Gian Marco Iodice","Julian McAuley","Taylor Berg-Kirkpatrick","Jordi Pons"],"pdf_url":"https://arxiv.org/pdf/2505.08175v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14728v1","updated":"2025-05-20T01:11:17Z","published":"2025-05-20T01:11:17Z","title":"MORALISE: A Structured Benchmark for Moral Alignment in Visual Language\n  Models","summary":"  Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.\n","authors":["Xiao Lin","Zhining Liu","Ze Yang","Gaotang Li","Ruizhong Qiu","Shuke Wang","Hui Liu","Haotian Li","Sumit Keswani","Vishwa Pardeshi","Huijun Zhao","Wei Fan","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2505.14728v1.pdf","comment":"21 pages, 11 figures, 7 tables"}]},"2025-05-19T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.13757v1","updated":"2025-05-19T22:10:27Z","published":"2025-05-19T22:10:27Z","title":"LLM-Based Compact Reranking with Document Features for Scientific\n  Retrieval","summary":"  Scientific retrieval is essential for advancing academic discovery. Within\nthis process, document reranking plays a critical role by refining first-stage\nretrieval results. However, large language model (LLM) listwise reranking faces\nunique challenges in the scientific domain. First-stage retrieval is often\nsuboptimal in the scientific domain, so relevant documents are ranked lower.\nMoreover, conventional listwise reranking uses the full text of candidate\ndocuments in the context window, limiting the number of candidates that can be\nconsidered. As a result, many relevant documents are excluded before reranking,\nwhich constrains overall retrieval performance. To address these challenges, we\nexplore compact document representations based on semantic features such as\ncategories, sections, and keywords, and propose a training-free, model-agnostic\nreranking framework for scientific retrieval called CoRank. The framework\ninvolves three stages: (i) offline extraction of document-level features, (ii)\ncoarse reranking using these compact representations, and (iii) fine-grained\nreranking on full texts of the top candidates from stage (ii). This hybrid\ndesign provides a high-level abstraction of document semantics, expands\ncandidate coverage, and retains critical details required for precise ranking.\nExperiments on LitSearch and CSFCube show that CoRank significantly improves\nreranking performance across different LLM backbones, increasing nDCG@10 from\n32.0 to 39.7. Overall, these results highlight the value of information\nextraction for reranking in scientific retrieval.\n","authors":["Runchu Tian","Xueqiang Xu","Bowen Jin","SeongKu Kang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2505.13757v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.09795v2","updated":"2025-05-19T17:37:21Z","published":"2025-05-14T20:45:29Z","title":"Beyond Pairwise Learning-To-Rank At Airbnb","summary":"  There are three fundamental asks from a ranking algorithm: it should scale to\nhandle a large number of items, sort items accurately by their utility, and\nimpose a total order on the items for logical consistency. But here's the\ncatch-no algorithm can achieve all three at the same time. We call this\nlimitation the SAT theorem for ranking algorithms. Given the dilemma, how can\nwe design a practical system that meets user needs? Our current work at Airbnb\nprovides an answer, with a working solution deployed at scale. We start with\npairwise learning-to-rank (LTR) models-the bedrock of search ranking tech\nstacks today. They scale linearly with the number of items ranked and perform\nstrongly on metrics like NDCG by learning from pairwise comparisons. They are\nat a sweet spot of performance vs. cost, making them an ideal choice for\nseveral industrial applications. However, they have a drawback-by ignoring\ninteractions between items, they compromise on accuracy. To improve accuracy,\nwe create a \"true\" pairwise LTR model-one that captures interactions between\nitems during pairwise comparisons. But accuracy comes at the expense of\nscalability and total order, and we discuss strategies to counter these\nchallenges. For greater accuracy, we take each item in the search result, and\ncompare it against the rest of the items along two dimensions: (1) Superiority:\nHow strongly do searchers prefer the given item over the remaining ones? (2)\nSimilarity: How similar is the given item to all the other items? This forms\nthe basis of our \"all-pairwise\" LTR framework, which factors in interactions\nacross all items at once. Looking at items on the search result page all\ntogether-superiority and similarity combined-gives us a deeper understanding of\nwhat searchers truly want. We quantify the resulting improvements in searcher\nexperience through offline and online experiments at Airbnb.\n","authors":["Malay Haldar","Daochen Zha","Huiji Gao","Liwei He","Sanjeev Katariya"],"pdf_url":"https://arxiv.org/pdf/2505.09795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13306v1","updated":"2025-05-19T16:25:55Z","published":"2025-05-19T16:25:55Z","title":"GMM-Based Comprehensive Feature Extraction and Relative Distance\n  Preservation For Few-Shot Cross-Modal Retrieval","summary":"  Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods.\n","authors":["Chengsong Sun","Weiping Li","Xiang Li","Yuankun Liu","Lianlei Shan"],"pdf_url":"https://arxiv.org/pdf/2505.13306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13581v1","updated":"2025-05-19T15:41:39Z","published":"2025-05-19T15:41:39Z","title":"RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection","summary":"  Content moderation for large language models (LLMs) remains a significant\nchallenge, requiring flexible and adaptable solutions that can quickly respond\nto emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),\na novel approach that leverages a retrieval-augmented generation (RAG)\narchitecture to dynamically reject unsafe user queries without model\nretraining. By strategically inserting and marking malicious documents into the\nvector database, the system can identify and reject harmful requests when these\ndocuments are retrieved. Our preliminary results show that RAR achieves\ncomparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,\nwhile offering superior flexibility and real-time customization capabilities, a\nfundamental feature to timely address critical vulnerabilities. This approach\nintroduces no architectural changes to existing RAG systems, requiring only the\naddition of specially crafted documents and a simple rejection mechanism based\non retrieval results.\n","authors":["Tommaso Mario Buonocore","Enea Parimbelli"],"pdf_url":"https://arxiv.org/pdf/2505.13581v1.pdf","comment":"7 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.18383v2","updated":"2025-05-19T14:37:44Z","published":"2024-07-25T20:36:20Z","title":"Supporting Evidence-Based Medicine by Finding Both Relevant and\n  Significant Works","summary":"  In this paper, we present a new approach to improving the relevance and\nreliability of medical IR, which builds upon the concept of Level of Evidence\n(LoE). LoE framework categorizes medical publications into 7 distinct levels\nbased on the underlying empirical evidence. Despite LoE framework's relevance\nin medical research and evidence-based practice, only few medical publications\nexplicitly state their LoE. Therefore, we develop a classification model for\nautomatically assigning LoE to medical publications, which successfully\nclassifies over 26 million documents in MEDLINE database into LoE classes. The\nsubsequent retrieval experiments on TREC PM datasets show substantial\nimprovements in retrieval relevance, when LoE is used as a search filter.\n","authors":["Sameh Frihat","Norbert Fuhr"],"pdf_url":"https://arxiv.org/pdf/2407.18383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13129v1","updated":"2025-05-19T14:00:10Z","published":"2025-05-19T14:00:10Z","title":"Optimizing Retrieval Augmented Generation for Object Constraint Language","summary":"  The Object Constraint Language (OCL) is essential for defining precise\nconstraints within Model-Based Systems Engineering (MBSE). However, manually\nwriting OCL rules is complex and time-consuming. This study explores the\noptimization of Retrieval-Augmented Generation (RAG) for automating OCL rule\ngeneration, focusing on the impact of different retrieval strategies. We\nevaluate three retrieval approaches $\\unicode{x2013}$ BM25 (lexical-based),\nBERT-based (semantic retrieval), and SPLADE (sparse-vector retrieval)\n$\\unicode{x2013}$ analyzing their effectiveness in providing relevant context\nfor a large language model.\n  To further assess our approach, we compare and benchmark our\nretrieval-optimized generation results against PathOCL, a state-of-the-art\ngraph-based method. We directly compare BM25, BERT, and SPLADE retrieval\nmethods with PathOCL to understand how different retrieval methods perform for\na unified evaluation framework. Our experimental results, focusing on\nretrieval-augmented generation, indicate that while retrieval can enhance\ngeneration accuracy, its effectiveness depends on the retrieval method and the\nnumber of retrieved chunks (k). BM25 underperforms the baseline, whereas\nsemantic approaches (BERT and SPLADE) achieve better results, with SPLADE\nperforming best at lower k values. However, excessive retrieval with high k\nparameter can lead to retrieving irrelevant chunks which degrades model\nperformance. Our findings highlight the importance of optimizing retrieval\nconfigurations to balance context relevance and output consistency. This\nresearch provides insights into improving OCL rule generation using RAG and\nunderscores the need for tailoring retrieval.\n","authors":["Kevin Chenhao Li","Vahid Zolfaghari","Nenad Petrovic","Fengjunjie Pan","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2505.13129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13572v1","updated":"2025-05-19T13:26:51Z","published":"2025-05-19T13:26:51Z","title":"Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for\n  Question-Answering Over Knowledge Graphs","summary":"  The SPARQL query language is the standard method to access knowledge graphs\n(KGs). However, formulating SPARQL queries is a significant challenge for\nnon-expert users, and remains time-consuming for the experienced ones. Best\npractices recommend to document KGs with competency questions and example\nqueries to contextualise the knowledge they contain and illustrate their\npotential applications. In practice, however, this is either not the case or\nthe examples are provided in limited numbers. Large Language Models (LLMs) are\nbeing used in conversational agents and are proving to be an attractive\nsolution with a wide range of applications, from simple question-answering\nabout common knowledge to generating code in a targeted programming language.\nHowever, training and testing these models to produce high quality SPARQL\nqueries from natural language questions requires substantial datasets of\nquestion-query pairs. In this paper, we present Q${}^2$Forge that addresses the\nchallenge of generating new competency questions for a KG and corresponding\nSPARQL queries. It iteratively validates those queries with human feedback and\nLLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,\nmeaning that the different modules of the application (CQ generation, query\ngeneration and query refinement) can be used separately, as an integrated\npipeline, or replaced by alternative services. The result is a complete\npipeline from competency question formulation to query evaluation, supporting\nthe creation of reference query sets for any target KG.\n","authors":["Yousouf Taghzouti","Franck Michel","Tao Jiang","Louis-Flix Nothias","Fabien Gandon"],"pdf_url":"https://arxiv.org/pdf/2505.13572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08071v2","updated":"2025-05-19T12:37:24Z","published":"2025-02-12T02:24:26Z","title":"Collaborative Filtering Meets Spectrum Shift: Connecting User-Item\n  Interaction with Graph-Structured Side Information","summary":"  Graph Neural Networks (GNNs) have demonstrated their superiority in\ncollaborative filtering, where the user-item (U-I) interaction bipartite graph\nserves as the fundamental data format. However, when graph-structured side\ninformation (e.g., multimodal similarity graphs or social networks) is\nintegrated into the U-I bipartite graph, existing graph collaborative filtering\nmethods fall short of achieving satisfactory performance. We quantitatively\nanalyze this problem from a spectral perspective. Recall that a bipartite graph\npossesses a full spectrum within the range of [-1, 1], with the highest\nfrequency exactly achievable at -1 and the lowest frequency at 1; however, we\nobserve as more side information is incorporated, the highest frequency of the\naugmented adjacency matrix progressively shifts rightward. This spectrum shift\nphenomenon has caused previous approaches built for the full spectrum [-1, 1]\nto assign mismatched importance to different frequencies. To this end, we\npropose Spectrum Shift Correction (dubbed SSC), incorporating shifting and\nscaling factors to enable spectral GNNs to adapt to the shifted spectrum.\nUnlike previous paradigms of leveraging side information, which necessitate\ntailored designs for diverse data types, SSC directly connects traditional\ngraph collaborative filtering with any graph-structured side information.\nExperiments on social and multimodal recommendation demonstrate the\neffectiveness of SSC, achieving relative improvements of up to 23% without\nincurring any additional computational overhead. Our code is available at\nhttps://github.com/yhhe2004/SSC-KDD.\n","authors":["Yunhang He","Cong Xu","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08071v2.pdf","comment":"Accepted at KDD 2025"},{"id":"http://arxiv.org/abs/2504.20734v2","updated":"2025-05-19T11:09:12Z","published":"2025-04-29T13:18:58Z","title":"UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse\n  Modalities and Granularities","summary":"  Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single aggregated corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over various modality-specific and unified\nbaselines.\n","authors":["Woongyeong Yeo","Kangsan Kim","Soyeong Jeong","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2504.20734v2.pdf","comment":"Project page : https://universalrag.github.io"},{"id":"http://arxiv.org/abs/2502.11571v2","updated":"2025-05-19T10:58:50Z","published":"2025-02-17T09:05:21Z","title":"FaMTEB: Massive Text Embedding Benchmark in Persian Language","summary":"  In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.\n","authors":["Erfan Zinvandi","Morteza Alikhani","Mehran Sarmadi","Zahra Pourbahman","Sepehr Arvin","Reza Kazemi","Arash Amini"],"pdf_url":"https://arxiv.org/pdf/2502.11571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12925v1","updated":"2025-05-19T10:07:51Z","published":"2025-05-19T10:07:51Z","title":"CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive\n  Programming","summary":"  Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet\n","authors":["Han Deng","Yuan Meng","Shixiang Tang","Wanli Ouyang","Xinzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2505.12925v1.pdf","comment":"main 9 pages"},{"id":"http://arxiv.org/abs/2502.03041v2","updated":"2025-05-19T09:48:53Z","published":"2025-02-05T09:56:52Z","title":"Large Language Model as Universal Retriever in Industrial-Scale\n  Recommender System","summary":"  In real-world recommender systems, different retrieval objectives are\ntypically addressed using task-specific datasets with carefully designed model\narchitectures. We demonstrate that Large Language Models (LLMs) can function as\nuniversal retrievers, capable of handling multiple objectives within a\ngenerative retrieval framework. To model complex user-item relationships within\ngenerative retrieval, we propose multi-query representation. To address the\nchallenge of extremely large candidate sets in industrial recommender systems,\nwe introduce matrix decomposition to boost model learnability,\ndiscriminability, and transferability, and we incorporate probabilistic\nsampling to reduce computation costs. Finally, our Universal Retrieval Model\n(URM) can adaptively generate a set from tens of millions of candidates based\non arbitrary given objective while keeping the latency within tens of\nmilliseconds. Applied to industrial-scale data, URM outperforms expert models\nelaborately designed for different retrieval objectives on offline experiments\nand significantly improves the core metric of online advertising platform by\n$3\\%$.\n","authors":["Junguang Jiang","Yanwen Huang","Bin Liu","Xiaoyu Kong","Xinhang Li","Ziru Xu","Han Zhu","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.03041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13557v1","updated":"2025-05-19T08:59:08Z","published":"2025-05-19T08:59:08Z","title":"AMAQA: A Metadata-based QA Dataset for RAG Systems","summary":"  Retrieval-augmented generation (RAG) systems are widely used in\nquestion-answering (QA) tasks, but current benchmarks lack metadata\nintegration, hindering evaluation in scenarios requiring both textual data and\nexternal information. To address this, we present AMAQA, a new open-access QA\ndataset designed to evaluate tasks combining text and metadata. The integration\nof metadata is especially important in fields that require rapid analysis of\nlarge volumes of data, such as cybersecurity and intelligence, where timely\naccess to relevant information is critical. AMAQA includes about 1.1 million\nEnglish messages collected from 26 public Telegram groups, enriched with\nmetadata such as timestamps, topics, emotional tones, and toxicity indicators,\nwhich enable precise and contextualized queries by filtering documents based on\nspecific criteria. It also includes 450 high-quality QA pairs, making it a\nvaluable resource for advancing research on metadata-driven QA and RAG systems.\nTo the best of our knowledge, AMAQA is the first single-hop QA benchmark to\nincorporate metadata and labels such as topics covered in the messages. We\nconduct extensive tests on the benchmark, establishing a new standard for\nfuture research. We show that leveraging metadata boosts accuracy from 0.12 to\n0.61, highlighting the value of structured context. Building on this, we\nexplore several strategies to refine the LLM input by iterating over provided\ncontext and enriching it with noisy documents, achieving a further 3-point gain\nover the best baseline and a 14-point improvement over simple metadata\nfiltering. The dataset is available at\nhttps://anonymous.4open.science/r/AMAQA-5D0D/\n","authors":["Davide Bruni","Marco Avvenuti","Nicola Tonellotto","Maurizio Tesconi"],"pdf_url":"https://arxiv.org/pdf/2505.13557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12989v2","updated":"2025-05-19T07:26:25Z","published":"2024-11-20T02:34:21Z","title":"Data Watermarking for Sequential Recommender Systems","summary":"  In the era of large foundation models, data has become a crucial component in\nbuilding high-performance AI systems. As the demand for high-quality and\nlarge-scale data continues to rise, data copyright protection is attracting\nincreasing attention. In this work, we explore the problem of data watermarking\nfor sequential recommender systems, where a watermark is embedded into the\ntarget dataset and can be detected in models trained on that dataset. We focus\non two settings: dataset watermarking, which protects the ownership of the\nentire dataset, and user watermarking, which safeguards the data of individual\nusers. We present a method named Dataset Watermarking for Recommender Systems\n(DWRS) to address them. We define the watermark as a sequence of consecutive\nitems inserted into normal users' interaction sequences. We define a Receptive\nField (RF) to guide the inserting process to facilitate the memorization of the\nwatermark. Extensive experiments on five representative sequential\nrecommendation models and three benchmark datasets demonstrate the\neffectiveness of DWRS in protecting data copyright while preserving model\nutility.\n","authors":["Sixiao Zhang","Cheng Long","Wei Yuan","Hongxu Chen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12791v1","updated":"2025-05-19T07:23:46Z","published":"2025-05-19T07:23:46Z","title":"Unlearning for Federated Online Learning to Rank: A Reproducibility\n  Study","summary":"  This paper reports on findings from a comparative study on the effectiveness\nand efficiency of federated unlearning strategies within Federated Online\nLearning to Rank (FOLTR), with specific attention to systematically analysing\nthe unlearning capabilities of methods in a verifiable manner.\n  Federated approaches to ranking of search results have recently garnered\nattention to address users privacy concerns. In FOLTR, privacy is safeguarded\nby collaboratively training ranking models across decentralized data sources,\npreserving individual user data while optimizing search results based on\nimplicit feedback, such as clicks.\n  Recent legislation introduced across numerous countries is establishing the\nso called \"the right to be forgotten\", according to which services based on\nmachine learning models like those in FOLTR should provide capabilities that\nallow users to remove their own data from those used to train models. This has\nsparked the development of unlearning methods, along with evaluation practices\nto measure whether unlearning of a user data successfully occurred. Current\nevaluation practices are however often controversial, necessitating the use of\nmultiple metrics for a more comprehensive assessment -- but previous proposals\nof unlearning methods only used single evaluation metrics.\n  This paper addresses this limitation: our study rigorously assesses the\neffectiveness of unlearning strategies in managing both under-unlearning and\nover-unlearning scenarios using adapted, and newly proposed evaluation metrics.\nThanks to our detailed analysis, we uncover the strengths and limitations of\nfive unlearning strategies, offering valuable insights into optimizing\nfederated unlearning to balance data privacy and system performance within\nFOLTR. We publicly release our code and complete results at\nhttps://github.com/Iris1026/Unlearning-for-FOLTR.git.\n","authors":["Yiling Tao","Shuyi Wang","Jiaxi Yang","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2505.12791v1.pdf","comment":"Accepted at SIGIR2025"},{"id":"http://arxiv.org/abs/2505.12782v1","updated":"2025-05-19T07:11:07Z","published":"2025-05-19T07:11:07Z","title":"AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large\n  Multimodal-Models Reasoning","summary":"  Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning.\n","authors":["Kai Zhang","Xingyu Chen","Xiaofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09436v2","updated":"2025-05-19T06:27:01Z","published":"2025-05-14T14:44:30Z","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios","summary":"  Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.\n","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"pdf_url":"https://arxiv.org/pdf/2505.09436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13550v1","updated":"2025-05-19T04:49:47Z","published":"2025-05-19T04:49:47Z","title":"JIR-Arena: The First Benchmark Dataset for Just-in-time Information\n  Recommendation","summary":"  Just-in-time Information Recommendation (JIR) is a service designed to\ndeliver the most relevant information precisely when users need it, ,\naddressing their knowledge gaps with minimal effort and boosting\ndecision-making and efficiency in daily life. Advances in device-efficient\ndeployment of foundation models and the growing use of intelligent wearable\ndevices have made always-on JIR assistants feasible. However, there has been no\nsystematic effort to formally define JIR tasks or establish evaluation\nframeworks. To bridge this gap, we present the first mathematical definition of\nJIR tasks and associated evaluation metrics. Additionally, we introduce\nJIR-Arena, a multimodal benchmark dataset featuring diverse,\ninformation-request-intensive scenarios to evaluate JIR systems across critical\ndimensions: i) accurately inferring user information needs, ii) delivering\ntimely and relevant recommendations, and iii) avoiding irrelevant content that\nmay distract users.\n  Developing a JIR benchmark dataset poses challenges due to subjectivity in\nestimating user information needs and uncontrollable system variables affecting\nreproducibility. To address these, JIR-Arena: i) combines input from multiple\nhumans and large AI models to approximate information need distributions; ii)\nassesses JIR quality through information retrieval outcomes using static\nknowledge base snapshots; and iii) employs a multi-turn, multi-entity\nvalidation framework to improve objectivity and generality. Furthermore, we\nimplement a baseline JIR system capable of processing real-time information\nstreams aligned with user inputs. Our evaluation of this baseline system on\nJIR-Arena indicates that while foundation model-based JIR systems simulate user\nneeds with reasonable precision, they face challenges in recall and effective\ncontent retrieval. To support future research in this new area, we fully\nrelease our code and data.\n","authors":["Ke Yang","Kevin Ros","Shankar Kumar Senthil Kumar","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2505.13550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12697v1","updated":"2025-05-19T04:37:53Z","published":"2025-05-19T04:37:53Z","title":"Towards A Generalist Code Embedding Model Based On Massive Data\n  Synthesis","summary":"  Code embedding models attract increasing attention due to the widespread\npopularity of retrieval-augmented generation (RAG) in software development.\nThese models are expected to capture the rich semantic relationships inherent\nto code, which differ significantly from those found in text. However, existing\nmodels remain severely limited due to the scarcity of high-quality training\ndata. In this work, we introduce \\textbf{CodeR} (\\underline{Code}\n\\underline{R}etrieval), a state-of-the-art embedding model for general-purpose\ncode retrieval. The superior performance of CodeR is built upon CodeR-Pile, a\nlarge-scale synthetic dataset constructed under the DRU (Diversity,\nReliability, Usability) principle via a novel data synthesis pipeline. To\noptimize training effectiveness, we propose Annealing, a curriculum learning\nstrategy that enables effective knowledge transfer across heterogeneous sources\nof data. We evaluate CodeR based on 16 diverse code retrieval tasks, where it\nsignificantly outperforms existing baselines and exhibits strong out-of-domain\ngeneralization performance. We have publicly released our code and the\nwell-trained model to facilitate further research in this critical area.\nhttps://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_Coder.\n","authors":["Chaofan Li","Jianlyu Chen","Yingxia Shao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2505.12697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12694v1","updated":"2025-05-19T04:33:09Z","published":"2025-05-19T04:33:09Z","title":"LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries","summary":"  Query expansion (QE) enhances retrieval by incorporating relevant terms, with\nlarge language models (LLMs) offering an effective alternative to traditional\nrule-based and statistical methods. However, LLM-based QE suffers from a\nfundamental limitation: it often fails to generate relevant knowledge,\ndegrading search performance. Prior studies have focused on hallucination, yet\nits underlying cause--LLM knowledge deficiencies--remains underexplored. This\npaper systematically examines two failure cases in LLM-based QE: (1) when the\nLLM lacks query knowledge, leading to incorrect expansions, and (2) when the\nquery is ambiguous, causing biased refinements that narrow search coverage. We\nconduct controlled experiments across multiple datasets, evaluating the effects\nof knowledge and query ambiguity on retrieval performance using sparse and\ndense retrieval models. Our results reveal that LLM-based QE can significantly\ndegrade the retrieval effectiveness when knowledge in the LLM is insufficient\nor query ambiguity is high. We introduce a framework for evaluating QE under\nthese conditions, providing insights into the limitations of LLM-based\nretrieval augmentation.\n","authors":["Kenya Abe","Kunihiro Takeoka","Makoto P. Kato","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2505.12694v1.pdf","comment":"Accepted at SIGIR 2025 short paper track"},{"id":"http://arxiv.org/abs/2501.15087v2","updated":"2025-05-19T03:34:25Z","published":"2025-01-25T05:30:58Z","title":"Multi-Grained Patch Training for Efficient LLM-based Recommendation","summary":"  Large Language Models (LLMs) have emerged as a new paradigm for\nrecommendation by converting interacted item history into language modeling.\nHowever, constrained by the limited context length of LLMs, existing approaches\nhave to truncate item history in the prompt, focusing only on recent\ninteractions and sacrificing the ability to model long-term history. To enable\nLLMs to model long histories, we pursue a concise embedding representation for\nitems and sessions. In the LLM embedding space, we construct an item's\nembedding by aggregating its textual token embeddings; similarly, we construct\na session's embedding by aggregating its item embeddings. While efficient, this\nway poses two challenges since it ignores the temporal significance of user\ninteractions and LLMs do not natively interpret our custom embeddings. To\novercome these, we propose PatchRec, a multi-grained patch training method\nconsisting of two stages: (1) Patch Pre-training, which familiarizes LLMs with\naggregated embeddings -- patches, and (2) Patch Fine-tuning, which enables LLMs\nto capture time-aware significance in interaction history. Extensive\nexperiments show that PatchRec effectively models longer behavior histories\nwith improved efficiency. This work facilitates the practical use of LLMs for\nmodeling long behavior histories. Codes are available at\nhttps://github.com/ljy0ustc/PatchRec.\n","authors":["Jiayi Liao","Ruobing Xie","Sihang Li","Xiang Wang","Xingwu Sun","Zhanhui Kang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2501.15087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13545v1","updated":"2025-05-19T03:17:41Z","published":"2025-05-19T03:17:41Z","title":"Know Or Not: a library for evaluating out-of-knowledge base robustness","summary":"  While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot.\n","authors":["Jessica Foo","Pradyumna Shyama Prasad","Shaun Khoo"],"pdf_url":"https://arxiv.org/pdf/2505.13545v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2505.13337v1","updated":"2025-05-19T16:45:30Z","published":"2025-05-19T16:45:30Z","title":"Neural-Enhanced Rate Adaptation and Computation Distribution for\n  Emerging mmWave Multi-User 3D Video Streaming Systems","summary":"  We investigate multitask edge-user communication-computation resource\nallocation for $360^\\circ$ video streaming in an edge-computing enabled\nmillimeter wave (mmWave) multi-user virtual reality system. To balance the\ncommunication-computation trade-offs that arise herein, we formulate a video\nquality maximization problem that integrates interdependent\nmultitask/multi-user action spaces and rebuffering time/quality variation\nconstraints. We formulate a deep reinforcement learning framework for\n\\underline{m}ulti-\\underline{t}ask \\underline{r}ate adaptation and\n\\underline{c}omputation distribution (MTRC) to solve the problem of interest.\nOur solution does not rely on a priori knowledge about the environment and uses\nonly prior video streaming statistics (e.g., throughput, decoding time, and\ntransmission delay), and content information, to adjust the assigned video\nbitrates and computation distribution, as it observes the induced streaming\nperformance online. Moreover, to capture the task interdependence in the\nenvironment, we leverage neural network cascades to extend our MTRC method to\ntwo novel variants denoted as R1C2 and C1R2. We train all three methods with\nreal-world mmWave network traces and $360^\\circ$ video datasets to evaluate\ntheir performance in terms of expected quality of experience (QoE), viewport\npeak signal-to-noise ratio (PSNR), rebuffering time, and quality variation. We\noutperform state-of-the-art rate adaptation algorithms, with C1R2 showing best\nresults and achieving $5.21-6.06$ dB PSNR gains, $2.18-2.70$x rebuffering time\nreduction, and $4.14-4.50$ dB quality variation reduction.\n","authors":["Babak Badnava","Jacob Chakareski","Morteza Hashemi"],"pdf_url":"https://arxiv.org/pdf/2505.13337v1.pdf","comment":"Accepted to be published in IEEE Transaction on Multimedia"},{"id":"http://arxiv.org/abs/2505.13331v1","updated":"2025-05-19T16:44:02Z","published":"2025-05-19T16:44:02Z","title":"Learning Driven Elastic Task Multi-Connectivity Immersive Computing\n  Systems","summary":"  In virtual reality (VR) environments, computational tasks exhibit an elastic\nnature, meaning they can dynamically adjust based on various user and system\nconstraints. This elasticity is essential for maintaining immersive\nexperiences; however, it also introduces challenges for communication and\ncomputing in VR systems. In this paper, we investigate elastic task offloading\nfor multi-user edge-computing-enabled VR systems with multi-connectivity,\naiming to maximize the computational energy-efficiency (computational\nthroughput per unit of energy consumed). To balance the induced communication,\ncomputation, energy consumption, and quality of experience trade-offs due to\nthe elasticity of VR tasks, we formulate a constrained stochastic computational\nenergy-efficiency optimization problem that integrates the\nmulti-connectivity/multi-user action space and the elastic nature of VR\ncomputational tasks. We formulate a centralized phasic policy gradient (CPPG)\nframework to solve the problem of interest online, using only prior elastic\ntask offloading statistics (energy consumption, response time, and transmission\ntime), and task information (i.e., task size and computational intensity),\nwhile observing the induced system performance (energy consumption and\nlatency). We further extend our approach to decentralized learning by\nformulating an independent phasic policy gradient (IPPG) method and a\ndecentralized shared multi-armed bandit (DSMAB) method. We train our methods\nwith real-world 4G, 5G, and WiGig network traces and 360 video datasets to\nevaluate their performance in terms of response time, energy efficiency,\nscalability, and delivered quality of experience. We also provide a\ncomprehensive analysis of task size and its effect on offloading policy and\nsystem performance. In particular, we show that CPPG reduces latency by 28% and\nenergy consumption by 78% compared to IPPG.\n","authors":["Babak Badnava","Jacob Chakareski","Morteza Hashemi"],"pdf_url":"https://arxiv.org/pdf/2505.13331v1.pdf","comment":"Under review by IEEE Transaction on Mobile Computing"},{"id":"http://arxiv.org/abs/2505.13032v1","updated":"2025-05-19T12:18:42Z","published":"2025-05-19T12:18:42Z","title":"MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio,\n  Music, and Their Mix","summary":"  We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area.\n","authors":["Ziyang Ma","Yinghao Ma","Yanqiao Zhu","Chen Yang","Yi-Wen Chao","Ruiyang Xu","Wenxi Chen","Yuanzhe Chen","Zhuo Chen","Jian Cong","Kai Li","Keliang Li","Siyou Li","Xinfeng Li","Xiquan Li","Zheng Lian","Yuzhe Liang","Minghao Liu","Zhikang Niu","Tianrui Wang","Yuping Wang","Yuxuan Wang","Yihao Wu","Guanrou Yang","Jianwei Yu","Ruibin Yuan","Zhisheng Zheng","Ziya Zhou","Haina Zhu","Wei Xue","Emmanouil Benetos","Kai Yu","Eng-Siong Chng","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2505.13032v1.pdf","comment":"Open-source at https://github.com/ddlBoJack/MMAR"},{"id":"http://arxiv.org/abs/2505.13023v1","updated":"2025-05-19T12:07:29Z","published":"2025-05-19T12:07:29Z","title":"Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based\n  Inpainters under Unknown Conditions","summary":"  As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.\n","authors":["Yimao Guo","Zuomin Qu","Wei Lu","Xiangyang Luo"],"pdf_url":"https://arxiv.org/pdf/2505.13023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01976v3","updated":"2025-05-19T11:31:04Z","published":"2024-07-02T06:29:05Z","title":"A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding","summary":"  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. LayTextLLM\nprojects each bounding box to a single embedding and interleaves it with text,\nefficiently avoiding long sequence issues while leveraging autoregressive\ntraits of LLMs. LayTextLLM not only streamlines the interaction of layout and\ntextual data but also shows enhanced performance in KIE and VQA. Comprehensive\nbenchmark evaluations reveal significant improvements of LayTextLLM, with a\n15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA\nOCR-based LLMs. All resources are available at\nhttps://github.com/LayTextLLM/LayTextLLM.\n","authors":["Jinghui Lu","Haiyang Yu","Yanjie Wang","Yongjie Ye","Jingqun Tang","Ziwei Yang","Binghong Wu","Qi Liu","Hao Feng","Han Wang","Hao Liu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2407.01976v3.pdf","comment":"Accept to ACL2025 Findings"},{"id":"http://arxiv.org/abs/2409.14319v3","updated":"2025-05-19T06:06:13Z","published":"2024-09-22T05:13:11Z","title":"Scene-Text Grounding for Text-Based Video Question Answering","summary":"  Existing efforts in text-based video question answering (TextVideoQA) are\ncriticized for their opaque decisionmaking and heavy reliance on scene-text\nrecognition. In this paper, we propose to study Grounded TextVideoQA by forcing\nmodels to answer questions and spatio-temporally localize the relevant\nscene-text regions, thus decoupling QA from scenetext recognition and promoting\nresearch towards interpretable QA. The task has three-fold significance. First,\nit encourages scene-text evidence versus other short-cuts for answer\npredictions. Second, it directly accepts scene-text regions as visual answers,\nthus circumventing the problem of ineffective answer evaluation by stringent\nstring matching. Third, it isolates the challenges inherited in VideoQA and\nscene-text recognition. This enables the diagnosis of the root causes for\nfailure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve\nGrounded TextVideoQA, we propose the T2S-QA model that highlights a\ndisentangled temporal-to-spatial contrastive learning strategy for\nweakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate\nevaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text\nbounding boxes within 2.2K temporal segments related to 2K questions and 729\nvideos. With ViTXT-GQA, we perform extensive experiments and demonstrate the\nsevere limitations of existing techniques in Grounded TextVideoQA. While T2S-QA\nachieves superior results, the large performance gap with human leaves ample\nspace for improvement. Our further analysis of oracle scene-text inputs posits\nthat the major challenge is scene-text recognition. To advance the research of\nGrounded TextVideoQA, our dataset and code are at\nhttps://github.com/zhousheng97/ViTXT-GQA.git\n","authors":["Sheng Zhou","Junbin Xiao","Xun Yang","Peipei Song","Dan Guo","Angela Yao","Meng Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14319v3.pdf","comment":"Accepted by IEEE TMM"},{"id":"http://arxiv.org/abs/2505.12728v1","updated":"2025-05-19T05:35:30Z","published":"2025-05-19T05:35:30Z","title":"FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for\n  Multimodal Tasks","summary":"  Large language and multimodal models (LLMs and LMMs) exhibit strong inference\ncapabilities but are often limited by slow decoding speeds. This challenge is\nespecially acute in LMMs, where visual inputs typically comprise more tokens\nwith lower information density than text -- an issue exacerbated by recent\ntrends toward finer-grained visual tokenizations to boost performance.\nSpeculative decoding has been effective in accelerating LLM inference by using\na smaller draft model to generate candidate tokens, which are then selectively\nverified by the target model, improving speed without sacrificing output\nquality. While this strategy has been extended to LMMs, existing methods\nlargely overlook the unique properties of visual inputs and depend solely on\ntext-based draft models. In this work, we propose \\textbf{FLASH} (Fast\nLatent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework\ndesigned specifically for LMMs, which leverages two key properties of\nmultimodal data to design the draft model. First, to address redundancy in\nvisual tokens, we propose a lightweight latent-aware token compression\nmechanism. Second, recognizing that visual objects often co-occur within a\nscene, we employ a semi-autoregressive decoding strategy to generate multiple\ntokens per forward pass. These innovations accelerate draft decoding while\nmaintaining high acceptance rates, resulting in faster overall inference.\nExperiments show that FLASH significantly outperforms prior speculative\ndecoding approaches in both unimodal and multimodal settings, achieving up to\n\\textbf{2.68$\\times$} speed-up on video captioning and \\textbf{2.55$\\times$} on\nvisual instruction tuning tasks compared to the original LMM.\n","authors":["Zihua Wang","Ruibo Li","Haozhe Du","Joey Tianyi Zhou","Yu Zhang","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2505.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12669v1","updated":"2025-05-19T03:36:06Z","published":"2025-05-19T03:36:06Z","title":"Text2midi-InferAlign: Improving Symbolic Music Generation with\n  Inference-Time Alignment","summary":"  We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics.\n","authors":["Abhinaba Roy","Geeta Puri","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2505.12669v1.pdf","comment":"7 pages, 1 figure, 5 tables"}]},"2025-05-18T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2505.12393v1","updated":"2025-05-18T12:43:10Z","published":"2025-05-18T12:43:10Z","title":"Protocol as Poetry: Case Study on Pak's Protocol Arts","summary":"  Protocol art emerges at the confluence of blockchain-based smart contracts\nand a century-long lineage of conceptual art, participatory art, and\nalgorithmic generative art practices. Yet existing definitions-most notably\nPrimavera De Filippi's \"protocolism\"-struggle to demarcate this nascent genre\nfrom other art forms in practice. Addressing this definition-to-practice gap,\nthis paper offers a focused case study of pioneering protocol artworks by Pak,\nan early and influential pseudonymous protocol artist who treats smart\ncontracts as medium and protocol participation as message. Tracing the\nevolution from early open-edition releases of The Fungible and the dynamic\nmechanics of Merge to the soul-bound messaging of Censored and the reflective\nabsence of Not Found, we examine how Pak choreographs distributed agency across\ncollectors and autonomous contracts, showing how programmable protocols become\na social fabric in artistic meaning-making. Through thematic analysis of Pak's\nworks, we identify seven core characteristics that distinguish protocol art:\n(1) system-centric rather than object-centric composition, (2) autonomous\ngovernance for open-ended control, (3) distributed agency and communal\nauthorship, (4) temporal dynamism and lifecycle aesthetics, (5) economic-driven\nengagement, (6) poetic message embedding in interaction rituals, and (7)\ninteroperability enabling composability for emergence. We then discuss how\nthese features set protocol art apart from adjacent artistic movements. By\ndeveloping a theoretical framework grounded in Pak's practice, we contribute to\nthe emerging literature on protocolism while offering design implications for\nartists shaping this evolving art form.\n","authors":["Botao Amber Hu"],"pdf_url":"https://arxiv.org/pdf/2505.12393v1.pdf","comment":"Submitted to Ars Electronica Expanded Conference 2025 Research Paper"},{"id":"http://arxiv.org/abs/2503.06677v3","updated":"2025-05-18T06:50:03Z","published":"2025-03-09T16:05:36Z","title":"REArtGS: Reconstructing and Generating Articulated Objects via 3D\n  Gaussian Splatting with Geometric and Motion Constraints","summary":"  Articulated objects, as prevalent entities in human life, their 3D\nrepresentations play crucial roles across various applications. However,\nachieving both high-fidelity textured surface reconstruction and dynamic\ngeneration for articulated objects remains challenging for existing methods. In\nthis paper, we present REArtGS, a novel framework that introduces additional\ngeometric and motion constraints to 3D Gaussian primitives, enabling\nhigh-quality textured surface reconstruction and generation for articulated\nobjects. Specifically, given multi-view RGB images of arbitrary two states of\narticulated objects, we first introduce an unbiased Signed Distance Field (SDF)\nguidance to regularize Gaussian opacity fields, enhancing geometry constraints\nand improving surface reconstruction quality. Then we establish deformable\nfields for 3D Gaussians constrained by the kinematic structures of articulated\nobjects, achieving unsupervised generation of surface meshes in unseen states.\nExtensive experiments on both synthetic and real datasets demonstrate our\napproach achieves high-quality textured surface reconstruction for given\nstates, and enables high-fidelity surface generation for unseen states. Codes\nwill be released after acceptance and the project website is at\nhttps://sites.google.com/view/reartgs/home.\n","authors":["Di Wu","Liu Liu","Zhou Linli","Anran Huang","Liangtu Song","Qiaojun Yu","Qi Wu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2503.06677v3.pdf","comment":"11pages, 6 figures"}]}}